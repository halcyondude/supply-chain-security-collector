This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: schema/github-v15.26.0.graphql, src/mockdata/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
docs/
  codegen-guide.md
  data-model.md
  decisions.md
  design.md
  github-api-limits.md
  HACKING.md
  output-architecture.md
  schema.json
  schema.md
  size-reporting.md
  viewing-parquet-data.md
input/
  cncf-all.jsonl
  graduated.jsonl
  incubation.jsonl
  project-crossplane.jsonl
  sandbox.jsonl
  test-allmocked.jsonl
  test-flux.jsonl
  test-single.jsonl
  test-three-repos.jsonl
schema/
  download-schema.sh
scripts/
  ensure-env.sh
  explore-raw-responses.sh
  fetch-cncf-landscape.ts
  generate-schema-docs.sh
  generate-schema-docs.ts
  ingest-raw-data.sql
  load-raw-responses-duckdb.sql
  run-cncf-all.sh
  run-target.sh
  view-parquet.sh
src/
  graphql/
    GetRepoDataArtifacts.graphql
    GetRepoDataExtendedInfo.graphql
  analysis.ts
  api.ts
  config.ts
  main.ts
  mockData.ts
  parquetWriter.ts
  rawResponseWriter.ts
  report.ts
  sizeReport.ts
.env.template
.gitignore
.markdownlintrc
AGENTS.md
codegen.ts
eslint.config.js
package.json
README.md
REGENERATION_PLAN.md
TEST_PLAN.md
test-extended.sh
tsconfig.json
validate-with-mocks.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="input/cncf-all.jsonl">
{"owner":"project-akri","name":"akri","maturity":"sandbox"}
{"owner":"runatlantis","name":"atlantis","maturity":"sandbox"}
{"owner":"cadence-workflow","name":"cadence","maturity":"sandbox"}
{"owner":"cdk8s-team","name":"cdk8s","maturity":"sandbox"}
{"owner":"cloud-custodian","name":"cloud-custodian","maturity":"incubation"}
{"owner":"kagent-dev","name":"kagent","maturity":"sandbox"}
{"owner":"kairos-io","name":"kairos","maturity":"sandbox"}
{"owner":"kcl-lang","name":"kcl","maturity":"sandbox"}
{"owner":"kitops-ml","name":"kitops","maturity":"sandbox"}
{"owner":"kptdev","name":"kpt","maturity":"sandbox"}
{"owner":"kubean-io","name":"kubean","maturity":"sandbox"}
{"owner":"kubeedge","name":"kubeedge","maturity":"graduated"}
{"owner":"KusionStack","name":"kusion","maturity":"sandbox"}
{"owner":"meshery","name":"meshery","maturity":"sandbox"}
{"owner":"metal3-io","name":"baremetal-operator","maturity":"incubation"}
{"owner":"opentofu","name":"opentofu","maturity":"sandbox"}
{"owner":"openyurtio","name":"openyurt","maturity":"incubation"}
{"owner":"runmedev","name":"runme","maturity":"sandbox"}
{"owner":"tinkerbell","name":"tinkerbell","maturity":"sandbox"}
{"owner":"distribution","name":"distribution","maturity":"sandbox"}
{"owner":"dragonflyoss","name":"dragonfly","maturity":"incubation"}
{"owner":"goharbor","name":"harbor","maturity":"graduated"}
{"owner":"project-zot","name":"zot","maturity":"sandbox"}
{"owner":"bank-vaults","name":"bank-vaults","maturity":"sandbox"}
{"owner":"bpfman","name":"bpfman","maturity":"sandbox"}
{"owner":"cartography-cncf","name":"cartography","maturity":"sandbox"}
{"owner":"cert-manager","name":"cert-manager","maturity":"graduated"}
{"owner":"confidential-containers","name":"confidential-containers","maturity":"sandbox"}
{"owner":"containerssh","name":"containerssh","maturity":"sandbox"}
{"owner":"project-copacetic","name":"copacetic","maturity":"sandbox"}
{"owner":"dexidp","name":"dex","maturity":"sandbox"}
{"owner":"external-secrets","name":"external-secrets","maturity":"sandbox"}
{"owner":"falcosecurity","name":"falco","maturity":"graduated"}
{"owner":"hexa-org","name":"policy-orchestrator","maturity":"sandbox"}
{"owner":"in-toto","name":"in-toto","maturity":"graduated"}
{"owner":"keycloak","name":"keycloak","maturity":"incubation"}
{"owner":"keylime","name":"keylime","maturity":"sandbox"}
{"owner":"kubearmor","name":"kubearmor","maturity":"sandbox"}
{"owner":"kubescape","name":"kubescape","maturity":"incubation"}
{"owner":"kubewarden","name":"kubewarden-controller","maturity":"sandbox"}
{"owner":"kyverno","name":"kyverno","maturity":"incubation"}
{"owner":"notaryproject","name":"notation","maturity":"incubation"}
{"owner":"open-policy-agent","name":"opa","maturity":"graduated"}
{"owner":"opcr-io","name":"policy","maturity":"sandbox"}
{"owner":"openfga","name":"openfga","maturity":"sandbox"}
{"owner":"oscal-compass","name":"compliance-trestle","maturity":"sandbox"}
{"owner":"paralus","name":"paralus","maturity":"sandbox"}
{"owner":"parallaxsecond","name":"parsec","maturity":"sandbox"}
{"owner":"ratify-project","name":"ratify","maturity":"sandbox"}
{"owner":"slimtoolkit","name":"slim","maturity":"sandbox"}
{"owner":"getsops","name":"sops","maturity":"sandbox"}
{"owner":"theupdateframework","name":"python-tuf","maturity":"graduated"}
{"owner":"tokenetes","name":"tokenetes","maturity":"sandbox"}
{"owner":"AthenZ","name":"athenz","maturity":"sandbox"}
{"owner":"spiffe","name":"spiffe","maturity":"graduated"}
{"owner":"spiffe","name":"spire","maturity":"graduated"}
{"owner":"carina-io","name":"carina","maturity":"sandbox"}
{"owner":"cubeFS","name":"cubefs","maturity":"graduated"}
{"owner":"hwameistor","name":"hwameistor","maturity":"sandbox"}
{"owner":"k8up-io","name":"k8up","maturity":"sandbox"}
{"owner":"kanisterio","name":"kanister","maturity":"sandbox"}
{"owner":"longhorn","name":"longhorn","maturity":"incubation"}
{"owner":"openebs","name":"openebs","maturity":"sandbox"}
{"owner":"oras-project","name":"oras","maturity":"sandbox"}
{"owner":"piraeusdatastore","name":"piraeus-operator","maturity":"sandbox"}
{"owner":"rook","name":"rook","maturity":"graduated"}
{"owner":"v6d-io","name":"v6d","maturity":"sandbox"}
{"owner":"containerd","name":"containerd","maturity":"graduated"}
{"owner":"containers","name":"composefs","maturity":"sandbox"}
{"owner":"containers","name":"bootc","maturity":"sandbox"}
{"owner":"cri-o","name":"cri-o","maturity":"graduated"}
{"owner":"hyperlight-dev","name":"hyperlight","maturity":"sandbox"}
{"owner":"inclavare-containers","name":"inclavare-containers","maturity":"sandbox"}
{"owner":"kuasar-io","name":"kuasar","maturity":"sandbox"}
{"owner":"lima-vm","name":"lima","maturity":"sandbox"}
{"owner":"containers","name":"podman","maturity":"sandbox"}
{"owner":"urunc-dev","name":"urunc","maturity":"sandbox"}
{"owner":"virtual-kubelet","name":"virtual-kubelet","maturity":"sandbox"}
{"owner":"interTwin-eu","name":"interLink","maturity":"sandbox"}
{"owner":"WasmEdge","name":"WasmEdge","maturity":"sandbox"}
{"owner":"youki-dev","name":"youki","maturity":"sandbox"}
{"owner":"antrea-io","name":"antrea","maturity":"sandbox"}
{"owner":"cilium","name":"cilium","maturity":"graduated"}
{"owner":"containernetworking","name":"cni","maturity":"incubation"}
{"owner":"kubeovn","name":"kube-ovn","maturity":"sandbox"}
{"owner":"kube-vip","name":"kube-vip","maturity":"sandbox"}
{"owner":"networkservicemesh","name":"api","maturity":"sandbox"}
{"owner":"ovn-kubernetes","name":"ovn-kubernetes","maturity":"sandbox"}
{"owner":"spidernet-io","name":"spiderpool","maturity":"sandbox"}
{"owner":"submariner-io","name":"submariner","maturity":"sandbox"}
{"owner":"armadaproject","name":"armada","maturity":"sandbox"}
{"owner":"projectcapsule","name":"capsule","maturity":"sandbox"}
{"owner":"clusternet","name":"clusternet","maturity":"sandbox"}
{"owner":"clusterpedia-io","name":"clusterpedia","maturity":"sandbox"}
{"owner":"crossplane","name":"crossplane","maturity":"incubation"}
{"owner":"eraser-dev","name":"eraser","maturity":"sandbox"}
{"owner":"fluid-cloudnative","name":"fluid","maturity":"sandbox"}
{"owner":"Project-HAMi","name":"HAMi","maturity":"sandbox"}
{"owner":"karmada-io","name":"karmada","maturity":"incubation"}
{"owner":"kcp-dev","name":"kcp","maturity":"sandbox"}
{"owner":"k0sproject","name":"k0s","maturity":"sandbox"}
{"owner":"kedacore","name":"keda","maturity":"graduated"}
{"owner":"knative","name":"serving","maturity":"graduated"}
{"owner":"koordinator-sh","name":"koordinator","maturity":"sandbox"}
{"owner":"kube-rs","name":"kube","maturity":"sandbox"}
{"owner":"kubeflow","name":"kubeflow","maturity":"incubation"}
{"owner":"kubefleet-dev","name":"kubefleet","maturity":"sandbox"}
{"owner":"kubernetes","name":"kubernetes","maturity":"graduated"}
{"owner":"kubeslice","name":"kubeslice","maturity":"sandbox"}
{"owner":"kubestellar","name":"kubestellar","maturity":"sandbox"}
{"owner":"kubereboot","name":"kured","maturity":"sandbox"}
{"owner":"open-cluster-management-io","name":"ocm","maturity":"sandbox"}
{"owner":"OpenFunction","name":"OpenFunction","maturity":"sandbox"}
{"owner":"serverless-devs","name":"serverless-devs","maturity":"sandbox"}
{"owner":"volcano-sh","name":"volcano","maturity":"incubation"}
{"owner":"wasmCloud","name":"wasmCloud","maturity":"incubation"}
{"owner":"coredns","name":"coredns","maturity":"graduated"}
{"owner":"etcd-io","name":"etcd","maturity":"graduated"}
{"owner":"k8gb-io","name":"k8gb","maturity":"sandbox"}
{"owner":"grpc","name":"grpc","maturity":"incubation"}
{"owner":"connectrpc","name":"connect-go","maturity":"sandbox"}
{"owner":"bfenetworks","name":"bfe","maturity":"sandbox"}
{"owner":"projectcontour","name":"contour","maturity":"incubation"}
{"owner":"envoyproxy","name":"envoy","maturity":"graduated"}
{"owner":"loxilb-io","name":"loxilb","maturity":"sandbox"}
{"owner":"metallb","name":"metallb","maturity":"sandbox"}
{"owner":"easegress-io","name":"easegress","maturity":"sandbox"}
{"owner":"emissary-ingress","name":"emissary","maturity":"incubation"}
{"owner":"kgateway-dev","name":"kgateway","maturity":"sandbox"}
{"owner":"kuadrant","name":"kuadrant-operator","maturity":"sandbox"}
{"owner":"aeraki-mesh","name":"aeraki","maturity":"sandbox"}
{"owner":"istio","name":"istio","maturity":"graduated"}
{"owner":"kmesh-net","name":"kmesh","maturity":"sandbox"}
{"owner":"kumahq","name":"kuma","maturity":"sandbox"}
{"owner":"linkerd","name":"linkerd2","maturity":"graduated"}
{"owner":"sermant-io","name":"Sermant","maturity":"sandbox"}
{"owner":"cloudnative-pg","name":"cloudnative-pg","maturity":"sandbox"}
{"owner":"openGemini","name":"openGemini","maturity":"sandbox"}
{"owner":"schemahero","name":"schemahero","maturity":"sandbox"}
{"owner":"tikv","name":"tikv","maturity":"graduated"}
{"owner":"vitessio","name":"vitess","maturity":"graduated"}
{"owner":"cloudevents","name":"spec","maturity":"graduated"}
{"owner":"drasi-project","name":"drasi-platform","maturity":"sandbox"}
{"owner":"nats-io","name":"nats-server","maturity":"incubation"}
{"owner":"strimzi","name":"strimzi-kafka-operator","maturity":"incubation"}
{"owner":"tremor-rs","name":"tremor-runtime","maturity":"sandbox"}
{"owner":"artifacthub","name":"hub","maturity":"incubation"}
{"owner":"backstage","name":"backstage","maturity":"incubation"}
{"owner":"buildpacks","name":"pack","maturity":"incubation"}
{"owner":"carvel-dev","name":"ytt","maturity":"sandbox"}
{"owner":"dapr","name":"dapr","maturity":"graduated"}
{"owner":"devfile","name":"api","maturity":"sandbox"}
{"owner":"devspace-sh","name":"devspace","maturity":"sandbox"}
{"owner":"helm","name":"helm","maturity":"graduated"}
{"owner":"ko-build","name":"ko","maturity":"sandbox"}
{"owner":"konveyor","name":"operator","maturity":"sandbox"}
{"owner":"kubevela","name":"kubevela","maturity":"incubation"}
{"owner":"kubevirt","name":"kubevirt","maturity":"incubation"}
{"owner":"kudobuilder","name":"kudo","maturity":"sandbox"}
{"owner":"microcks","name":"microcks","maturity":"sandbox"}
{"owner":"modelpack","name":"model-spec","maturity":"sandbox"}
{"owner":"operator-framework","name":"operator-sdk","maturity":"incubation"}
{"owner":"podman-desktop","name":"podman-desktop","maturity":"sandbox"}
{"owner":"getporter","name":"porter","maturity":"sandbox"}
{"owner":"radius-project","name":"radius","maturity":"sandbox"}
{"owner":"score-spec","name":"spec","maturity":"sandbox"}
{"owner":"serverlessworkflow","name":"specification","maturity":"sandbox"}
{"owner":"shipwright-io","name":"build","maturity":"sandbox"}
{"owner":"project-stacker","name":"stacker","maturity":"sandbox"}
{"owner":"telepresenceio","name":"telepresence","maturity":"sandbox"}
{"owner":"vscode-kubernetes-tools","name":"vscode-kubernetes-tools","maturity":"sandbox"}
{"owner":"xregistry","name":"server","maturity":"sandbox"}
{"owner":"argoproj","name":"argo-cd","maturity":"graduated"}
{"owner":"fluxcd","name":"flux2","maturity":"graduated"}
{"owner":"open-gitops","name":"project","maturity":"sandbox"}
{"owner":"openkruise","name":"kruise","maturity":"incubation"}
{"owner":"pipe-cd","name":"pipecd","maturity":"sandbox"}
{"owner":"werf","name":"werf","maturity":"sandbox"}
{"owner":"kube-burner","name":"kube-burner","maturity":"sandbox"}
{"owner":"flatcar","name":"Flatcar","maturity":"incubation"}
{"owner":"k3s-io","name":"k3s","maturity":"sandbox"}
{"owner":"kubeclipper","name":"kubeclipper","maturity":"sandbox"}
{"owner":"cozystack","name":"cozystack","maturity":"sandbox"}
{"owner":"SlimPlanet","name":"SlimFaas","maturity":"sandbox"}
{"owner":"open-feature","name":"spec","maturity":"incubation"}
{"owner":"chaos-mesh","name":"chaos-mesh","maturity":"incubation"}
{"owner":"chaosblade-io","name":"chaosblade","maturity":"sandbox"}
{"owner":"litmuschaos","name":"litmus","maturity":"incubation"}
{"owner":"krkn-chaos","name":"krkn","maturity":"sandbox"}
{"owner":"opencost","name":"opencost","maturity":"incubation"}
{"owner":"cortexproject","name":"cortex","maturity":"incubation"}
{"owner":"fluent","name":"fluentd","maturity":"graduated"}
{"owner":"inspektor-gadget","name":"inspektor-gadget","maturity":"sandbox"}
{"owner":"jaegertracing","name":"jaeger","maturity":"graduated"}
{"owner":"k8sgpt-ai","name":"k8sgpt","maturity":"sandbox"}
{"owner":"sustainable-computing-io","name":"kepler","maturity":"sandbox"}
{"owner":"kuberhealthy","name":"kuberhealthy","maturity":"sandbox"}
{"owner":"kube-logging","name":"logging-operator","maturity":"sandbox"}
{"owner":"open-telemetry","name":"community","maturity":"incubation"}
{"owner":"perses","name":"perses","maturity":"sandbox"}
{"owner":"pixie-io","name":"pixie","maturity":"sandbox"}
{"owner":"prometheus","name":"prometheus","maturity":"graduated"}
{"owner":"thanos-io","name":"thanos","maturity":"incubation"}
{"owner":"trickstercache","name":"trickster","maturity":"sandbox"}
{"owner":"spinframework","name":"spin-operator","maturity":"sandbox"}
{"owner":"container2wasm","name":"container2wasm","maturity":"sandbox"}
{"owner":"kaito-project","name":"kaito","maturity":"sandbox"}
{"owner":"kserve","name":"kserve","maturity":"incubation"}
</file>

<file path="input/project-crossplane.jsonl">
{"owner":"crossplane","name":"crossplane","maturity":"incubation"}
</file>

<file path="scripts/explore-raw-responses.sh">
#!/usr/bin/env bash
#
# explore-raw-responses.sh
# Launch DuckDB UI with raw JSONL responses loaded and reshaped for analysis
#
# Usage:
#   ./scripts/explore-raw-responses.sh              # Launch UI with in-memory DB
#   ./scripts/explore-raw-responses.sh mydata.db    # Launch UI with persistent DB
#   ./scripts/explore-raw-responses.sh --help       # Show help

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

usage() {
    cat <<EOF
${BLUE}GitHub Supply Chain Data Collector - Raw Response Explorer${NC}

Load and explore raw JSONL GitHub API responses using DuckDB's powerful analytics.

${GREEN}Usage:${NC}
    $0 [OPTIONS] [DATABASE]

${GREEN}Arguments:${NC}
    DATABASE            Optional path to DuckDB database file (default: in-memory)

${GREEN}Options:${NC}
    -h, --help          Show this help message
    -c, --cli           Launch DuckDB CLI instead of UI
    -o, --output DIR    Specify output directory to load (default: output/*)
    -r, --read-only     Open database in read-only mode

${GREEN}Examples:${NC}
    # Launch UI with in-memory database
    $0

    # Launch UI with persistent database
    $0 analysis.duckdb

    # Launch CLI instead of UI
    $0 --cli

    # Load specific output run
    $0 --output output/graduated-2025-10-10T03-46-40

    # Open existing database read-only
    $0 --read-only analysis.duckdb

${GREEN}Views Created:${NC}
    - raw_responses              Raw JSONL data
    - repositories               Repository metadata
    - releases_flattened         All releases with artifact counts
    - release_artifacts          Individual artifacts per release
    - artifacts_classified       Artifacts with supply chain signals
    - repo_summary               Per-repo aggregate metrics
    - release_summary            Per-release artifact breakdown
    - supply_chain_adoption      Overall adoption metrics
    - adoption_trends            Time-series adoption data

${GREEN}Example Queries:${NC}
    -- Find repos with both SBOMs and signatures
    SELECT * FROM repo_summary WHERE sbom_count > 0 AND signature_count > 0;

    -- Recent releases with supply chain artifacts
    SELECT * FROM release_summary WHERE signatures > 0 OR sboms > 0 LIMIT 10;

    -- Adoption trends over time
    SELECT * FROM adoption_trends ORDER BY month;

EOF
}

# Parse arguments
USE_UI=1
DATABASE=""
OUTPUT_PATTERN="output/*/raw-responses.jsonl"
READ_ONLY=""

while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            usage
            exit 0
            ;;
        -c|--cli)
            USE_UI=0
            shift
            ;;
        -o|--output)
            OUTPUT_PATTERN="$2/raw-responses.jsonl"
            shift 2
            ;;
        -r|--read-only)
            READ_ONLY="-readonly"
            shift
            ;;
        -*)
            echo -e "${RED}Error: Unknown option $1${NC}" >&2
            usage
            exit 1
            ;;
        *)
            DATABASE="$1"
            shift
            ;;
    esac
done

# Check if DuckDB is installed
if ! command -v duckdb &> /dev/null; then
    echo -e "${RED}Error: duckdb command not found${NC}" >&2
    echo "Install DuckDB: https://duckdb.org/docs/installation/" >&2
    exit 1
fi

# Check if raw-responses.jsonl files exist
cd "$PROJECT_ROOT"
if ! ls $OUTPUT_PATTERN &> /dev/null; then
    echo -e "${YELLOW}Warning: No raw-responses.jsonl files found matching pattern:${NC}" >&2
    echo "  $OUTPUT_PATTERN" >&2
    echo "" >&2
    echo "Run the data collector first to generate raw responses:" >&2
    echo "  npm start -- --input input/graduated.jsonl" >&2
    exit 1
fi

# Create temporary SQL file with output pattern substitution
TEMP_SQL=$(mktemp)
trap "rm -f $TEMP_SQL" EXIT

# Generate SQL with correct output pattern
sed "s|output/\\*/raw-responses.jsonl|$OUTPUT_PATTERN|g" \
    "$SCRIPT_DIR/load-raw-responses-duckdb.sql" > "$TEMP_SQL"

echo -e "${GREEN}Loading raw JSONL responses from:${NC} $OUTPUT_PATTERN"

# Launch DuckDB
if [ -z "$DATABASE" ]; then
    echo -e "${BLUE}Using in-memory database${NC}"
    if [ $USE_UI -eq 1 ]; then
        echo -e "${GREEN}Launching DuckDB UI...${NC}"
        duckdb -init "$TEMP_SQL" -ui $READ_ONLY
    else
        echo -e "${GREEN}Launching DuckDB CLI...${NC}"
        duckdb -init "$TEMP_SQL" $READ_ONLY
    fi
else
    echo -e "${BLUE}Using database file:${NC} $DATABASE"
    if [ $USE_UI -eq 1 ]; then
        echo -e "${GREEN}Launching DuckDB UI...${NC}"
        duckdb "$DATABASE" -init "$TEMP_SQL" -ui $READ_ONLY
    else
        echo -e "${GREEN}Launching DuckDB CLI...${NC}"
        duckdb "$DATABASE" -init "$TEMP_SQL" $READ_ONLY
    fi
fi
</file>

<file path="scripts/ingest-raw-data.sql">
-- Step 1: Install and load required extensions.
INSTALL json;
INSTALL fts;
LOAD json;
LOAD fts;

-- Step 2: Create a base view to read and parse all raw-responses.jsonl files.
-- The glob pattern efficiently finds all raw response files in any timestamped output directory.
-- The format is explicitly set to handle newline-delimited JSON.
CREATE OR REPLACE VIEW raw_github_responses AS
SELECT * FROM read_json_auto('output/*-*/raw-responses.jsonl', format = 'newline_delimited', union_by_name=true);

-- Step 3: Progressively flatten the nested JSON structure using a series of views.

-- Unpack the first level of nesting to separate metadata from the main response object.
CREATE OR REPLACE VIEW unpacked_data AS
SELECT
    metadata.queryType AS query_type,
    metadata.timestamp AS query_timestamp,
    metadata.owner AS owner,
    metadata.repo AS repo,
    metadata.maturity AS maturity,
    response.repository AS repository_data
FROM raw_github_responses
WHERE repository_data IS NOT NULL;

-- Unnest the releases array. The UNNEST function creates a new row for each release.
CREATE OR REPLACE VIEW releases_flattened AS
SELECT
    query_type,
    query_timestamp,
    owner,
    repo,
    repository_data.nameWithOwner AS repo_name_with_owner,
    repository_data.url AS repo_url,
    repository_data.description AS repo_description,
    UNNEST(repository_data.releases.nodes) AS release
FROM unpacked_data;

-- Unnest the artifacts within each release. This creates the final flat structure where each row is a single artifact.
CREATE OR REPLACE VIEW artifacts_flattened AS
SELECT
    rf.query_type,
    rf.query_timestamp,
    rf.owner,
    rf.repo,
    rf.repo_name_with_owner,
    rf.repo_url,
    rf.repo_description,
    rf.release.tagName AS release_tag_name,
    rf.release.name AS release_name,
    rf.release.createdAt::TIMESTAMP AS release_created_at, -- Cast to TIMESTAMP
    UNNEST(rf.release.releaseAssets.nodes) AS artifact
FROM releases_flattened rf;

-- Step 4: Create the final, materialized table.
-- This SELECT statement constructs the final schema, pulling fields from the flattened views
-- and casting them to appropriate types as informed by docs/schema.json.
CREATE OR REPLACE TABLE github_supply_chain_data AS
SELECT
    query_type,
    query_timestamp,
    owner AS repository_owner,
    repo AS repository_name,
    repo_name_with_owner,
    repo_url,
    repo_description,
    release_tag_name,
    release_name,
    release_created_at,
    artifact.name as artifact_name,
    artifact.downloadUrl as artifact_download_url
FROM artifacts_flattened;

-- Step 5: Create a Full-Text Search (FTS) index on the most common text fields.
-- This is the "generic smart way" to enable fast, free-text searching on this data.
PRAGMA create_fts_index('github_supply_chain_data', 'repo_name_with_owner', 'repo_description', 'release_tag_name', 'release_name', 'artifact_name');

-- Step 6: Export the final, indexed table to a compressed Parquet file.
-- This single file will be the primary artifact for all downstream analysis.
COPY github_supply_chain_data TO 'output/cncf-all-analyzed.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);

-- Final confirmation message
SELECT 'Success: Created output/cncf-all-analyzed.parquet with FTS indexes.' AS status;
</file>

<file path="scripts/load-raw-responses-duckdb.sql">
-- GitHub Supply Chain Data Collector: Raw JSONL Response Analysis
-- This script loads raw-responses.jsonl files and reshapes them for analysis
-- Usage: duckdb -ui < scripts/load-raw-responses-duckdb.sql
--    or: duckdb mydb.duckdb < scripts/load-raw-responses-duckdb.sql

-- =============================================================================
-- 1. LOAD RAW JSONL DATA
-- =============================================================================

-- Load the JSONL file directly - DuckDB auto-detects the format
CREATE OR REPLACE VIEW raw_responses AS
SELECT *
FROM read_json('output/*/raw-responses.jsonl',
    format='newline_delimited',
    auto_detect=true,
    maximum_object_size=10000000
);

-- =============================================================================
-- 2. EXTRACT METADATA AND FLATTEN
-- =============================================================================

CREATE OR REPLACE VIEW response_metadata AS
SELECT
    metadata->>'queryType' AS query_type,
    metadata->>'timestamp' AS timestamp,
    metadata->>'owner' AS owner,
    metadata->>'repo' AS repo,
    metadata->'inputs'->>'owner' AS input_owner,
    metadata->'inputs'->>'name' AS input_name
FROM raw_responses;

-- =============================================================================
-- 3. REPOSITORY-LEVEL DATA
-- =============================================================================

CREATE OR REPLACE VIEW repositories AS
SELECT
    metadata->>'owner' AS owner,
    metadata->>'repo' AS repo,
    response->'repository'->>'id' AS repository_id,
    response->'repository'->>'name' AS name,
    response->'repository'->>'nameWithOwner' AS name_with_owner
FROM raw_responses
WHERE response IS NOT NULL
  AND response->'repository' IS NOT NULL;

-- =============================================================================
-- 4. RELEASES WITH NESTED ARTIFACT DATA
-- =============================================================================

CREATE OR REPLACE VIEW releases_raw AS
SELECT
    r.metadata->>'owner' AS owner,
    r.metadata->>'repo' AS repo,
    r.response->'repository'->>'nameWithOwner' AS name_with_owner,
    UNNEST(json_extract(r.response, '$.repository.releases.nodes')) AS release_node
FROM raw_responses r
WHERE r.response->'repository'->'releases' IS NOT NULL;

CREATE OR REPLACE VIEW releases_flattened AS
SELECT
    owner,
    repo,
    name_with_owner,
    release_node->>'id' AS release_id,
    release_node->>'name' AS release_name,
    release_node->>'tagName' AS tag_name,
    release_node->>'url' AS release_url,
    release_node->>'createdAt' AS created_at,
    json_array_length(release_node->'releaseAssets'->'nodes') AS asset_count,
    release_node->'releaseAssets'->'nodes' AS assets_json
FROM releases_raw;

-- =============================================================================
-- 5. INDIVIDUAL RELEASE ARTIFACTS (FULLY NORMALIZED)
-- =============================================================================

CREATE OR REPLACE VIEW release_artifacts AS
SELECT
    rf.owner,
    rf.repo,
    rf.name_with_owner,
    rf.release_id,
    rf.release_name,
    rf.tag_name,
    rf.release_url,
    rf.created_at::TIMESTAMP AS release_created_at,
    rf.asset_count,
    UNNEST(json_extract(rf.assets_json, '$[*]')) AS artifact_node,
    artifact_node->>'id' AS artifact_id,
    artifact_node->>'name' AS artifact_name,
    artifact_node->>'downloadUrl' AS download_url
FROM releases_flattened rf
WHERE rf.assets_json IS NOT NULL
  AND json_array_length(rf.assets_json) > 0;

-- =============================================================================
-- 6. ARTIFACT CLASSIFICATION (SUPPLY CHAIN SIGNALS)
-- =============================================================================

CREATE OR REPLACE VIEW artifacts_classified AS
SELECT
    owner,
    repo,
    name_with_owner,
    release_id,
    release_name,
    tag_name,
    release_created_at,
    artifact_name,
    download_url,
    -- Signature detection
    CASE
        WHEN artifact_name ILIKE '%.asc' THEN true
        WHEN artifact_name ILIKE '%.sig' THEN true
        WHEN artifact_name ILIKE '%.pem' THEN true
        ELSE false
    END AS is_signature,
    -- SBOM detection
    CASE
        WHEN artifact_name ILIKE '%sbom%' THEN true
        WHEN artifact_name ILIKE '%.spdx%' THEN true
        WHEN artifact_name ILIKE '%cyclonedx%' THEN true
        ELSE false
    END AS is_sbom,
    -- Checksum/hash detection
    CASE
        WHEN artifact_name ILIKE '%checksum%' THEN true
        WHEN artifact_name ILIKE '%.sha%' THEN true
        WHEN artifact_name ILIKE '%.md5' THEN true
        WHEN artifact_name ILIKE '%checksums%' THEN true
        ELSE false
    END AS is_checksum,
    -- Platform hints
    CASE
        WHEN artifact_name ILIKE '%linux%' THEN 'linux'
        WHEN artifact_name ILIKE '%darwin%' THEN 'darwin'
        WHEN artifact_name ILIKE '%windows%' THEN 'windows'
        WHEN artifact_name ILIKE '%amd64%' THEN 'amd64'
        WHEN artifact_name ILIKE '%arm%' THEN 'arm'
        WHEN artifact_name ILIKE '%macos%' THEN 'darwin'
        ELSE 'unknown'
    END AS platform_hint,
    -- File extension
    regexp_extract(artifact_name, '\\.([^.]+)$', 1) AS file_extension
FROM release_artifacts;

-- =============================================================================
-- 7. AGGREGATE SUMMARY VIEWS
-- =============================================================================

-- Repository summary: count of releases and artifacts per repo
CREATE OR REPLACE VIEW repo_summary AS
SELECT
    owner,
    repo,
    name_with_owner,
    COUNT(DISTINCT release_id) AS total_releases,
    COUNT(DISTINCT artifact_id) AS total_artifacts,
    SUM(CASE WHEN is_signature THEN 1 ELSE 0 END) AS signature_count,
    SUM(CASE WHEN is_sbom THEN 1 ELSE 0 END) AS sbom_count,
    SUM(CASE WHEN is_checksum THEN 1 ELSE 0 END) AS checksum_count,
    MAX(release_created_at) AS latest_release_date,
    MIN(release_created_at) AS earliest_release_date
FROM artifacts_classified
GROUP BY owner, repo, name_with_owner
ORDER BY total_releases DESC;

-- Release summary: artifact breakdown per release
CREATE OR REPLACE VIEW release_summary AS
SELECT
    owner,
    repo,
    release_name,
    tag_name,
    release_created_at,
    COUNT(*) AS artifact_count,
    SUM(CASE WHEN is_signature THEN 1 ELSE 0 END) AS signatures,
    SUM(CASE WHEN is_sbom THEN 1 ELSE 0 END) AS sboms,
    SUM(CASE WHEN is_checksum THEN 1 ELSE 0 END) AS checksums,
    array_agg(DISTINCT platform_hint) AS platforms
FROM artifacts_classified
GROUP BY owner, repo, release_name, tag_name, release_created_at
ORDER BY release_created_at DESC;

-- =============================================================================
-- 8. SUPPLY CHAIN ADOPTION METRICS
-- =============================================================================

CREATE OR REPLACE VIEW supply_chain_adoption AS
SELECT
    COUNT(DISTINCT CASE WHEN signature_count > 0 THEN name_with_owner END) AS repos_with_signatures,
    COUNT(DISTINCT CASE WHEN sbom_count > 0 THEN name_with_owner END) AS repos_with_sboms,
    COUNT(DISTINCT CASE WHEN checksum_count > 0 THEN name_with_owner END) AS repos_with_checksums,
    COUNT(DISTINCT CASE WHEN signature_count > 0 AND sbom_count > 0 THEN name_with_owner END) AS repos_with_both_sig_sbom,
    COUNT(DISTINCT name_with_owner) AS total_repos,
    ROUND(100.0 * COUNT(DISTINCT CASE WHEN signature_count > 0 THEN name_with_owner END) / COUNT(DISTINCT name_with_owner), 2) AS pct_repos_with_signatures,
    ROUND(100.0 * COUNT(DISTINCT CASE WHEN sbom_count > 0 THEN name_with_owner END) / COUNT(DISTINCT name_with_owner), 2) AS pct_repos_with_sboms
FROM repo_summary;

-- =============================================================================
-- 9. TIME SERIES: ADOPTION TRENDS
-- =============================================================================

CREATE OR REPLACE VIEW adoption_trends AS
SELECT
    DATE_TRUNC('month', release_created_at) AS month,
    COUNT(DISTINCT name_with_owner) AS repos_released,
    SUM(CASE WHEN is_signature THEN 1 ELSE 0 END) AS total_signatures_released,
    SUM(CASE WHEN is_sbom THEN 1 ELSE 0 END) AS total_sboms_released,
    ROUND(100.0 * SUM(CASE WHEN is_signature THEN 1 ELSE 0 END) / COUNT(*), 2) AS pct_artifacts_with_signature,
    ROUND(100.0 * SUM(CASE WHEN is_sbom THEN 1 ELSE 0 END) / COUNT(*), 2) AS pct_artifacts_with_sbom
FROM artifacts_classified
GROUP BY DATE_TRUNC('month', release_created_at)
ORDER BY month;

-- =============================================================================
-- 10. FINAL REPORT QUERIES
-- =============================================================================

.print ''
.print '=============================================================================='
.print 'GitHub Supply Chain Data Analysis - Raw JSONL Response Dashboard'
.print '=============================================================================='
.print ''

.print '--- SUPPLY CHAIN ADOPTION OVERVIEW ---'
FROM supply_chain_adoption;

.print ''
.print '--- TOP 10 REPOSITORIES BY RELEASE COUNT ---'
FROM repo_summary
LIMIT 10;

.print ''
.print '--- RECENT RELEASES WITH SUPPLY CHAIN ARTIFACTS ---'
FROM release_summary
WHERE signatures > 0 OR sboms > 0
ORDER BY release_created_at DESC
LIMIT 15;

.print ''
.print '--- ADOPTION TRENDS BY MONTH ---'
FROM adoption_trends
ORDER BY month DESC
LIMIT 12;

.print ''
.print '--- ARTIFACT FILE EXTENSION BREAKDOWN ---'
SELECT
    file_extension,
    COUNT(*) AS count,
    ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS pct
FROM artifacts_classified
WHERE file_extension IS NOT NULL
GROUP BY file_extension
ORDER BY count DESC
LIMIT 20;

.print ''
.print '=============================================================================='
.print 'Analysis complete. Views available for further exploration:'
.print '  - raw_responses, repositories, releases_flattened, release_artifacts'
.print '  - artifacts_classified, repo_summary, release_summary'
.print '  - supply_chain_adoption, adoption_trends'
.print '=============================================================================='
.print ''
</file>

<file path="REGENERATION_PLAN.md">
### **Revised Goal:**
Perform a full data collection for all CNCF projects. Then, using the project's own schema definition (`schema.json`), create a dynamic TypeScript script to orchestrate DuckDB. This script will ingest the raw, nested JSONL, transform it into a flat, analytics-ready structure with proper data types (like timestamps), and automatically create full-text search indexes on relevant text fields. The final output will be a single, highly optimized Parquet file, ready for immediate exploration in DuckDB and Jupyter.

### **Revised Actionable Steps:**

1.  **Update and Persist Plan:** I will update `REGENERATION_PLAN.md` with this new, more detailed plan.
2.  **Update Input Data:** Refresh the CNCF project lists using the existing `fetch:cncf` script.
3.  **Full Data Collection:** Run the data collection for all three CNCF tiers (sandbox, incubation, graduated) for both default and extended queries (6 total runs).
4.  **Create Dynamic Ingestion Script (`scripts/ingest-raw-data.ts`):** This is the core of the new plan. I will write a TypeScript script that:
    *   Reads the target schema from `docs/schema.json`.
    *   Uses `glob` to find all `raw-responses.jsonl` files to be processed.
    *   Programmatically connects to an in-memory DuckDB instance.
    *   Executes a series of queries to ingest and progressively flatten the nested JSON data using `UNNEST`.
    *   Dynamically generates a final `SELECT` statement that casts fields to their correct types (e.g., `string` to `TIMESTAMP`) based on the schema.
    *   Creates a final, transformed table from this data.
    *   Intelligently identifies text-heavy columns from the schema and automatically creates full-text search (FTS) indexes on them.
    *   Exports the final, indexed table to a single, compressed Parquet file: `output/cncf-all-analyzed.parquet`.
5.  **Create Analysis Notebook:** Generate a new `analysis.ipynb` notebook that demonstrates how to load and query this new Parquet file.
6.  **Provide Access Instructions:** Deliver the final command to launch the DuckDB UI for exploration.
</file>

<file path="TEST_PLAN.md">
# Test Plan: GitHub Supply Chain Security Analyzer

This document outlines a comprehensive test plan for the **GitHub Supply Chain Security Analyzer**. Its purpose is to guide an AI agent, acting as a Quality Assurance Engineer, through a series of validation steps to ensure the tool's functionality, accuracy, and robustness. The plan covers environment setup, core feature validation, edge case handling, and output verification, moving the project from a "bootstrap and untested" state to "fully working."

---

## 1. Introduction

The GitHub Supply Chain Security Analyzer is a Node.js-based command-line tool designed to collect and analyze software supply chain security metadata from GitHub repositories. This test plan will validate the following core aspects:

* Correct setup and execution of the tool.
* Accurate data fetching from the GitHub GraphQL API.
* Correct identification and flagging of SBOMs, signatures, and attestations in release artifacts.
* Accurate detection of security-related tools within GitHub Actions CI workflows.
* Effective caching of API responses.
* Graceful handling of various error conditions and edge cases.
* Correct generation and content of JSON and CSV reports.

---

## 2. Test Environment Setup

### 2.3. Mock Mode for Development and Testing

To avoid GitHub API rate limits and enable rapid development/testing, the tool supports a **mock mode**. When the environment variable `MOCK_GITHUB=1` is set, the analyzer will use local mock data (from `src/mockData.ts`) instead of making real API calls. This allows you to:

* Run all core analysis and reporting logic without a GitHub PAT or network access.
* Test most features and edge cases instantly, without waiting for API responses or risking rate limits.
* Safely develop and debug the tool before using a real PAT.

**How to use mock mode:**

1. In your shell, run:

    ```bash
    export MOCK_GITHUB=1
    npm start
    ```

2. The tool will print a message indicating mock mode is enabled. All repository data will be loaded from `src/mockData.ts`.
3. Reports will be generated as usual in `output/`.

**Limitations:**
* Only repositories present in `src/mockData.ts` will return data; others will be skipped with a warning.
* No real API calls are made, so this mode cannot test live error handling (e.g., rate limits, real network failures).
* To test with real GitHub data, unset `MOCK_GITHUB` and ensure `GITHUB_PAT` is set.

---

Before executing any test cases, ensure the environment is correctly configured.

### 2.1. Prerequisites

* **Node.js**: Version 18.x or later installed.
* **npm**: Latest version installed (comes with Node.js).
* **Git**: Installed for cloning the repository.
* **GitHub Personal Access Token (PAT)**: A PAT with `repo` scope (for public and private repository access) is required. This should be obtained from [GitHub Developer Settings](https://github.com/settings/tokens).

### 2.2. Project Installation

The AI agent should perform the following shell commands to set up the project.

1. **Clone the repository:**

    ```bash
    git clone <repository_url> # Replace <repository_url> with the actual repo URL
    cd <project_root>
    ```

2. **Install dependencies:**

    ```bash
    npm install
    ```

3. **Configure Environment Variables (`.env`):**
    * Create a file named `.env` in the root of the project directory.

    * Add your GitHub PAT to this file (skip if using mock mode):

        ```env
        GITHUB_PAT=ghp_YourPersonalAccessTokenHere
        ```

        **Note:** Replace `ghp_YourPersonalAccessTokenHere` with your actual GitHub PAT. If you are using mock mode, you do not need to set this variable.
4. **Define Target Repositories (`src/config.ts`):**
    * Ensure `src/config.ts` contains the desired list of repositories to analyze. For testing, it's recommended to start with a diverse set including:
        * If using mock mode, only repositories present in `src/mockData.ts` will return data.
        * A repository known to have SBOMs/signatures (e.g., `sigstore/cosign`).
        * A repository with identifiable CI security tools (e.g., `anchore/syft`).
        * A simple repository with no explicit SBOMs or security workflows.
        * A non-existent repository (for error handling tests).

        ```typescript
        // Example for src/config.ts for testing
        export const repositories: RepositoryTarget[] = [
            { owner: 'sigstore', name: 'cosign' },
            { owner: 'anchore', name: 'syft' },
            { owner: 'github', name: 'docs' }, // A large repo, unlikely to have direct SBOMs/signatures in releases, might have basic workflows
            { owner: 'nonexistent-org', name: 'nonexistent-repo-123' }, // For error testing
        ];
        // In mock mode, only the first three will return data (see src/mockData.ts)
        ```

5. **Generate the GraphQL SDK:**

    ```bash
    npm run codegen
    ```

    Confirm that `src/generated/graphql.ts` is created or updated.

---

## 3. Testing Strategy

The testing strategy will focus on **End-to-End (E2E) and Integration tests**. Given the nature of the tool (CLI utility interacting with external APIs and generating reports), these types of tests are most effective for validating the overall workflow.

### 3.1. General Validation Steps for Each Test Case

**Mock Mode Applicability:**
* All test cases except those specifically requiring real API/network errors (e.g., rate limit handling, real non-existent repo errors) can be run in mock mode.
* For most development, set `MOCK_GITHUB=1` to avoid using your PAT and to iterate quickly.

For each test case, the agent should:

1. **Execute the `npm start` command.**
2. **Monitor console output:** Look for success messages, error messages, and caching indications.
3. **Inspect generated files:** Verify the existence and content of `output/report.json` and `output/report.csv`.
4. **Analyze file content:**
    * For `report.json`: Check the overall structure, presence of expected data points (repository info, releases, artifacts, workflows), and the accuracy of `summary` flags (`hasSbomArtifact`, `hasSignatureArtifact`, `sbomCiTools`).
    * For `report.csv`: Check column headers, row counts, and the accuracy of boolean flags and comma-separated lists for detected tools.

---

## 4. Detailed Test Cases

### Category A: Core Functionality (Happy Path)

#### Test Case A.1: Successful Analysis of Multiple Repositories

* **Objective:** Verify the tool can successfully fetch data for multiple valid repositories and produce reports without errors.
* **Preconditions:** `src/config.ts` contains at least two *valid, accessible* GitHub repositories (e.g., `sigstore/cosign`, `anchore/syft`). GitHub PAT is correctly configured.
* **Execution:**

    ```bash
    npm start
    ```

* **Expected Outcome:**
  * Console output indicates successful fetching and analysis for each configured repository.
  * `output/report.json` and `output/report.csv` are created.
  * Reports contain entries for all configured repositories.

#### Test Case A.2: Artifact Detection (SBOM, Signature, Attestation)

* **Objective:** Verify the tool correctly identifies and flags SBOMs, signatures, and attestations within release assets.
* **Preconditions:** `src/config.ts` includes `sigstore/cosign` (known to have signed releases and potentially SBOMs).
* **Execution:**

    ```bash
    npm start
    ```

* **Expected Outcome:**
  * In `output/report.json`, for `sigstore/cosign`, `analysis.summary.hasSbomArtifact` and `analysis.summary.hasSignatureArtifact` should be `true`. Inspect `releases[].artifacts` for specific names matching SBOM/signature patterns.
  * In `output/report.csv`, for `sigstore/cosign`, `has_sbom_artifact` and `has_signature_artifact` columns should show `TRUE`.

#### Test Case A.3: CI Workflow Tool Detection

* **Objective:** Verify the tool correctly detects security-related tools within GitHub Actions workflow files.
* **Preconditions:** `src/config.ts` includes `anchore/syft` (known to use `syft` or similar SBOM generators in CI) and potentially `sigstore/cosign` (known to use `goreleaser` which can generate SBOMs).
* **Execution:**

    ```bash
    npm start
    ```

* **Expected Outcome:**
  * In `output/report.json`:
    * For `anchore/syft`, `analysis.summary.sbomCiTools` should contain relevant keywords (e.g., `'sbom-generator'`, `'goreleaser'`).
    * For `sigstore/cosign`, `analysis.summary.sbomCiTools` should contain `'goreleaser'` and potentially `'signer'`.
  * In `output/report.csv`:
    * `sbom_ci_tools_detected` column for `anchore/syft` and `sigstore/cosign` should list the detected tools.

#### Test Case A.4: Caching Mechanism Validation

* **Objective:** Verify that API responses are cached and subsequent runs for the same repositories use cached data.
* **Preconditions:** `src/config.ts` contains valid repositories.
* **Execution:**
    1. Run `npm start` once.
    2. Wait for completion.
    3. Run `npm start` a second time *immediately* (within the cache TTL, which is 24 hours by default).
* **Expected Outcome:**
  * First run: Console output should show "🔄 Fetching data from GitHub API..." and "👍 Data fetched and cached successfully." for each repository.
  * Second run: Console output should show "✅ Found data in cache." for each repository, indicating no new API calls were made for cached data.
  * Reports should be identical to the first run.

### Category B: Edge Cases & Error Handling

#### Test Case B.1: Missing GitHub PAT

**Note:** In mock mode, the tool does not require a GitHub PAT and will not error if it is missing. To test this case, ensure `MOCK_GITHUB` is unset.

* **Objective:** Verify the tool gracefully handles the absence of the `GITHUB_PAT` environment variable.
* **Preconditions:** Comment out or delete the `GITHUB_PAT` line in `.env`.
* **Execution:**

    ```bash
    npm start
    ```

* **Expected Outcome:**
  * The script should exit with an error.
  * Console output should display a clear error message: "Error: GITHUB_PAT environment variable not set."
  * No report files should be generated.

#### Test Case B.2: Non-Existent Repository

**Note:** In mock mode, only repositories present in `src/mockData.ts` will return data. Others will be skipped with a warning, not a real API error. To test real error handling, unset `MOCK_GITHUB` and use a real PAT.

* **Objective:** Verify the tool handles requests for non-existent repositories gracefully, skipping them and continuing with others.
* **Preconditions:** `src/config.ts` contains at least one *valid* repository and one *non-existent* repository (e.g., `{ owner: 'nonexistent-org', name: 'nonexistent-repo-123' }`). GitHub PAT is configured.
* **Execution:**

    ```bash
    npm start
    ```

* **Expected Outcome:**
  * Console output should show an error message for the non-existent repository (e.g., "Failed to fetch data for nonexistent-org/nonexistent-repo-123").
  * The tool should continue processing other valid repositories.
  * `output/report.json` and `output/report.csv` should be generated, but they should *not* contain an entry for the non-existent repository.

#### Test Case B.3: Repository with No Releases/Workflows

* **Objective:** Verify the tool handles repositories that might lack releases or GitHub Actions workflows without crashing.
* **Preconditions:** `src/config.ts` includes a repository known to have no releases or no `.github/workflows` directory (e.g., a very new, empty, or non-CI-driven repository).
* **Execution:**

    ```bash
    npm start
    ```

* **Expected Outcome:**
  * The tool should process the repository without errors.
  * In `output/report.json`, for this repository, the `releases` array or `workflows` array should be empty, and corresponding `summary` flags (`hasSbomArtifact`, `hasSignatureArtifact`, `sbomCiTools`) should be `false` or empty.
  * In `output/report.csv`, the relevant columns should reflect 'N/A' or `FALSE`.

#### Test Case B.4: API Rate Limit Handling (Conceptual)

**Note:** Mock mode does not simulate real API rate limits. This test must be performed with real API calls.

* **Objective:** Understand and document the tool's behavior when GitHub API rate limits are encountered.
* **Preconditions:** This is difficult to replicate programmatically. It requires making many requests in a short period to exceed GitHub's rate limits.
* **Execution:** The agent should *not* explicitly attempt to trigger rate limits unless specifically instructed. Instead, it should document the expected behavior.
* **Expected Outcome (Conceptual):**
  * If a rate limit is hit, the `graphql-request` client would typically throw an error. The `main.ts` script's `try-catch` block for `sdk.GetRepoData` should log this error, and the tool should gracefully skip the affected repository (as currently implemented).
  * The tool does not currently have explicit retry logic or rate limit awareness beyond basic error catching. This could be a future enhancement.

### Category C: Report Validation

#### Test Case C.1: JSON Report Structure and Content

* **Objective:** Validate the structure, completeness, and accuracy of `output/report.json`.
* **Preconditions:** Successful run with a diverse set of repositories (from Test Cases A.1, A.2, A.3).
* **Execution:** Open `output/report.json` and perform a manual/programmatic review.
* **Expected Outcome:**
  * The file should be a valid JSON array.
  * Each array element should correspond to a analyzed repository.
  * Each repository object should contain `name`, `url`, `description`, `releases`, `workflows`, and `summary` properties.
  * `releases` should be an array of objects, each containing `tagName`, `name`, `createdAt`, and an `artifacts` array.
  * `artifacts` should contain `name`, `isSbom`, `isSignature`, `isAttestation`.
  * `workflows` should be an array of objects, each containing `name` and `detectedSbomTools`.
  * `summary` should accurately reflect the `hasSbomArtifact`, `hasSignatureArtifact`, `hasAttestationArtifact` (booleans), and `sbomCiTools` (array of strings) based on the detailed data.

#### Test Case C.2: CSV Report Structure and Content

* **Objective:** Validate the structure, readability, and data integrity of `output/report.csv`.
* **Preconditions:** Successful run with a diverse set of repositories (from Test Cases A.1, A.2, A.3).
* **Execution:** Open `output/report.csv` using a spreadsheet viewer or text editor and perform a manual/programmatic review.
* **Expected Outcome:**
  * The first row should contain correct, human-readable column headers (e.g., `repository_name`, `has_sbom_artifact`, `sbom_ci_tools_detected`).
  * Each subsequent row should correspond to a analyzed repository.
  * Data types should be appropriate (e.g., booleans as `TRUE`/`FALSE`, lists as comma-separated strings).
  * Repository descriptions should be sanitized (newlines removed) for single-cell readability.
  * The number of rows (excluding header) should match the number of successfully analyzed repositories.

---

## 5. Agent Instructions for Execution and Reporting

1. **Prioritize `FIXNOW.md`:** Before executing any tests, apply all changes specified in `FIXNOW.md`. Confirm these changes have been made in the respective files.
2. **Follow Setup:** Execute all steps under "2. Test Environment Setup". Document any issues encountered during setup.
3. **Execute Test Cases:** Proceed through "4. Detailed Test Cases" sequentially. For each test case:
    * Clearly state the test case being executed (e.g., "Executing Test Case A.1: Successful Analysis of Multiple Repositories").
    * Perform the "Execution" steps.
    * Record the actual outcome (console output, file existence, partial file contents, specific flag values).
    * Compare the "Actual Outcome" against the "Expected Outcome."
    * Mark the test case as **PASS** or **FAIL**. If **FAIL**, provide detailed steps to reproduce and observed discrepancies.
4. **Consolidate Findings:** After all test cases are executed, summarize the results:
    * Total number of test cases run.
    * Number of passed, failed, and skipped test cases.
    * List all failed test cases with a brief description of the failure.
5. **Final Recommendation:** Based on the test results, provide a recommendation on the project's readiness:
    * **Ready for Production:** All critical tests passed.
    * **Needs Further Development/Fixes:** Identified failures need addressing.
    * **Requires More Investigation:** Ambiguous results or unhandled edge cases.
</file>

<file path="docs/codegen-guide.md">
# Developer Guide: A Modern Approach to GraphQL Code Generation

This guide documents the principles and practices for using `graphql-codegen` to create a robust, maintainable, and type-safe data access layer for the GitHub API.

### The Core Philosophy: Operations are the Source of Truth

The most critical principle of type-safe GraphQL is this: **The shape of the data you receive is defined by your query, not the server's entire schema.**

Therefore, we **always** generate TypeScript types based on the specific `.graphql` documents our application actually uses. This ensures a perfect, compile-time match between the code and the data, eliminating an entire class of runtime errors.

---

### The Modern Stack: `client-preset`

We use the `client-preset` from GraphQL Code Generator. It is the modern, canonical way to set up codegen for any application that *consumes* a GraphQL API (like this CLI).

**Why the `client-preset` is superior:**

* **Simplicity:** It bundles the best plugins (`typescript`, `typescript-operations`) with an optimal configuration out of the box.
* **TypedDocumentNode:** Instead of just generating types, it produces a `TypedDocumentNode` for each operation. This is a highly optimized object that bundles the query string, its result type, and its variable types together.
* **Efficiency:** It prevents the need to parse GraphQL query strings at runtime.

---

### Our Configuration: `codegen.ts`

Our configuration is centralized in `codegen.ts`. It is designed to be simple and powerful.

```typescript
import type { CodegenConfig } from '@graphql-codegen/cli';

const config: CodegenConfig = {
  overwrite: true,
  // The schema is our single source of truth for what's possible.
  schema: 'schema/github-v15.26.0.graphql',
  // We tell codegen to find all our operations in this directory.
  documents: ['src/graphql/**/*.graphql'],
  prune: true,
  generates: {
    // We generate a single, cohesive set of artifacts here.
    'src/generated/': {
      preset: 'client',
      plugins: [
        {
          add: { content: '/* eslint-disable */' },
        },
      ],
      // All plugin configurations go under the 'config' key.
      config: {
        avoidOptionals: true,
        strictScalars: true,
        enumsAsTypes: true,
        // Map custom GraphQL scalars to simple TypeScript types.
        scalars: {
          DateTime: 'string',
          Date: 'string',
          URI: 'string',
          // ... and other custom GitHub scalars
        },
      },
    },
  },
  ignoreNoDocuments: true,
};

export default config;
```

---

### The Development Workflow

Follow this process to add or modify data fetching logic.

#### 1. Define Your Operation in a `.graphql` File

Your work starts here. All GraphQL queries, mutations, or fragments **must** be defined in `.graphql` files inside the `src/graphql/` directory.

**Example: `src/graphql/GetRepoData.graphql`**

```graphql
query GetRepoData($owner: String!, $name: String!) {
  repository(owner: $owner, name: $name) {
    name
    url
    description
    # ... other fields
  }
}
```

#### 2. Regenerate the SDK

After saving your `.graphql` file, run the codegen script. This will read your new operation and generate all the necessary types and the `TypedDocumentNode`.

```bash
npm run codegen
```

For an optimal developer experience, run it in watch mode during development:

```bash
npm run codegen -- --watch
```

#### 3. Integrate the Generated Code

This is the most important step. **Do not write GraphQL strings in your application code.** Instead, import the generated `TypedDocumentNode` and pass it directly to the GraphQL client.

**Correct Usage (`src/main.ts`):**

```typescript
// 1. Import the generated DocumentNode and any necessary types.
import { GetRepoDataDocument, GetRepoDataQuery } from './generated/graphql';
import { GraphQLClient } from 'graphql-request';

// 2. Initialize your client.
const client = new GraphQLClient('https://api.github.com/graphql', { /* ... */ });

// 3. Use the imported document directly in your API call.
//    The client understands this object and will infer all types automatically.
const variables = { owner: 'sigstore', name: 'cosign' };
const response = await client.request(GetRepoDataDocument, variables);

// `response` is now fully typed as GetRepoDataQuery!
// No manual typing is needed.
console.log(response.repository?.name);
```

By following this pattern, we guarantee that our application's data access is always synchronized with our GraphQL operations, which are in turn validated against the official GitHub schema.
</file>

<file path="docs/data-model.md">
# Data Model: Raw Responses vs Analyzed JSON

## Overview

This project produces two distinct data representations from GitHub API queries: **raw responses** (unprocessed API output) and **analyzed JSON** (computed domain model). Understanding the difference is critical for working with the output data.

## Raw Responses (`raw-responses.jsonl`)

### What It Is

The unmodified GraphQL API response from GitHub, exactly as returned by the API, with metadata wrapper.

### Structure

```jsonl
{"metadata": {...}, "response": {"data": {"repository": {...}}}}
```

Each line is a complete JSON object containing:
- `metadata` - Query context (queryType, timestamp, owner, repo, inputs)
- `response` - The full GitHub GraphQL response

### Characteristics

- **Direct API output** - No processing, computation, or transformation
- **GitHub's data model** - Nested structure with `nodes`, `edges`, pagination cursors
- **Complete information** - Everything GitHub returns (may include fields you don't use)
- **JSONL format** - One line per repository (newline-delimited JSON)
- **Includes metadata wrapper** - Tracks when and how data was fetched
- **Append behavior** - Historical record, adds to file on each run
- **Large files** - Lines can be 10KB-100KB+ (full response with 5 releases × N assets)
- **Not generated in mock mode** - Only created when fetching from real GitHub API

### Example Structure

```json
{
  "metadata": {
    "queryType": "GetRepoDataArtifacts",
    "timestamp": "2025-10-06T22:30:15.123Z",
    "owner": "sigstore",
    "repo": "cosign",
    "inputs": {"owner": "sigstore", "name": "cosign"}
  },
  "response": {
    "data": {
      "repository": {
        "name": "cosign",
        "nameWithOwner": "sigstore/cosign",
        "url": "https://github.com/sigstore/cosign",
        "description": "Code signing and transparency for containers and binaries",
        "owner": {
          "login": "sigstore"
        },
        "releases": {
          "nodes": [
            {
              "name": "v0.1.0",
              "tagName": "v0.1.0",
              "createdAt": "2021-03-20T00:22:31Z",
              "releaseAssets": {
                "nodes": [
                  {
                    "name": "cosign.sig",
                    "downloadUrl": "https://github.com/sigstore/cosign/releases/download/v0.1.0/cosign.sig"
                  }
                ]
              }
            }
          ]
        },
        "workflows": {
          "entries": [
            {
              "name": "build.yaml",
              "text": "... full YAML content ..."
            }
          ]
        }
      }
    }
  }
}
```

### Use Cases

- **Full data preservation** - Keep complete API responses for future reprocessing
- **Historical tracking** - Append behavior creates timeline of API responses
- **Debugging** - Inspect exact API responses when troubleshooting
- **Alternative analysis** - Reprocess data with different logic without requerying GitHub
- **API cost savings** - Avoid rate limits by reusing cached responses
- **Mock mode source** - Future enhancement to read from JSONL instead of individual files

## Analyzed JSON (`{dataset}-analyzed.json`)

### What It Is

Processed, domain-specific data model with computed classifications and security insights. This is the result of running raw API responses through `analyzeRepositoryData()` in `src/analysis.ts`.

### Structure

```json
[
  {
    "repository": {...},
    "releases": [...],
    "workflows": [...],
    "summary": {...}
  }
]
```

Top-level array of repository analysis objects.

### Characteristics

- **Transformed data** - Flattened, simplified, domain-specific structure
- **Computed fields** - Adds security classifications via regex pattern matching
- **Filtered information** - Only relevant supply chain security data
- **Array of repositories** - Clean JSON array, easy to query with jq/DuckDB
- **No metadata wrapper** - Pure domain objects
- **Overwrite behavior** - Current state only, not historical
- **Smaller files** - Flattened structure, only relevant data retained
- **Query-optimized** - Designed for analysis and reporting

### Example Structure

```json
[
  {
    "repository": {
      "id": "MDEwOlJlcG9zaXRvcnkzMzU5NTI0MTc=",
      "owner": "sigstore",
      "name": "cosign",
      "url": "https://github.com/sigstore/cosign"
    },
    "releases": [
      {
        "tagName": "v0.1.0",
        "name": "v0.1.0",
        "createdAt": "2021-03-20T00:22:31Z",
        "artifacts": [
          {
            "name": "cosign.sig",
            "isSbom": false,
            "isSignature": true,
            "isAttestation": false,
            "sbomFormat": "",
            "isVex": false,
            "isSlsaProvenance": false,
            "isInTotoLink": false,
            "isContainerAttestation": true,
            "downloadUrl": "https://github.com/sigstore/cosign/releases/download/v0.1.0/cosign.sig"
          }
        ]
      }
    ],
    "workflows": [
      {
        "name": "build.yaml",
        "detectedSbomTools": ["goreleaser", "signer"]
      }
    ],
    "summary": {
      "hasSbomArtifact": false,
      "hasSignatureArtifact": true,
      "hasAttestationArtifact": false,
      "sbomCiTools": ["goreleaser", "signer"]
    }
  }
]
```

### Use Cases

- **Security analysis** - Query for repos with specific security artifacts
- **Reporting** - Generate summaries and statistics
- **CSV generation** - Source data for flattened tabular format
- **Parquet conversion** - Input for DuckDB-based Parquet generation
- **Programmatic queries** - Easy to parse and filter with standard tools

## Key Transformations (Raw → Analyzed)

### 1. Structure Flattening

**Raw:**
```
repository.releases.nodes[].releaseAssets.nodes[]
```

**Analyzed:**
```
releases[].artifacts[]
```

Removes GraphQL connection wrappers (`nodes`, `edges`, `pageInfo`).

### 2. Security Classification

Applies regex patterns to detect artifact types:

**SBOM Detection:**
- Patterns: `/\b(sbom|spdx|cyclonedx)\b/i`
- Sets: `isSbom: true`, `sbomFormat: "spdx"|"cyclonedx"`

**Signature Detection:**
- Patterns: `/\.(sig|asc|pem|pub)$/i`
- Sets: `isSignature: true`

**Attestation Detection:**
- Patterns: `/attestation/i`, `/provenance/i`, `/\.intoto\.jsonl/i`
- Sets: `isAttestation: true`, `isSlsaProvenance: true`

**CI Tool Detection:**
- Scans workflow YAML for: `syft`, `trivy`, `cosign`, `goreleaser`, etc.
- Populates: `summary.sbomCiTools[]`

### 3. Field Simplification

**Removed:**
- GitHub API metadata (cursors, hasNextPage, totalCount)
- Deeply nested connection objects
- Unused fields

**Added:**
- Boolean classification flags
- Enum-like format identifiers
- Aggregated summary statistics

### 4. Summary Aggregation

Computes repository-level flags:

```typescript
summary: {
  hasSbomArtifact: boolean,        // Any artifact classified as SBOM
  hasSignatureArtifact: boolean,   // Any artifact classified as signature
  hasAttestationArtifact: boolean, // Any artifact classified as attestation
  sbomCiTools: string[]            // List of detected CI tools
}
```

## Processing Pipeline

```
1. Fetch from GitHub API
   ↓
2. Save to raw-responses.jsonl (append, with metadata)
   ↓
3. Pass response.data.repository to analyzeRepositoryData()
   ↓
4. Apply regex patterns for classification
   ↓
5. Flatten structure, remove GraphQL wrappers
   ↓
6. Generate summary statistics
   ↓
7. Save to {dataset}-analyzed.json (overwrite)
   ↓
8. Flatten to normalized rows
   ↓
9. Generate {dataset}.csv
   ↓
10. Convert analyzed JSON to Parquet with schema metadata
```

## File Comparison

| Aspect | Raw Responses | Analyzed JSON |
|--------|--------------|---------------|
| **Format** | JSONL (newline-delimited) | JSON array |
| **Source** | GitHub GraphQL API | analyzeRepositoryData() |
| **Structure** | Nested, with `nodes`/`edges` | Flattened arrays |
| **Size** | 10-100KB+ per line | Smaller, filtered |
| **Metadata** | Includes query metadata | No metadata wrapper |
| **Behavior** | Append (historical) | Overwrite (current state) |
| **Purpose** | Full preservation | Analysis-ready |
| **Computed fields** | None | Many (isSbom, etc.) |
| **Mock mode** | Not generated | Generated |

## Implementation Details

### Raw Response Writing

```typescript
// src/rawResponseWriter.ts
await appendRawResponse(rawResponsesPath, {
  queryType: 'GetRepoDataArtifacts',
  owner: repo.owner,
  repo: repo.name,
  response: repoData,
});
```

### Analysis

```typescript
// src/analysis.ts
export function analyzeRepositoryData(repo: unknown) {
  // Extract releases, apply patterns, compute flags
  const artifacts = releases.map(asset => ({
    name: asset.name,
    isSbom: ARTIFACT_KEYWORDS.SBOM.test(asset.name),
    isSignature: ARTIFACT_KEYWORDS.SIGNATURE.test(asset.name),
    // ... more classifications
  }));
  
  return {
    repository: { owner, name, url },
    releases: [...],
    workflows: [...],
    summary: {
      hasSbomArtifact: artifacts.some(a => a.isSbom),
      // ... more summaries
    }
  };
}
```

### Report Generation

```typescript
// src/report.ts
async function generateReports(results, outputDir, baseName, runMetadata) {
  // Write analyzed JSON
  const jsonPath = path.join(outputDir, `${baseName}-analyzed.json`);
  await fs.writeFile(jsonPath, JSON.stringify(results, null, 2));
  
  // Generate CSV from analyzed data
  // Generate Parquet from analyzed data
}
```

## Querying Examples

### Raw Responses

```bash
# Extract metadata from all responses
jq -c '.metadata' output/graduated-*/raw-responses.jsonl

# Get all responses for a specific repo
jq -c 'select(.metadata.repo == "cosign")' output/graduated-*/raw-responses.jsonl

# Count lines (= number of API calls)
wc -l output/graduated-*/raw-responses.jsonl
```

### Analyzed JSON

```bash
# Count repos with SBOMs
jq '[.[] | select(.summary.hasSbomArtifact == true)] | length' output/graduated-*/graduated-analyzed.json

# List all detected CI tools
jq -r '.[] | .summary.sbomCiTools[]' output/graduated-*/graduated-analyzed.json | sort -u

# Find repos using goreleaser
jq -r '.[] | select(.summary.sbomCiTools | contains(["goreleaser"])) | .repository.name' \
  output/graduated-*/graduated-analyzed.json
```

## Future Enhancements

### Phase 2: JSONL Mock Mode

Read from `raw-responses.jsonl` instead of individual `src/mockdata/*.json` files:

```typescript
// src/rawResponseWriter.ts - TODO
export async function readRawResponse(
  filePath: string,
  owner: string,
  repo: string
): Promise<unknown | null> {
  // Read JSONL, filter by owner/repo, return most recent
}
```

### Phase 3: Raw Responses to Parquet

Convert `raw-responses.jsonl` → `raw-responses.parquet` for efficient querying:

```sql
COPY (
  SELECT 
    metadata.queryType,
    metadata.timestamp,
    metadata.owner,
    metadata.repo,
    response
  FROM read_json_auto('raw-responses.jsonl', format='newline_delimited')
) TO 'raw-responses.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);
```

## Summary

Raw responses preserve complete API output for historical tracking and future reprocessing. Analyzed JSON provides a domain-specific, query-optimized model with computed security classifications. Both serve distinct purposes in the data pipeline and should be preserved for different use cases.
</file>

<file path="docs/design.md">
# System Design

## Overview

The GitHub Supply Chain Security Analyzer is a data collection and analysis tool that searches GitHub repositories for the **presence** of supply chain security artifacts. It does NOT validate or analyze the content of these artifacts - it simply reports what exists.

**Target Use Case**: Analyzing CNCF landscape projects to understand adoption of supply chain security practices (SBOMs, signatures, attestations, SLSA provenance, etc.).

**Core Philosophy**:

- Presence detection only - not content validation
- Raw data preservation for reproducibility
- Multiple output formats for different consumers
- API-efficient with targeted queries

## Query Architecture

The tool uses a **two-query design** to balance API efficiency with flexibility.

### GetRepoDataArtifacts (Default)

Lightweight query fetching:

- Repository metadata (name, owner, id)
- Last 5 releases with up to 50 artifacts each
- Basic release info (tag, name, created date)

**Use Case**: Default mode. Fast presence detection of security artifacts in releases.

### GetRepoDataExtendedInfo (--extended flag)

Comprehensive query adding:

- GitHub Actions workflows (YAML content)
- Security policies (SECURITY.md)
- Branch protection settings
- Dependabot configuration
- Repository metadata (URL, description, license)

**Use Case**: Deep analysis of CI/CD security tooling. Detects security tools in workflows (syft, trivy, cosign, goreleaser, etc.).

**Status**: ✅ Implemented. Use `--extended` flag to enable.

**Why Two Queries?**

- Rate limits matter - don't fetch workflow YAML if you only need artifacts
- Each query returns a distinct type (no union types, no runtime guards)
- Easy to extend - just add another query file and API function

See [QUERY-ARCHITECTURE.md](docs/QUERY-ARCHITECTURE.md) for implementation details.

## Data Pipeline

The tool processes data through three distinct stages:

```text
┌─────────────────────┐
│  GitHub GraphQL API │
└──────────┬──────────┘
           │
           │ Raw Response (nested GraphQL structure)
           │
           ▼
┌─────────────────────┐
│   analysis.ts       │  ← Pattern matching & classification
│   analyzeRepo()     │
└──────────┬──────────┘
           │
           │ Analyzed Domain Model (computed fields)
           │
           ▼
┌─────────────────────┐
│   report.ts         │  ← Format conversion & normalization
│   generateReports() │
└──────────┬──────────┘
           │
           │ Multiple output formats
           │
           ▼
     Output Artifacts
```

### Stage 1: Raw API Response

**Structure**: Nested GraphQL response with pagination nodes and type wrappers.

**Example**:

```json
{
  "data": {
    "repository": {
      "id": "MDEwOlJlcG9zaXRvcnk...",
      "name": "cosign",
      "nameWithOwner": "sigstore/cosign",
      "releases": {
        "nodes": [
          {
            "tagName": "v0.1.0",
            "name": "v0.1.0",
            "createdAt": "2021-03-20T00:22:31Z",
            "releaseAssets": {
              "nodes": [
                { "name": "cosign", "downloadUrl": "https://..." },
                { "name": "cosign.sig", "downloadUrl": "https://..." }
              ]
            }
          }
        ]
      }
    }
  }
}
```

**Current State**: ✅ Saved to `{dataset}-raw-responses.jsonl` with metadata (timestamp, queryType, owner, repo)

### Stage 2: Analyzed Domain Model

**Purpose**: Transform raw GraphQL into business domain objects with computed fields.

**Processing** (in `analysis.ts`):

- Extract owner from `nameWithOwner`
- Flatten GraphQL pagination nodes
- Classify artifacts by name patterns (regex matching):
  - `isSbom`, `isSignature`, `isAttestation`
  - `sbomFormat` (spdx, cyclonedx, unknown)
  - `isVex`, `isSlsaProvenance`, `isInTotoLink`, `isContainerAttestation`
- Compute summary flags (presence of each artifact type)
- Collect unique security tool types into `sbomCiTools` array

**Example**:

```json
{
  "repository": {
    "id": "MDEwOlJlcG9zaXRvcnk...",
    "owner": "sigstore",
    "name": "cosign",
    "url": "https://github.com/sigstore/cosign"
  },
  "releases": [
    {
      "tagName": "v0.1.0",
      "name": "v0.1.0",
      "createdAt": "2021-03-20T00:22:31Z",
      "artifacts": [
        {
          "name": "cosign",
          "isSbom": false,
          "isSignature": false,
          "isAttestation": false,
          "sbomFormat": "",
          "downloadUrl": "https://..."
        },
        {
          "name": "cosign.sig",
          "isSbom": false,
          "isSignature": true,
          "isAttestation": false,
          "sbomFormat": "",
          "downloadUrl": "https://..."
        }
      ]
    }
  ],
  "workflows": [],
  "summary": {
    "hasSbomArtifact": false,
    "hasSignatureArtifact": true,
    "hasAttestationArtifact": false,
    "sbomCiTools": ["signature"]
  }
}
```

**Current State**: ✅ Generated and saved to `{dataset}-analyzed.json`

### Stage 3: Normalized CSV

**Purpose**: Flatten hierarchical JSON into wide tabular format for spreadsheet/SQL analysis.

**Processing** (in `report.ts`):

- One row per artifact
- Repository and release info duplicated across artifact rows
- Nested arrays encoded as JSON strings
- Additional computed fields:
  - `artifact_type` classification
  - `platform_hint` extraction
  - `size_hint` extraction
  - `release_has_container_images`, `release_has_slsa_provenance`

**Current State**: ✅ Generated and saved to `{dataset}.csv`

## Output Artifacts

### Output Files

Each run produces **four files** per dataset:

1. **`{dataset}-raw-responses.jsonl`** - Raw GraphQL API responses with metadata
   - One line per API fetch (appends on each run for historical tracking)
   - Each line: `{"metadata": {...}, "response": {...}}`
   - Metadata includes:
     - `timestamp`: When the data was fetched (ISO 8601)
     - `queryType`: Which GraphQL query was used (GetRepoDataArtifacts or GetRepoDataExtendedInfo)
     - `owner`: Repository owner
     - `repo`: Repository name
   - **Append behavior**: New runs add to existing file, preserving history

2. **`{dataset}-analyzed.json`** - Analyzed domain model (array of repository analysis objects)
   - Computed fields and classifications
   - **Overwrite behavior**: Current state only, not historical

3. **`{dataset}.csv`** - Normalized flat CSV (one row per artifact)
   - For spreadsheet analysis, SQL imports, etc.
   - **Overwrite behavior**: Current state only

4. **`{dataset}-schema.json`** - Parquet-style schema documentation
   - Field types, descriptions, examples, categories
   - Used for documentation generation
   - Enables future JSON → Parquet conversion via external tools
   - **Overwrite behavior**: Current state only

**Example JSONL line**:

```json
{"metadata": {"queryType": "GetRepoDataArtifacts", "timestamp": "2025-10-06T17:30:00Z", "owner": "sigstore", "repo": "cosign", "inputs": {"owner": "sigstore", "name": "cosign"}}, "response": {"data": {"repository": { ... }}}}
```

**Benefits**:

- **Historical tracking**: See how repos change over time
- **Reproducibility**: Re-analyze old data without re-fetching
- **Debugging**: See exactly what GitHub returned
- **Audit trail**: Know when data was collected
- **Data validation**: Verify analysis logic against raw responses
- **Mock data source**: Becomes input for mock mode (see below)

### File Naming Convention

```text
output/
  {dataset}-raw-responses.jsonl     # Raw API responses with metadata (APPEND)
  {dataset}-analyzed.json           # Analyzed domain model (OVERWRITE)
  {dataset}.csv                     # Normalized CSV (OVERWRITE)
  {dataset}-schema.json             # Schema documentation (OVERWRITE)
```

Examples:

- `sandbox-raw-responses.jsonl`
- `sandbox-analyzed.json`
- `sandbox.csv`
- `sandbox-schema.json`

## Mock Data Strategy

### Current Implementation

Mock mode (`--mock` flag) reads from `src/mockdata/` directory:

- Individual files: `GetRepoData_{owner}_{repo}.json`
- Contains full raw API responses
- 8 hand-picked repos for testing specific scenarios

**Limitations**:

- Separate from production output
- No connection to actual captured data
- Manual maintenance required

### Future: Two-Tier System (Not Yet Implemented)

#### Tier 1: JSONL Bulk Data (TODO)

- Mock mode would read from `output/{dataset}-raw-responses.jsonl` first
- Filter by owner/repo to find matching response
- Use most recent entry for that repo
- **Real captured data becomes mock data automatically**

#### Tier 2: Individual Files (Current)

- Currently: Only reads from `src/mockdata/GetRepoData_{owner}_{repo}.json`
- Useful for curated test cases committed to git
- Specific test scenarios (e.g., "repo with SLSA provenance")

### Current Mock Resolution Order

When running with `--mock`:

1. Read from `src/mockdata/GetRepoData_{owner}_{repo}.json`
2. Error if not found

**Future Enhancement**: Add JSONL reading as Tier 1 fallback.

### Export-Mocks Subcommand (Phase 2)

New subcommand to materialize JSONL → individual files:

```bash
npm start -- export-mocks \
  --source output/test-single-raw-responses.jsonl \
  --dest src/mockdata/
```

**Use Cases**:

- Commit curated test fixtures to git
- Share example responses in documentation
- CI/CD with specific test scenarios
- Smaller git diffs (individual files vs. large JSONL)

## Implementation Status

### Completed Features ✅

✅ **Query Architecture**

- Two queries defined (`GetRepoDataArtifacts`, `GetRepoDataExtendedInfo`)
- GraphQL codegen generates TypeScript types
- API client with error handling and rate limit detection
- Both queries fully implemented and functional

✅ **Extended Query Support**

- `--extended` flag enables GetRepoDataExtendedInfo
- Fetches workflows, security policies, branch protection
- Workflow analysis detects CI tools (syft, trivy, cosign, goreleaser, etc.)
- CI_TOOL_KEYWORDS patterns in analysis.ts

✅ **Type System**

- **No type unions**: Removed complex union types
- Uses `unknown` with runtime type guards
- Flexible: handles arbitrary GraphQL response structures
- Decoupled from specific query schemas

✅ **Data Collection**

- `fetchRepositoryArtifacts()` and `fetchRepositoryExtendedInfo()` in api.ts
- Sequential and parallel execution modes
- Batch processing to avoid rate limits
- Mock mode support (reads from src/mockdata/)

✅ **Analysis Pipeline**

- `analyzeRepositoryData()` accepts `unknown` and uses runtime checks
- Classifies artifacts by regex patterns
- Detects security tools in workflows
- Computes summary flags

✅ **Raw API Response Preservation**

- Saves to `{dataset}-raw-responses.jsonl` with metadata
- Includes timestamp, queryType, owner, repo
- Appends on each run for historical tracking
- Full reproducibility and audit trail

✅ **Report Generation**

- Analyzed JSON output (renamed to `-analyzed.json`)
- Normalized CSV with one row per artifact
- Schema documentation (Parquet-style)

✅ **Console Output**

- Dual-table view:
  1. **Repository Summary**: One row per repo with rollup stats
  2. **Detailed Release View**: One row per release with artifact counts
- Shows SBOM, signature, attestation presence
- Shows CI tool detection results

✅ **Mock Data**

- 8 example repos in src/mockdata/
- `--mock` flag reads from individual files

### Remaining Work (Future)

❌ **Mock Data Integration**

- Mock mode doesn't yet read from JSONL files
- Still uses only src/mockdata/ individual files
- **TODO**: Add JSONL reading as Tier 1 fallback

❌ **Export-Mocks Subcommand**

- No way to materialize JSONL → individual files
- Useful for git commits, CI/CD, documentation
- **TODO**: Add `npm start -- export-mocks` command

❌ **Data Lineage Tracking**

- No way to know which raw responses produced which analyzed data
- Could add `raw_response_id` field linking to JSONL line number
- **TODO**: Add lineage metadata

❌ **Incremental Updates**

- No detection of changed repos to avoid re-fetching
- Could use timestamps in JSONL to skip recent fetches
- **TODO**: Add smart re-fetch logic

## Data Flow Diagram

```text
┌──────────────────────────────────────────────────────────────────────────┐
│                         GitHub GraphQL API                                │
└────────────────────────────────┬─────────────────────────────────────────┘
                                 │
                                 │ HTTP Request
                                 │
                    ┌────────────▼────────────┐
                    │      api.ts             │
                    │  fetchRepositoryXxx()   │
                    └────────────┬────────────┘
                                 │
                ┌────────────────┼────────────────┐
                │                │                │
                │ Raw Response   │                │ (Phase 1: Save to JSONL)
                │                │                │
                ▼                ▼                ▼
         ┌─────────────┐  ┌─────────────┐  ┌──────────────────────┐
         │ analysis.ts │  │   main.ts   │  │ {dataset}-raw-       │
         │ analyzeRepo │  │             │  │   responses.jsonl    │
         └──────┬──────┘  └──────┬──────┘  │  (with metadata)     │
                │                │          └──────────────────────┘
                │ Analyzed       │
                │ Domain Model   │
                │                │
                ▼                ▼
         ┌──────────────────────────┐
         │      report.ts           │
         │  generateReports()       │
         └──────┬───────────────────┘
                │
    ┌───────────┼───────────┬───────────┐
    │           │           │           │
    ▼           ▼           ▼           ▼
┌─────────┐ ┌─────────┐ ┌─────────┐ ┌────────────┐
│ .json   │ │ .csv    │ │ -schema │ │ (Mock Mode)│
│ analyzed│ │ flat    │ │ .json   │ │ reads JSONL│
└─────────┘ └─────────┘ └─────────┘ └────────────┘
```

## Configuration & Input

**Input**: JSONL file with one repository per line

```json
{"owner": "sigstore", "name": "cosign"}
{"owner": "fluxcd", "name": "flux2"}
```

**Datasets**: Pre-configured input files

- `input/sandbox.jsonl` - CNCF sandbox projects
- `input/incubation.jsonl` - CNCF incubating projects  
- `input/graduated.jsonl` - CNCF graduated projects
- `input/test-*.jsonl` - Test fixtures

**Environment Variables**:

- `GITHUB_PAT` - GitHub Personal Access Token (required unless `--mock`)
- `MOCK_GITHUB=1` - Alternative to `--mock` flag

**CLI Flags**:

- `--input <file>` - Input JSONL file (default: input/sandbox.jsonl)
- `--output <dir>` - Output directory (default: output/)
- `--mock` - Use mock data instead of API calls
- `--parallel [batchSize]` - Parallel execution with batch size (default: 10)
- `--verbose` - Detailed logging

## Type System Architecture

**Design Philosophy**: Accept arbitrary JSON structures, check at runtime.

### No Type Unions

Previously tried:

```typescript
// ❌ Don't do this
type Repository = RepositoryArtifacts | RepositoryExtended;
```

Problems:

- Couples analysis to specific GraphQL schemas
- Breaks when GraphQL types change
- Requires complex type guards
- Not truly flexible

### Runtime Checks with Unknown

Current approach:

```typescript
// ✅ Do this
export function analyzeRepositoryData(repo: unknown) {
  if (!repo || typeof repo !== 'object') return null;
  const repoObj = repo as Record<string, unknown>;
  
  // Safe extraction with runtime checks
  const releases = repoObj.releases as Record<string, unknown> | undefined;
  const nodes = releases?.nodes;
  if (Array.isArray(nodes)) {
    // Work with what we find
  }
}
```

Benefits:

- Works with any GraphQL response structure
- Decoupled from auto-generated types
- Natural duck typing: "if it has the fields, use them"
- Easy to extend with new queries
- Honest about dynamic data

### Workflow Detection Example

```typescript
// Extract workflows if present (extended query only)
if ('workflows' in repoObj && repoObj.workflows && typeof repoObj.workflows === 'object') {
  const workflows = repoObj.workflows as Record<string, unknown>;
  if ('entries' in workflows && Array.isArray(workflows.entries)) {
    // Analyze workflow YAML for security tools
  }
}
```

## Console Output

The tool displays results in two tables:

### 1. Repository Summary

One row per repository with aggregate statistics:

```text
┌─────────────────┬──────────┬──────────┬──────────┬────────────────┐
│ Repository      │ Releases │ SBOM     │ Sig      │ Total Artifacts│
├─────────────────┼──────────┼──────────┼──────────┼────────────────┤
│ sigstore/cosign │ 5        │          │ ✔        │ 30             │
│ anchore/syft    │ 5        │          │ ✔        │ 30             │
│ github/docs     │ 1        │          │          │                │
└─────────────────┴──────────┴──────────┴──────────┴────────────────┘
```

Shows:

- Repository name (owner/repo)
- Number of releases analyzed
- Presence of SBOM, signatures, attestations (✔ if any release has them)
- Total artifact count across all releases
- CI tool detection (SBOM CI, Sign CI, GoRel CI)

### 2. Detailed Release View

One row per release with per-release artifact counts:

```text
┌─────────────┬────────────┬──────────┬──────────┬────────────┐
│ Repo        │ Release    │ SBOM     │ Sig      │ Artifacts  │
├─────────────┼────────────┼──────────┼──────────┼────────────┤
│ cosign      │ v0.4.0     │          │ ✔        │ 9          │
│             │ v0.3.1     │          │ ✔        │ 6          │
│             │ v0.3.0     │          │ ✔        │ 6          │
└─────────────┴────────────┴──────────┴──────────┴────────────┘
```

Shows:

- Repository name (only on first release)
- Release tag
- Per-release presence indicators
- Artifact count for this specific release

## Next Steps

### Immediate

1. ✅ ~~Add raw response JSONL output with metadata~~ DONE
2. ✅ ~~Rename analyzed output file~~ DONE  
3. ✅ ~~Implement extended query support~~ DONE
4. ✅ ~~Add workflow analysis~~ DONE
5. ✅ ~~Add dual-table console output~~ DONE
6. Update mock mode to read JSONL
7. Run on all graduated projects

### Future Enhancements

1. Create export-mocks subcommand
2. Add data lineage tracking
3. Implement incremental update detection
4. Add filtering/search in detailed view
5. Export tables to markdown/HTML

### Documentation Updates

- ✅ DESIGN.md - Updated to reflect current state
- README.md - Update output artifacts section
- HACKING.md - Update with type system approach
- schema.md - Regenerate after field changes
</file>

<file path="docs/github-api-limits.md">
# GitHub GraphQL API Rate Limits: A Primer

This document provides essential information about GitHub's GraphQL API rate limits and how this tool avoids being flagged as abusive or triggering secondary rate limits.

**Official Documentation**: [GitHub GraphQL API Rate Limits](https://docs.github.com/en/graphql/overview/rate-limits-and-query-limits-for-the-graphql-api)

---

## Overview

GitHub enforces multiple types of limits to protect API availability and prevent abuse:

1. **Primary Rate Limit**: Point-based quota per hour
2. **Secondary Rate Limits**: Multiple constraints on request patterns
3. **Node Limits**: Maximum objects per query
4. **Timeouts**: 10-second maximum query execution time
5. **Resource Limits**: CPU time and compute constraints

---

## Primary Rate Limit

### Points Quota

Each authenticated user gets **5,000 points per hour**. Queries consume points based on their complexity.

**How points are calculated:**

1. Count the number of API requests needed to fulfill all connections
2. Divide by 100 and round to get the point cost
3. Minimum cost is 1 point per query

**Our queries:**

- `GetRepoDataArtifacts`: ~1-2 points (fetches releases with artifacts)
- `GetRepoDataExtendedInfo`: ~2-3 points (adds workflows, security policies, branch protection)

**Example:** Processing 100 repositories in parallel mode:

- With artifacts query: ~100-200 points
- With extended query: ~200-300 points
- **Both well under the 5,000/hour limit**

### Checking Rate Limit Status

GitHub returns headers with every response:

```text
x-ratelimit-limit: 5000
x-ratelimit-remaining: 4800
x-ratelimit-used: 200
x-ratelimit-reset: 1696867200
```

**Our implementation:** We log these headers when errors occur, helping you diagnose rate limit issues.

---

## Secondary Rate Limits (The Real Risk)

Secondary rate limits are **undocumented abuse detection mechanisms** that can flag aggressive clients. These are the ones that get you (and your friends) blocked!

### Key Constraints

1. **Concurrent Requests Limit**
   - **Maximum: 100 concurrent requests** across REST + GraphQL
   - **Violation risk:** Parallel mode with large batches
   - **Our mitigation:** Default batch size of 5 (configurable via `--parallel=N`)

2. **Per-Minute Point Limit**
   - **Maximum: 2,000 points per minute** for GraphQL endpoint
   - **Violation risk:** Sustained high-volume parallel execution
   - **Our mitigation:**
     - 1-second delay between batches
     - Small batch sizes (5 default, max 20 recommended)

3. **CPU Time Limit**
   - **Maximum: 90 seconds CPU time per 60 seconds real time**
   - **GraphQL subset: 60 seconds CPU time max**
   - **Violation risk:** Complex queries (extended mode with large repos)
   - **Our mitigation:** Keep queries focused on specific fields, limit node counts

4. **Excessive Compute Resources**
   - **Undisclosed limits** on queries consuming too many resources in short bursts
   - **Violation risk:** Rapidly firing many extended queries
   - **Our mitigation:** Sequential mode available for sensitive environments

### What Happens When You Violate Secondary Limits

- **Response:** Status `200` or `403` with error message
- **Headers:** May include `retry-after` (seconds to wait)
- **Consequence:** Continued violations can result in **integration banning**
- **Recovery:** Exponential backoff, wait at least 1 minute between retries

---

## Tool Design: Safe Defaults

This tool is designed to **stay well under all limits by default** while still being fast.

### Default Configuration

| Setting | Default Value | Rationale |
|---------|--------------|-----------|
| Execution Mode | Sequential | Safest, easiest to debug, ordered output |
| Parallel Batch Size | 5 repos | Well under 100 concurrent limit |
| Batch Delay | 1 second | Prevents burst detection |
| Max Node Requests | ~50 releases/repo | Conservative to avoid resource limits |

### Recommended Settings by Dataset Size

**Small datasets (1-20 repos):**

```bash
# Sequential is fine, takes <1 minute
npm start -- --input input/test-single.jsonl
```

**Medium datasets (20-100 repos):**

```bash
# Parallel with default batch size (5)
npm start -- --input input/graduated.jsonl --parallel
```

**Large datasets (100-500 repos):**

```bash
# Parallel with small batches, extended mode adds minimal overhead
npm start -- --input input/incubation.jsonl --parallel=3 --extended
```

**Very large datasets (500+ repos):**

```bash
# Sequential or very small batches, consider splitting input file
npm start -- --input input/all-repos.jsonl --parallel=2 --extended

# OR split into multiple runs:
npm start -- --input input/all-repos-part1.jsonl --parallel=5
npm start -- --input input/all-repos-part2.jsonl --parallel=5
```

---

## Understanding Parallel Mode

### How Batching Works

When you run with `--parallel=5`:

1. Load all repositories from input file (e.g., 100 repos)
2. Split into batches of 5: [batch1, batch2, batch3, ..., batch20]
3. For each batch:
   - Fire 5 concurrent API requests
   - Wait for all 5 to complete (`Promise.allSettled`)
   - Log results
   - **Wait 1 second before next batch**
4. Continue until all batches processed

### Why This Is Safe

- **Concurrent requests:** Max 5 at a time (far below 100 limit)
- **Per-minute points:** ~10-15 points/minute (well below 2,000 limit)
- **CPU time:** Small queries with pauses stay well within bounds
- **Resource consumption:** Gradual, not bursty

### When to Use Sequential

Use `--no-parallel` (sequential mode) when:

- **Debugging:** Need ordered, readable console output
- **Shared IP:** Multiple people using the same network/PAT
- **Conservative environments:** High security, strict monitoring
- **Rate limit concerns:** Already close to limits from other tools

---

## Query Optimization

Our queries are optimized to minimize point cost and resource usage:

### Artifacts Query (`GetRepoDataArtifacts`)

```graphql
query GetRepoDataArtifacts($owner: String!, $name: String!) {
  repository(owner: $owner, name: $name) {
    # Basic fields (no additional requests)
    name
    url
    description
    
    # Single connection: last 5 releases
    releases(last: 5, orderBy: {field: CREATED_AT, direction: DESC}) {
      nodes {
        tagName
        name
        createdAt
        
        # Single nested connection: first 100 assets
        releaseAssets(first: 100) {
          nodes {
            name
            downloadUrl
          }
        }
      }
    }
  }
}
```

**Point calculation:**

- 1 repository request
- 5 release requests (implicit)
- 5 × 100 = 500 asset requests (implicit)
- Total: ~506 requests → **~5 points** (506/100 rounded)

### Extended Query (`GetRepoDataExtendedInfo`)

Adds:

- Workflows (last 50)
- Security policy (1 file)
- Branch protection (1 query)
- Dependabot config (1 file)

**Additional point cost:** ~1-2 points

**Total for extended:** ~6-7 points per repository

---

## Best Practices for Large Runs

### 1. Start Small

Always test with a small dataset first:

```bash
npm start -- --input input/test-single.jsonl --mock  # No API calls
npm start -- --input input/test-three-repos.jsonl    # 3 real API calls
```

### 2. Monitor Rate Limits

Watch the console output for rate limit warnings:

```text
⚠️  API Error: [...]
  Rate Limit Remaining: 4500
  Rate Limit Resets At: 3:45:00 PM
```

### 3. Use Mock Mode for Development

```bash
npm start -- --input input/test-single.jsonl --mock
```

No API calls, instant results, test analysis logic.

### 4. Adjust Batch Size Conservatively

```bash
# If you get secondary rate limit errors:
npm start -- --parallel=2        # Reduce from default 5
npm start -- --no-parallel       # Switch to sequential
```

### 5. Pause Between Large Runs

If running multiple datasets back-to-back:

```bash
npm start -- --input input/graduated.jsonl --parallel=5
sleep 60  # Wait 1 minute before next run
npm start -- --input input/incubation.jsonl --parallel=5
```

### 6. Check GitHub Status

If you encounter unexpected errors: [githubstatus.com](https://www.githubstatus.com/)

---

## Troubleshooting

### "You have exceeded a secondary rate limit"

**Cause:** Too many concurrent requests or too many requests per minute.

**Solution:**

```bash
# Reduce batch size
npm start -- --parallel=2 --input input/yourfile.jsonl

# Or switch to sequential
npm start -- --no-parallel --input input/yourfile.jsonl
```

### "API request timed out"

**Cause:** Query took >10 seconds to execute (usually large repos with many releases).

**Solution:**

- This is rare with our queries (we limit to 5 releases)
- If it happens, the tool will skip that repo and continue
- Try again later or exclude that specific repo

### "We couldn't process your request"

**Cause:** Resource limits exceeded (CPU time, compute resources).

**Solution:**

- Use sequential mode: `--no-parallel`
- Avoid extended mode for very large repos
- Split input file into smaller chunks

### Rate limit remaining is 0

**Cause:** Used all 5,000 points this hour.

**Solution:**

- Wait until `x-ratelimit-reset` timestamp
- Check if other tools/scripts are using your PAT
- Consider using a different PAT if available

---

## Summary: Safe Operation Guidelines

✅ **DO:**

- Use default settings for most datasets (sequential or `--parallel=5`)
- Test with small datasets first
- Monitor console output for rate limit warnings
- Use mock mode for development
- Wait between large runs

❌ **DON'T:**

- Use batch sizes >20 (`--parallel=20` is aggressive)
- Run multiple instances simultaneously with same PAT
- Ignore rate limit warnings
- Fire rapid back-to-back runs without pauses
- Use in CI/CD without rate limit awareness

**Default configuration is production-safe:** Sequential mode with optional parallel mode (batch size 5) keeps you well under all limits while still being reasonably fast.

---

**Questions or issues?** Open an issue with:

- Input file size (number of repos)
- Command used (including flags)
- Error message and rate limit headers
- Whether it's a shared PAT/network
</file>

<file path="docs/HACKING.md">
# HACKING.md - Development Guide

## Quick Start for Contributors

This repository analyzes GitHub repositories for supply chain security metadata. The tool fetches data via GraphQL, processes it through enhanced detection patterns, and outputs normalized data for analysis.

## Schema Documentation Management

### Automatic Schema Documentation

We maintain automated documentation of our data schema to ensure consistency and enable data analysis workflows:

```bash
# Generate schema documentation
npm run docs:schema

# Or run directly
./scripts/generate-schema-docs.sh
```

This will:
1. Generate `schema.md` with field definitions and types
2. Create `schema.json` for programmatic access
3. Update data dictionary for analysts

### When to Update Schema Documentation

Run schema documentation generation whenever you modify:
- GraphQL queries (`src/graphql/GetRepoData.graphql`)
- Analysis logic (`src/analysis.ts`) 
- Report structure (`src/report.ts`)
- Artifact detection patterns

## Development Workflow

### 1. Setup
```bash
npm install
npm run codegen  # Generate GraphQL types
```

### 2. Making Changes

**GraphQL Schema Updates:**
1. Modify `src/graphql/GetRepoData.graphql`
2. Run `npm run codegen`
3. Update TypeScript types as needed
4. Run `npm run docs:schema`

**Analysis Enhancement:**
1. Update detection patterns in `src/analysis.ts`
2. Add test cases for new patterns
3. Run `npm run docs:schema`

**Output Format Changes:**
1. Modify `src/report.ts`
2. Update `NormalizedRow` type definition
3. Run `npm run docs:schema`

### 3. Testing
```bash
# Test with single repository
npm run test:single

# Test with multiple repositories  
npm run test:three

# Test with all CNCF data
npm run test:all
```

### 4. Documentation
```bash
# Update schema documentation
npm run docs:schema

# Verify README is current
npm run docs:verify
```

## Key Files and Their Roles

### Core Application

- `src/main.ts` - CLI entry point and orchestration
- `src/api.ts` - GitHub GraphQL API client
- `src/analysis.ts` - Artifact detection and classification
- `src/report.ts` - Report generation (analyzed JSON, CSV, Parquet schema)
- `src/rawResponseWriter.ts` - Raw API response logging to JSONL with metadata

### Data Schema

- `src/graphql/GetRepoData.graphql` - GraphQL query definition
- `src/generated/` - Auto-generated TypeScript types
- `schema/` - GraphQL schema files

### Configuration

- `src/config.ts` - Application configuration
- `.env` - Environment variables (create from .env.example)

## Supply Chain Security Features

### Enhanced Artifact Detection

The tool detects modern supply chain security artifacts:

```typescript
// Regex patterns to identify potential SBOM/security artifacts in release asset names
const ARTIFACT_KEYWORDS = {
  // Legacy patterns for backward compatibility
  SBOM: /\b(sbom|spdx|cyclonedx)\b/i,
  SIGNATURE: /\.(sig|asc|pem|pub)$/i,
  ATTESTATION: /attestation/i,
  
  // Enhanced supply chain security patterns
  SPDX_SBOM: /\b(spdx|\.spdx)\b/i,
  CYCLONEDX_SBOM: /\b(cyclonedx|cdx|\.cdx)\b/i,
  VEX_DOCUMENT: /\b(vex|\.vex)\b/i,
  SLSA_PROVENANCE: /\b(provenance|slsa|\.intoto\.jsonl)\b/i,
  IN_TOTO_LINK: /\b(link|\.link)\b/i,
  IN_TOTO_LAYOUT: /\b(layout|\.layout)\b/i,
  CONTAINER_ATTESTATION: /\b(cosign|rekor|fulcio)\b/i,
  LICENSE_FILE: /\b(license|copying|notice)\b/i,
};
```

### CI/CD Security Tool Detection

Identifies security tools in CI pipelines:


```typescript
// Regex patterns to identify security tools in GitHub Actions workflow YAML content
const CI_TOOL_KEYWORDS = {
  SBOM_GENERATORS: /\b(syft|trivy|cdxgen|spdx-sbom-generator)\b/i,
  SIGNERS: /\b(cosign|sigstore|slsa-github-generator)\b/i,
  GORELEASER: /\b(goreleaser\/goreleaser-action)\b/i, // Goreleaser can generate SBOMs
  
  // Enhanced security tool detection
  VULNERABILITY_SCANNERS: /\b(snyk|anchore|twistlock|aqua|clair)\b/i,
  DEPENDENCY_SCANNERS: /\b(dependabot|renovate|whitesource|fossa)\b/i,
  CODE_SCANNERS: /\b(codeql|semgrep|bandit|eslint-security)\b/i,
  CONTAINER_SCANNERS: /\b(docker-scout|grype|trivy)\b/i,
};
```

## Contributing Guidelines

1. Always run `npm run docs:schema` after modifications to queries
2. Maintain strict TypeScript compliance - no warnings!
3. Add test cases for new detection patterns
4. Keep HACKING.md current

## Troubleshooting

```bash
npm install 
```

```bash
# Regenerate types after .graphql schema changes
npm run codegen
```

```bash
# Force regeneration
npm run docs:schema --force
```

## Security Notes

- API tokens should be scoped to minimum required permissions
- .env file should not be committed to version control
</file>

<file path="docs/output-architecture.md">
# Output Architecture: Timestamped Runs + Parquet Generation

## Overview

This document describes the output file structure and Parquet generation strategy implemented in the GitHub Supply Chain Security Analyzer.

## Timestamped Run Directories

### Structure

Each run creates a timestamped directory:

```
output/
  graduated-2025-10-06T22-30-15/
    raw-responses.jsonl           # Raw GraphQL API responses (JSONL)
    graduated-analyzed.json       # Analyzed domain model
    graduated.csv                 # Normalized flat CSV
    graduated-schema.json         # Schema documentation with field descriptions
    graduated-analyzed.parquet    # Parquet with embedded metadata
```

### Benefits

- No file collisions between multiple runs
- Preserves history - every run is kept separately
- Easy comparison - diff between two run directories
- Clean organization - all outputs for one run in one place

### Timestamp Format

ISO 8601 with colons replaced: `2025-10-06T22-30-15`
- Sortable lexicographically
- Human-readable
- Filesystem-safe (no colons)

## Parquet Generation with DuckDB

### Why DuckDB?

- Stable and production-ready (used by many data tools)
- Simple SQL interface for JSON → Parquet conversion
- Built-in key-value metadata support (embeds schema docs)
- Handles nested JSON automatically
- ZSTD compression for smaller files

### Schema Metadata Preservation

We embed field descriptions from `{dataset}-schema.json` into the Parquet file's `KV_METADATA`.

Example schema.json:

```json
```json
{
  "fields": {
    "repository_name": {
      "type": "string",
      "description": "Name of the repository",
      "category": "repository"
    },
    "artifact_is_sbom": {
      "type": "boolean",
      "description": "Boolean indicating if artifact is a Software Bill of Materials",
      "category": "artifacts"
    }
  }
}
```

Embedded in Parquet as KV_METADATA:

```sql
SELECT * FROM parquet_kv_metadata('graduated-analyzed.parquet');

-- Returns:
key                         | value
---------------------------|----------------------------------------------
schema_title                | GitHub Supply Chain Security Analysis Schema
schema_version              | 1.0.0
run_query_type              | GetRepoDataArtifacts
run_timestamp               | 2025-10-06T22-30-15
run_total_repos             | 210
run_successful_repos        | 208
run_failed_repos            | 2
field_repository_name       | Name of the repository
field_artifact_is_sbom      | Boolean indicating if artifact is a Software Bill of Materials
...
```

### Implementation: `src/parquetWriter.ts`

Key functions:

1. `generateParquetFiles()` - Main entry point, reads schema.json and analyzed JSON, orchestrates DuckDB conversion

2. `buildKvMetadataFromSchema()` - Extracts field descriptions from schema, adds runtime metadata (query type, timestamp, repo counts)

3. `convertJsonToParquet()` - Executes DuckDB SQL with KV_METADATA clause, applies ZSTD compression, sets ROW_GROUP_SIZE for optimization

4. `readParquetMetadata()` - Queries Parquet KV metadata for validation and debugging

### DuckDB SQL Query

```sql
COPY (
  SELECT * FROM read_json_auto('graduated-analyzed.json')
) TO 'graduated-analyzed.parquet' (
  FORMAT PARQUET,
  COMPRESSION ZSTD,
  ROW_GROUP_SIZE 100000,
  KV_METADATA {
    schema_title: 'GitHub Supply Chain Security Analysis Schema',
    schema_version: '1.0.0',
    run_query_type: 'GetRepoDataArtifacts',
    run_timestamp: '2025-10-06T22-30-15',
    field_repository_name: 'Name of the repository',
    field_artifact_is_sbom: 'Boolean indicating if artifact is a Software Bill of Materials',
    -- ... up to 50 field descriptions ...
  }
);
```

### Integration

Automatically called by `generateReports()` in `src/report.ts`:

```typescript
// After JSON/CSV/schema generation:
if (runMetadata) {
  await generateParquetFiles(
    basePathForParquet,
    jsonPath,
    schemaPath,
    runMetadata
  );
}
```

---

## File Generation Flow

```
1. Fetch data from GitHub API
   ↓
2. Save raw responses to raw-responses.jsonl
   ↓
3. Analyze and classify artifacts
   ↓
4. Generate {dataset}-analyzed.json (domain model)
   ↓
5. Flatten to normalized rows
   ↓
6. Generate {dataset}.csv (normalized flat)
   ↓
7. Generate {dataset}-schema.json (field docs)
   ↓
8. Read schema.json + analyzed.json
   ↓
9. Convert to {dataset}-analyzed.parquet with embedded metadata
```

---

## JSONL Long Lines

Lines in `raw-responses.jsonl` can be 10KB-100KB+ (full GraphQL response with 5 releases × N assets).

This is fine because:

- `jq` handles it: `jq -c '.metadata' raw-responses.jsonl`
- DuckDB handles it: Designed for large JSON records
- Streaming parsers handle it: Node.js `readline`, Python `ijson`
- Parquet solves it: Columnar format, no line-length concerns

Mitigation:

- Document in README: "JSONL files have very long lines"
- Use `.jsonl` extension (not `.json`) to signal format
- Provide Parquet files for analysis (not JSONL)

---

## Querying Parquet Files

### Using DuckDB CLI

Count repos with SBOMs:

```sql
SELECT 
  repository_name_with_owner,
  COUNT(DISTINCT artifact_name) as sbom_count
FROM 'output/graduated-2025-10-06T22-30-15/graduated-analyzed.parquet'
WHERE artifact_is_sbom = true
GROUP BY repository_name_with_owner
ORDER BY sbom_count DESC;
```

Check embedded metadata:

```sql
SELECT * 
FROM parquet_kv_metadata('output/graduated-2025-10-06T22-30-15/graduated-analyzed.parquet')
WHERE key LIKE 'field_%'
LIMIT 10;
```

### Using Python (PyArrow)

```python
import pyarrow.parquet as pq

# Read Parquet file
table = pq.read_table('output/graduated-2025-10-06T22-30-15/graduated-analyzed.parquet')

# Access metadata
metadata = table.schema.metadata
print(metadata[b'field_repository_name'].decode())  # "Name of the repository"

# Query with Pandas
df = table.to_pandas()
sbom_repos = df[df['artifact_is_sbom'] == True]
print(sbom_repos['repository_name_with_owner'].unique())
```

---

## Future Enhancements

### Phase 3: Raw Responses to Parquet

Currently, only analyzed JSON is converted to Parquet. Future work will convert raw-responses.jsonl → raw-responses.parquet:

```sql
COPY (
  SELECT 
    metadata.queryType as query_type,
    metadata.timestamp as timestamp,
    metadata.owner as owner,
    metadata.repo as repo,
    response
  FROM read_json_auto('raw-responses.jsonl', format='newline_delimited')
) TO 'raw-responses.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);
```

Benefits:

- Smaller file size (ZSTD compression)
- Faster queries (columnar format)
- Same metadata embedding

### Phase 4: Query-Specific Tables

Split by query type for schema consistency:

```text
output/
  graduated-2025-10-06T22-30-15/
    artifacts-raw.parquet         # GetRepoDataArtifacts responses
    extended-raw.parquet          # GetRepoDataExtendedInfo responses
```

Implementation:

```sql
-- Filter by query type
COPY (
  SELECT * FROM 'raw-responses.jsonl'
  WHERE metadata.queryType = 'GetRepoDataArtifacts'
) TO 'artifacts-raw.parquet';
```

### Phase 5: DuckDB Query Engine

Add `query` subcommand for SQL over Parquet files:

```bash
npm start -- query --sql "SELECT COUNT(*) FROM '*/graduated-analyzed.parquet' WHERE artifact_is_sbom"
```

---

## Dependencies

- **duckdb** (^1.1.3) - SQL database for JSON/Parquet conversion
- **chalk** - Console colors
- **json-2-csv** - JSON to CSV conversion

---

## Testing

Validate Parquet generation:

```bash
# Run with mock data
npm start -- --mock --input input/test-single.jsonl

# Check output directory
ls output/test-single-*/

# Verify Parquet metadata
npx duckdb -c "SELECT * FROM parquet_kv_metadata('output/test-single-*/test-single-analyzed.parquet')"
```

Expected output:

- `raw-responses.jsonl` (or skipped in mock mode)
- `test-single-analyzed.json`
- `test-single.csv`
- `test-single-schema.json`
- `test-single-analyzed.parquet` (with 50+ KV metadata entries)

## Troubleshooting

### "Cannot find module 'duckdb'"

Solution: Install dependencies

```bash
npm install
```

### Parquet file not generated

Check:

1. Is `runMetadata` provided to `generateReports()`?
2. Does `{dataset}-analyzed.json` exist?
3. Does `{dataset}-schema.json` exist?
4. Are there compile errors in `parquetWriter.ts`?

Debug:

```typescript
// In src/report.ts, add logging:
console.log('Generating Parquet:', basePathForParquet);
await generateParquetFiles(...);
console.log('Parquet generation complete');
```

### DuckDB installation fails (native module)

Issue: DuckDB requires native compilation.

Solution:

- Ensure C++ compiler is installed (Xcode on macOS)
- Try: `npm install duckdb --build-from-source`
- Alternative: Use DuckDB CLI externally

## Summary

Implemented: Timestamped run directories prevent file collisions

Implemented: DuckDB Parquet generation with schema metadata embedding

Preserves: All field descriptions from schema.json in Parquet KV_METADATA

Enables: SQL queries over Parquet with full field documentation

Next: Add raw-responses.jsonl → Parquet conversion, query-specific tables
</file>

<file path="docs/schema.json">
{
  "title": "GitHub Supply Chain Security Analysis Schema",
  "description": "Normalized data schema for supply chain security analysis of GitHub repositories",
  "version": "1.0.0",
  "generatedAt": "2025-10-05T01:14:09.661Z",
  "sourceFile": "sandbox.parquet.json",
  "fields": [
    {
      "name": "repository_id",
      "type": "string",
      "nullable": false,
      "description": "Unique repository identifier (GitHub URL)",
      "category": "Repository Identity"
    },
    {
      "name": "repository_owner",
      "type": "string",
      "nullable": false,
      "description": "GitHub organization or user name",
      "category": "Repository Identity"
    },
    {
      "name": "repository_name",
      "type": "string",
      "nullable": false,
      "description": "Repository name without owner",
      "category": "Repository Identity"
    },
    {
      "name": "repository_name_with_owner",
      "type": "string",
      "nullable": false,
      "description": "Full repository path (owner/name)",
      "category": "Repository Identity"
    },
    {
      "name": "repository_url",
      "type": "string",
      "nullable": false,
      "description": "GitHub repository URL",
      "category": "Repository Identity"
    },
    {
      "name": "repository_description",
      "type": "string",
      "nullable": false,
      "description": "Repository description text",
      "category": "Repository Identity"
    },
    {
      "name": "repo_has_sbom_artifact",
      "type": "boolean",
      "nullable": false,
      "description": "Repository contains SBOM (Software Bill of Materials) files",
      "category": "Security Artifacts & Tools"
    },
    {
      "name": "repo_has_signature_artifact",
      "type": "boolean",
      "nullable": false,
      "description": "Repository contains cryptographic signature files",
      "category": "Security Artifacts & Tools"
    },
    {
      "name": "repo_has_attestation_artifact",
      "type": "boolean",
      "nullable": false,
      "description": "Repository contains attestation files",
      "category": "Security Artifacts & Tools"
    },
    {
      "name": "repo_security_tools_json",
      "type": "string",
      "nullable": false,
      "description": "JSON array of detected security tools across the repository",
      "category": "Security Artifacts & Tools"
    },
    {
      "name": "repo_security_tools_count",
      "type": "number",
      "nullable": false,
      "description": "Total count of unique security tool types",
      "category": "Security Artifacts & Tools"
    },
    {
      "name": "workflow_count",
      "type": "number",
      "nullable": false,
      "description": "Total number of CI/CD workflow files",
      "category": "CI/CD Workflows"
    },
    {
      "name": "workflow_names_json",
      "type": "string",
      "nullable": false,
      "description": "JSON array of workflow file names",
      "category": "CI/CD Workflows"
    },
    {
      "name": "workflow_security_tools_json",
      "type": "string",
      "nullable": false,
      "description": "JSON array of security tools found in each workflow",
      "category": "CI/CD Workflows"
    },
    {
      "name": "release_index",
      "type": "number",
      "nullable": false,
      "description": "Zero-based index of this release (0 = latest)",
      "category": "Release Information"
    },
    {
      "name": "release_tag_name",
      "type": "string",
      "nullable": false,
      "description": "Git tag name for this release",
      "category": "Release Information"
    },
    {
      "name": "release_name",
      "type": "string",
      "nullable": false,
      "description": "Display name of the release",
      "category": "Release Information"
    },
    {
      "name": "release_created_at",
      "type": "string",
      "nullable": false,
      "description": "ISO timestamp of release creation",
      "category": "Release Information"
    },
    {
      "name": "release_artifact_count",
      "type": "number",
      "nullable": false,
      "description": "Total number of artifacts in this release",
      "category": "Release Information"
    },
    {
      "name": "release_has_container_images",
      "type": "boolean",
      "nullable": false,
      "description": "Release contains container images",
      "category": "Release Information"
    },
    {
      "name": "release_has_slsa_provenance",
      "type": "boolean",
      "nullable": false,
      "description": "Release contains SLSA provenance documents",
      "category": "Release Information"
    },
    {
      "name": "release_has_in_toto_attestation",
      "type": "boolean",
      "nullable": false,
      "description": "Release contains in-toto attestations",
      "category": "Release Information"
    },
    {
      "name": "release_sbom_formats_json",
      "type": "string",
      "nullable": false,
      "description": "JSON array of SBOM formats found in release",
      "category": "Release Information"
    },
    {
      "name": "artifact_index",
      "type": "string | null",
      "nullable": true,
      "description": "Zero-based index of this artifact within the release",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_name",
      "type": "string",
      "nullable": false,
      "description": "Filename of the artifact",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_download_url",
      "type": "string",
      "nullable": false,
      "description": "Direct download URL for the artifact",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_size_hint",
      "type": "string",
      "nullable": false,
      "description": "Estimated size of the artifact",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_file_extension",
      "type": "string",
      "nullable": false,
      "description": "File extension extracted from artifact name",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_is_sbom",
      "type": "boolean",
      "nullable": false,
      "description": "This artifact is an SBOM file",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_is_signature",
      "type": "boolean",
      "nullable": false,
      "description": "This artifact is a cryptographic signature",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_is_attestation",
      "type": "boolean",
      "nullable": false,
      "description": "This artifact is an attestation document",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_sbom_format",
      "type": "string",
      "nullable": false,
      "description": "Specific SBOM format if this is an SBOM file",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_is_vex",
      "type": "boolean",
      "nullable": false,
      "description": "This artifact is a VEX (Vulnerability Exchange) document",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_is_slsa_provenance",
      "type": "boolean",
      "nullable": false,
      "description": "This artifact contains SLSA provenance information",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_is_in_toto_link",
      "type": "boolean",
      "nullable": false,
      "description": "This artifact is an in-toto link metadata file",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_is_container_attestation",
      "type": "boolean",
      "nullable": false,
      "description": "This artifact is a container attestation",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_type",
      "type": "string",
      "nullable": false,
      "description": "Classified type of artifact (signature, sbom, etc.)",
      "category": "Artifact Details"
    },
    {
      "name": "artifact_platform_hint",
      "type": "string",
      "nullable": false,
      "description": "Detected platform/architecture hints",
      "category": "Artifact Details"
    }
  ],
  "categories": {
    "Repository Identity": [
      {
        "name": "repository_id",
        "type": "string",
        "nullable": false,
        "description": "Unique repository identifier (GitHub URL)",
        "category": "Repository Identity"
      },
      {
        "name": "repository_owner",
        "type": "string",
        "nullable": false,
        "description": "GitHub organization or user name",
        "category": "Repository Identity"
      },
      {
        "name": "repository_name",
        "type": "string",
        "nullable": false,
        "description": "Repository name without owner",
        "category": "Repository Identity"
      },
      {
        "name": "repository_name_with_owner",
        "type": "string",
        "nullable": false,
        "description": "Full repository path (owner/name)",
        "category": "Repository Identity"
      },
      {
        "name": "repository_url",
        "type": "string",
        "nullable": false,
        "description": "GitHub repository URL",
        "category": "Repository Identity"
      },
      {
        "name": "repository_description",
        "type": "string",
        "nullable": false,
        "description": "Repository description text",
        "category": "Repository Identity"
      }
    ],
    "Security Artifacts & Tools": [
      {
        "name": "repo_has_sbom_artifact",
        "type": "boolean",
        "nullable": false,
        "description": "Repository contains SBOM (Software Bill of Materials) files",
        "category": "Security Artifacts & Tools"
      },
      {
        "name": "repo_has_signature_artifact",
        "type": "boolean",
        "nullable": false,
        "description": "Repository contains cryptographic signature files",
        "category": "Security Artifacts & Tools"
      },
      {
        "name": "repo_has_attestation_artifact",
        "type": "boolean",
        "nullable": false,
        "description": "Repository contains attestation files",
        "category": "Security Artifacts & Tools"
      },
      {
        "name": "repo_security_tools_json",
        "type": "string",
        "nullable": false,
        "description": "JSON array of detected security tools across the repository",
        "category": "Security Artifacts & Tools"
      },
      {
        "name": "repo_security_tools_count",
        "type": "number",
        "nullable": false,
        "description": "Total count of unique security tool types",
        "category": "Security Artifacts & Tools"
      }
    ],
    "CI/CD Workflows": [
      {
        "name": "workflow_count",
        "type": "number",
        "nullable": false,
        "description": "Total number of CI/CD workflow files",
        "category": "CI/CD Workflows"
      },
      {
        "name": "workflow_names_json",
        "type": "string",
        "nullable": false,
        "description": "JSON array of workflow file names",
        "category": "CI/CD Workflows"
      },
      {
        "name": "workflow_security_tools_json",
        "type": "string",
        "nullable": false,
        "description": "JSON array of security tools found in each workflow",
        "category": "CI/CD Workflows"
      }
    ],
    "Release Information": [
      {
        "name": "release_index",
        "type": "number",
        "nullable": false,
        "description": "Zero-based index of this release (0 = latest)",
        "category": "Release Information"
      },
      {
        "name": "release_tag_name",
        "type": "string",
        "nullable": false,
        "description": "Git tag name for this release",
        "category": "Release Information"
      },
      {
        "name": "release_name",
        "type": "string",
        "nullable": false,
        "description": "Display name of the release",
        "category": "Release Information"
      },
      {
        "name": "release_created_at",
        "type": "string",
        "nullable": false,
        "description": "ISO timestamp of release creation",
        "category": "Release Information"
      },
      {
        "name": "release_artifact_count",
        "type": "number",
        "nullable": false,
        "description": "Total number of artifacts in this release",
        "category": "Release Information"
      },
      {
        "name": "release_has_container_images",
        "type": "boolean",
        "nullable": false,
        "description": "Release contains container images",
        "category": "Release Information"
      },
      {
        "name": "release_has_slsa_provenance",
        "type": "boolean",
        "nullable": false,
        "description": "Release contains SLSA provenance documents",
        "category": "Release Information"
      },
      {
        "name": "release_has_in_toto_attestation",
        "type": "boolean",
        "nullable": false,
        "description": "Release contains in-toto attestations",
        "category": "Release Information"
      },
      {
        "name": "release_sbom_formats_json",
        "type": "string",
        "nullable": false,
        "description": "JSON array of SBOM formats found in release",
        "category": "Release Information"
      }
    ],
    "Artifact Details": [
      {
        "name": "artifact_index",
        "type": "string | null",
        "nullable": true,
        "description": "Zero-based index of this artifact within the release",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_name",
        "type": "string",
        "nullable": false,
        "description": "Filename of the artifact",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_download_url",
        "type": "string",
        "nullable": false,
        "description": "Direct download URL for the artifact",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_size_hint",
        "type": "string",
        "nullable": false,
        "description": "Estimated size of the artifact",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_file_extension",
        "type": "string",
        "nullable": false,
        "description": "File extension extracted from artifact name",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_is_sbom",
        "type": "boolean",
        "nullable": false,
        "description": "This artifact is an SBOM file",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_is_signature",
        "type": "boolean",
        "nullable": false,
        "description": "This artifact is a cryptographic signature",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_is_attestation",
        "type": "boolean",
        "nullable": false,
        "description": "This artifact is an attestation document",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_sbom_format",
        "type": "string",
        "nullable": false,
        "description": "Specific SBOM format if this is an SBOM file",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_is_vex",
        "type": "boolean",
        "nullable": false,
        "description": "This artifact is a VEX (Vulnerability Exchange) document",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_is_slsa_provenance",
        "type": "boolean",
        "nullable": false,
        "description": "This artifact contains SLSA provenance information",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_is_in_toto_link",
        "type": "boolean",
        "nullable": false,
        "description": "This artifact is an in-toto link metadata file",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_is_container_attestation",
        "type": "boolean",
        "nullable": false,
        "description": "This artifact is a container attestation",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_type",
        "type": "string",
        "nullable": false,
        "description": "Classified type of artifact (signature, sbom, etc.)",
        "category": "Artifact Details"
      },
      {
        "name": "artifact_platform_hint",
        "type": "string",
        "nullable": false,
        "description": "Detected platform/architecture hints",
        "category": "Artifact Details"
      }
    ]
  },
  "totalFields": 38
}
</file>

<file path="docs/schema.md">
# GitHub Supply Chain Security Analysis Schema

## Overview

Normalized data schema for supply chain security analysis of GitHub repositories

**Generated from:** `sandbox.parquet.json`  
**Generated at:** 10/4/2025, 9:14:09 PM  
**Total fields:** 38

## Schema Fields

| Field | Type | Nullable | Category | Description |
|-------|------|----------|----------|-------------|
| `repository_id` | `string` |  | Repository Identity | Unique repository identifier (GitHub URL) |
| `repository_owner` | `string` |  | Repository Identity | GitHub organization or user name |
| `repository_name` | `string` |  | Repository Identity | Repository name without owner |
| `repository_name_with_owner` | `string` |  | Repository Identity | Full repository path (owner/name) |
| `repository_url` | `string` |  | Repository Identity | GitHub repository URL |
| `repository_description` | `string` |  | Repository Identity | Repository description text |
| `repo_has_sbom_artifact` | `boolean` |  | Security Artifacts & Tools | Repository contains SBOM (Software Bill of Materials) files |
| `repo_has_signature_artifact` | `boolean` |  | Security Artifacts & Tools | Repository contains cryptographic signature files |
| `repo_has_attestation_artifact` | `boolean` |  | Security Artifacts & Tools | Repository contains attestation files |
| `repo_security_tools_json` | `string` |  | Security Artifacts & Tools | JSON array of detected security tools across the repository |
| `repo_security_tools_count` | `number` |  | Security Artifacts & Tools | Total count of unique security tool types |
| `workflow_count` | `number` |  | CI/CD Workflows | Total number of CI/CD workflow files |
| `workflow_names_json` | `string` |  | CI/CD Workflows | JSON array of workflow file names |
| `workflow_security_tools_json` | `string` |  | CI/CD Workflows | JSON array of security tools found in each workflow |
| `release_index` | `number` |  | Release Information | Zero-based index of this release (0 = latest) |
| `release_tag_name` | `string` |  | Release Information | Git tag name for this release |
| `release_name` | `string` |  | Release Information | Display name of the release |
| `release_created_at` | `string` |  | Release Information | ISO timestamp of release creation |
| `release_artifact_count` | `number` |  | Release Information | Total number of artifacts in this release |
| `release_has_container_images` | `boolean` |  | Release Information | Release contains container images |
| `release_has_slsa_provenance` | `boolean` |  | Release Information | Release contains SLSA provenance documents |
| `release_has_in_toto_attestation` | `boolean` |  | Release Information | Release contains in-toto attestations |
| `release_sbom_formats_json` | `string` |  | Release Information | JSON array of SBOM formats found in release |
| `artifact_index` | `string | null` | ✓ | Artifact Details | Zero-based index of this artifact within the release |
| `artifact_name` | `string` |  | Artifact Details | Filename of the artifact |
| `artifact_download_url` | `string` |  | Artifact Details | Direct download URL for the artifact |
| `artifact_size_hint` | `string` |  | Artifact Details | Estimated size of the artifact |
| `artifact_file_extension` | `string` |  | Artifact Details | File extension extracted from artifact name |
| `artifact_is_sbom` | `boolean` |  | Artifact Details | This artifact is an SBOM file |
| `artifact_is_signature` | `boolean` |  | Artifact Details | This artifact is a cryptographic signature |
| `artifact_is_attestation` | `boolean` |  | Artifact Details | This artifact is an attestation document |
| `artifact_sbom_format` | `string` |  | Artifact Details | Specific SBOM format if this is an SBOM file |
| `artifact_is_vex` | `boolean` |  | Artifact Details | This artifact is a VEX (Vulnerability Exchange) document |
| `artifact_is_slsa_provenance` | `boolean` |  | Artifact Details | This artifact contains SLSA provenance information |
| `artifact_is_in_toto_link` | `boolean` |  | Artifact Details | This artifact is an in-toto link metadata file |
| `artifact_is_container_attestation` | `boolean` |  | Artifact Details | This artifact is a container attestation |
| `artifact_type` | `string` |  | Artifact Details | Classified type of artifact (signature, sbom, etc.) |
| `artifact_platform_hint` | `string` |  | Artifact Details | Detected platform/architecture hints |

## Field Categories


### Repository Identity

- **`repository_id`** (`string`): Unique repository identifier (GitHub URL)
- **`repository_owner`** (`string`): GitHub organization or user name
- **`repository_name`** (`string`): Repository name without owner
- **`repository_name_with_owner`** (`string`): Full repository path (owner/name)
- **`repository_url`** (`string`): GitHub repository URL
- **`repository_description`** (`string`): Repository description text

### Security Artifacts & Tools

- **`repo_has_sbom_artifact`** (`boolean`): Repository contains SBOM (Software Bill of Materials) files
- **`repo_has_signature_artifact`** (`boolean`): Repository contains cryptographic signature files
- **`repo_has_attestation_artifact`** (`boolean`): Repository contains attestation files
- **`repo_security_tools_json`** (`string`): JSON array of detected security tools across the repository
- **`repo_security_tools_count`** (`number`): Total count of unique security tool types

### CI/CD Workflows

- **`workflow_count`** (`number`): Total number of CI/CD workflow files
- **`workflow_names_json`** (`string`): JSON array of workflow file names
- **`workflow_security_tools_json`** (`string`): JSON array of security tools found in each workflow

### Release Information

- **`release_index`** (`number`): Zero-based index of this release (0 = latest)
- **`release_tag_name`** (`string`): Git tag name for this release
- **`release_name`** (`string`): Display name of the release
- **`release_created_at`** (`string`): ISO timestamp of release creation
- **`release_artifact_count`** (`number`): Total number of artifacts in this release
- **`release_has_container_images`** (`boolean`): Release contains container images
- **`release_has_slsa_provenance`** (`boolean`): Release contains SLSA provenance documents
- **`release_has_in_toto_attestation`** (`boolean`): Release contains in-toto attestations
- **`release_sbom_formats_json`** (`string`): JSON array of SBOM formats found in release

### Artifact Details

- **`artifact_index`** (`string | null`): Zero-based index of this artifact within the release
- **`artifact_name`** (`string`): Filename of the artifact
- **`artifact_download_url`** (`string`): Direct download URL for the artifact
- **`artifact_size_hint`** (`string`): Estimated size of the artifact
- **`artifact_file_extension`** (`string`): File extension extracted from artifact name
- **`artifact_is_sbom`** (`boolean`): This artifact is an SBOM file
- **`artifact_is_signature`** (`boolean`): This artifact is a cryptographic signature
- **`artifact_is_attestation`** (`boolean`): This artifact is an attestation document
- **`artifact_sbom_format`** (`string`): Specific SBOM format if this is an SBOM file
- **`artifact_is_vex`** (`boolean`): This artifact is a VEX (Vulnerability Exchange) document
- **`artifact_is_slsa_provenance`** (`boolean`): This artifact contains SLSA provenance information
- **`artifact_is_in_toto_link`** (`boolean`): This artifact is an in-toto link metadata file
- **`artifact_is_container_attestation`** (`boolean`): This artifact is a container attestation
- **`artifact_type`** (`string`): Classified type of artifact (signature, sbom, etc.)
- **`artifact_platform_hint`** (`string`): Detected platform/architecture hints


## Usage Examples

### Loading in Python (Pandas)

```python
import pandas as pd

# Load CSV data
df = pd.read_csv('output/analysis.csv')

# Load Parquet data (more efficient)
df = pd.read_parquet('output/analysis.parquet')

# Display schema info
print(df.info())
print(f"Total rows: {len(df)}")
print(f"Total repositories: {df['repository_name'].nunique()}")
```

### Loading in R

```r
library(arrow)
library(dplyr)

# Load Parquet data
df <- read_parquet("output/analysis.parquet")

# Display schema
glimpse(df)

# Summary statistics
df %>% 
  group_by(repository_name) %>% 
  summarise(
    total_artifacts = n(),
    has_sbom = any(artifact_is_sbom),
    has_signatures = any(artifact_is_signature)
  )
```

### Loading in JavaScript/Node.js

```javascript
// Load JSON data
const data = require('./output/analysis.json');

// Load using Apache Arrow (for Parquet)
const { tableFromIPC } = require('apache-arrow');
const fs = require('fs');

// Process data
const repositories = [...new Set(data.map(row => row.repository_name))];
console.log(`Found ${repositories.length} repositories`);
```

## Data Quality Notes

- All timestamps are in ISO 8601 format
- JSON array fields are stored as JSON-encoded strings in CSV format
- Boolean fields use `true`/`false` values
- Missing values are represented as `null`
- Platform hints are comma-separated values (e.g., "linux,amd64")

## Schema Evolution

This schema follows semantic versioning principles:
- **Major version**: Breaking changes to field names or types
- **Minor version**: New fields added
- **Patch version**: Documentation updates or bug fixes

Current version: 1.0.0
</file>

<file path="docs/size-reporting.md">
# File Size Reporting

## Overview

The tool automatically generates a comprehensive file size report (`SIZE-REPORT.md`) in each output directory. This report tracks storage usage for all generated artifacts and is designed to be committed to git for tracking storage growth over time.

## Generated Report

The `SIZE-REPORT.md` file is created automatically after each run and includes:

### Report Structure

```markdown
# Output Size Report

**Generated:** 2025-10-08T21:57:27.115Z
**Dataset:** GetRepoDataArtifacts
**Repositories:** 200 (195 successful, 5 failed)

## Files by Category

### Raw Responses
- `raw-responses.jsonl`: 2.4 MB

### Analyzed Data
- `graduated-analyzed.parquet`: 187 KB
- `graduated.csv`: 98 KB
- `graduated-analyzed.json`: 456 KB

### Schema Documentation
- `graduated-schema.json`: 8 KB

## Summary

| Category | Files | Total Size |
|----------|-------|------------|
| Raw Responses | 1 | 2.4 MB |
| Analyzed Data | 3 | 741 KB |
| Schema Documentation | 1 | 8 KB |
| **Total** | **5** | **3.1 MB** |
```

## File Categories

Files are automatically categorized for easy tracking:

- **Raw Responses**: JSONL files containing raw GitHub API responses
- **Analyzed Data**: Processed outputs (Parquet, CSV, analyzed JSON)
- **Schema Documentation**: Schema definitions and field documentation
- **Reports**: Size reports and other metadata

## Console Output

During execution, the tool displays a summary in the console:

```
📊 Output Size Summary
────────────────────────────────────────────────────────────

Analyzed Data:
  graduated-analyzed.parquet                       187.23 KB
  graduated.csv                                     98.45 KB
  graduated-analyzed.json                          456.12 KB

Schema Documentation:
  graduated-schema.json                              8.15 KB

Raw Responses:
  raw-responses.jsonl                                2.40 MB

────────────────────────────────────────────────────────────
Total: 5 files, 3.14 MB
────────────────────────────────────────────────────────────

✅ Size report saved: output/graduated-2025-10-08T21-57-27/SIZE-REPORT.md
```

## Git Integration

### Tracking in Version Control

The `.gitignore` file is configured to allow `SIZE-REPORT.md` files to be committed:

```gitignore
# Output and cache
output/
# But allow SIZE-REPORT.md to be committed for tracking storage growth
!output/*/SIZE-REPORT.md
```

This enables tracking storage growth over time in git history.

### Weekly CI Commits

When running in CI (see [CI Deployment Guide](./ci-deployment.md)), each weekly run commits the size report along with Parquet and CSV files:

```bash
git add output/**/*.parquet
git add output/**/*.csv
git add output/**/SIZE-REPORT.md
git commit -m "chore: weekly data collection - 2025-01-08"
```

### Viewing Size History

To track size growth over time:

```bash
# View all size reports in git history
git log --all --oneline -- "**/SIZE-REPORT.md"

# Show size report from a specific run
git show abc123:output/graduated-2025-01-08T00-00-00/SIZE-REPORT.md

# Compare sizes between two runs
diff \
  <(git show abc123:output/graduated-2025-01-01T00-00-00/SIZE-REPORT.md) \
  <(git show def456:output/graduated-2025-01-08T00-00-00/SIZE-REPORT.md)
```

## Expected File Sizes

Based on actual measurements:

### Small Dataset (1-5 repos)
```
Raw Responses:      10-50 KB
Analyzed JSON:      2-10 KB
Parquet:            1-5 KB (70-80% smaller than JSON)
CSV:                2-5 KB
Schema:             8 KB (constant)
Total:              ~25-80 KB
```

### Medium Dataset (20-50 repos)
```
Raw Responses:      200-500 KB
Analyzed JSON:      40-100 KB
Parquet:            20-50 KB
CSV:                10-25 KB
Schema:             8 KB
Total:              ~280-680 KB
```

### Large Dataset (200+ repos)
```
Raw Responses:      2-5 MB (grows ~15 KB per repo)
Analyzed JSON:      400-800 KB
Parquet:            150-300 KB
CSV:                100-200 KB
Schema:             8 KB
Total:              ~2.6-6 MB
```

## Storage Optimization

The size report helps identify opportunities for optimization:

### Parquet Compression

Parquet files achieve 70-80% compression compared to JSON:
- **JSON**: 456 KB
- **Parquet**: 187 KB (59% smaller)
- **Savings**: 269 KB per dataset

### Selective Commits

For CI workflows, you can commit only the most efficient formats:

**Option 1: Parquet + CSV only (Recommended)**
- Excludes raw-responses.jsonl and analyzed JSON
- ~300 KB per weekly run for 200 repos
- Sustainable without Git LFS

**Option 2: All files with Git LFS**
- Includes raw responses for reproducibility
- ~3 MB per weekly run for 200 repos
- Requires Git LFS after ~300 MB

The size report makes it easy to make informed decisions about storage strategy.

## Automation

The size report is generated automatically as part of the normal `generateReports()` flow. No additional configuration or flags are required.

### Implementation

The report is generated by the `generateSizeReport()` function in `src/sizeReport.ts`, which:

1. Scans the output directory for all generated files
2. Calculates file sizes using `fs.stat()`
3. Groups files by category
4. Calculates subtotals and total size
5. Generates both console output and markdown file
6. Uses human-readable formatting (KB, MB, GB)

### Extensibility

The size report can be extended to include additional metrics:

- Compression ratios
- Growth rates compared to previous runs
- Per-repository size breakdown
- Storage projections

These enhancements can be added by modifying `src/sizeReport.ts`.

## Related Documentation

- [CI Deployment Guide](./ci-deployment.md) - Weekly automation and storage strategies
- [Output Architecture](./output-architecture.md) - Output directory structure
- [Data Model](./data-model.md) - Understanding generated files
</file>

<file path="docs/viewing-parquet-data.md">
# Viewing Parquet Data with DuckDB

This guide explains how to explore the generated Parquet files using DuckDB, both via the command line and the web-based DuckDB Shell.

## Overview

The tool generates Parquet files with embedded schema metadata in timestamped output directories. These files can be explored without writing a custom UI using DuckDB's built-in capabilities.

## Installation

### DuckDB CLI

Install DuckDB CLI on macOS:

```bash
# Using the installation script
curl https://install.duckdb.org | sh

# Or using Homebrew
brew install duckdb

# Verify installation
duckdb --version
```

For other platforms, see [DuckDB Installation Guide](https://duckdb.org/docs/installation/).

## Viewing Data

### Option 1: DuckDB Local UI (Recommended)

**Available in DuckDB v1.2.1+**, the DuckDB Local UI is a full-featured web interface that runs entirely on your computer. Your data never leaves your machine.

**Starting the UI:**

```bash
# Launch UI directly
duckdb -ui

# Or from within any DuckDB client (CLI, Python, etc.)
duckdb
> CALL start_ui();
```

This automatically:
1. Installs the `ui` extension (if needed)
2. Starts an embedded HTTP server on localhost
3. Opens the UI in your default browser (default: http://localhost:4213)

**Loading Parquet Files:**

```bash
# Open UI with a Parquet file loaded as a view
duckdb -cmd "CREATE VIEW data AS FROM 'output/run-20250108-120000/test-single-analyzed.parquet';" -ui
```

**Key Features:**

- **Interactive Notebooks**: Write and execute SQL queries in notebook cells with syntax highlighting and autocomplete
- **Visual Schema Browser**: Explore databases, tables, and schemas in a tree view on the left panel
- **Table Summaries**: Click any table to see row counts, column types, and data profiles
- **Column Explorer**: Right panel shows statistical summaries and data distributions for query results
- **Export Controls**: Export results to CSV, JSON, or clipboard directly from the UI
- **Local-First**: All queries run locally - no data sent to external servers (unless you opt-in to MotherDuck)
- **Metadata Access**: Query embedded Parquet metadata using `parquet_kv_metadata()` function

**Example Session:**

```sql
-- Create a view of your Parquet file
CREATE VIEW data AS 
SELECT * FROM 'output/run-20250108-120000/test-single-analyzed.parquet';

-- Browse the data interactively
SELECT * FROM data LIMIT 100;

-- View embedded metadata
SELECT * FROM parquet_kv_metadata('output/run-20250108-120000/test-single-analyzed.parquet');

-- Analyze security coverage
SELECT 
  has_sbom,
  has_signatures,
  has_attestations,
  COUNT(*) as repo_count
FROM data
GROUP BY has_sbom, has_signatures, has_attestations;
```

**Configuration Options:**

```sql
-- Change UI port (default: 4213)
SET ui_local_port = 8080;

-- Start server without opening browser
CALL start_ui_server();

-- Stop the UI server
CALL stop_ui_server();
```

**Files Created:**

The UI stores notebooks and state in `~/.duckdb/extension_data/ui/ui.db`. Export files (CSV, JSON) are temporarily written to the same directory and cleared after export.

### Option 2: DuckDB Command Line (Local)

Use the provided script to explore your Parquet files:

```bash
# View a specific Parquet file
./scripts/view-parquet.sh output/run-20250108-120000/test-single-analyzed.parquet

# Or run DuckDB directly
duckdb -c "SELECT * FROM 'output/run-20250108-120000/test-single-analyzed.parquet' LIMIT 10;"
```

The script provides an interactive DuckDB session with helpful commands:

```sql
-- Show all columns
DESCRIBE SELECT * FROM data;

-- Count total rows
SELECT COUNT(*) FROM data;

-- View schema metadata embedded in the file
SELECT * FROM parquet_kv_metadata('output/run-20250108-120000/test-single-analyzed.parquet');

-- Query specific columns
SELECT repo_owner, repo_name, has_sbom, has_signatures 
FROM data 
WHERE has_sbom = true;

-- Aggregate statistics
SELECT 
  has_sbom,
  has_signatures,
  has_attestations,
  COUNT(*) as count
FROM data
GROUP BY has_sbom, has_signatures, has_attestations
ORDER BY count DESC;

-- Export results to CSV
COPY (SELECT * FROM data WHERE has_sbom = true) 
TO 'sbom-repos.csv' (HEADER, DELIMITER ',');
```

### Option 3: DuckDB Web Shell (No Installation Required)

The DuckDB Shell runs entirely in your browser using WebAssembly. No server or installation needed. This is a different tool from the Local UI - it's purely browser-based with no local DuckDB installation required.

**Access:** <https://shell.duckdb.org/>

**Steps:**

1. Open <https://shell.duckdb.org/> in your browser
2. Upload your Parquet file or reference it via URL (if publicly accessible)
3. Query the data interactively

```sql
-- If file is uploaded to the browser
SELECT * FROM read_parquet('uploaded-file.parquet') LIMIT 10;

-- If file is accessible via HTTPS URL
SELECT * FROM 'https://example.com/path/to/file.parquet' LIMIT 10;

-- View metadata
SELECT * FROM parquet_kv_metadata('uploaded-file.parquet');
```

**Features:**

- Full SQL support in the browser
- Arrow-based columnar processing
- Parquet, CSV, and JSON support
- No data leaves your browser (privacy)
- Share queries via URL encoding

**Limitations:**

- Local files must be uploaded to the browser (no direct filesystem access)
- For large files, CLI may be more performant
- HTTPS required for remote file access
- CORS headers required for cross-origin requests

**Comparison: Local UI vs Web Shell:**

| Feature | DuckDB Local UI | DuckDB Web Shell |
|---------|----------------|------------------|
| Installation | Requires DuckDB CLI | None (browser only) |
| Data Access | Direct filesystem | Upload or HTTPS URLs |
| Performance | Native speed | WebAssembly (slower) |
| Privacy | 100% local | 100% browser-local |
| Persistence | Notebooks saved locally | Session-based |
| Use Case | Regular analysis | Quick exploration |

### Option 4: Python with DuckDB

For programmatic analysis or Jupyter notebooks:

```python
import duckdb

# Connect and query
con = duckdb.connect()
result = con.execute("""
    SELECT * FROM 'output/run-20250108-120000/test-single-analyzed.parquet'
    WHERE has_sbom = true
""").fetchdf()

print(result)

# View embedded metadata
metadata = con.execute("""
    SELECT * FROM parquet_kv_metadata('output/run-20250108-120000/test-single-analyzed.parquet')
""").fetchall()

for key, value in metadata:
    print(f"{key}: {value}")
```

## Embedded Metadata

Each Parquet file contains embedded metadata as key-value pairs:

### Schema-Level Metadata
- `schema_title`: Title of the schema
- `schema_version`: Version of the schema
- `schema_description`: Description of the data structure
- `schema_generated_at`: Timestamp when schema was generated

### Runtime Metadata
- `run_query_type`: Query type used (artifacts/extended)
- `run_timestamp`: When the data collection ran
- `run_total_repos`: Total repositories processed
- `run_successful_repos`: Successfully processed repos
- `run_failed_repos`: Failed repository queries

### Field Descriptions
- `field_<name>`: Description of each field in the dataset

To view all metadata:

```sql
SELECT * FROM parquet_kv_metadata('path/to/file.parquet');
```

## Common Queries

### Security Artifact Coverage

```sql
-- Repos with complete security coverage
SELECT repo_owner, repo_name, repo_url
FROM data
WHERE has_sbom = true 
  AND has_signatures = true 
  AND has_attestations = true;

-- Coverage summary
SELECT 
  CASE 
    WHEN has_sbom AND has_signatures AND has_attestations THEN 'Full Coverage'
    WHEN has_sbom OR has_signatures OR has_attestations THEN 'Partial Coverage'
    ELSE 'No Coverage'
  END as coverage_level,
  COUNT(*) as repo_count,
  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
FROM data
GROUP BY coverage_level;
```

### CI/CD Pipeline Analysis

```sql
-- Repos with SBOM generation in CI
SELECT repo_owner, repo_name, release_count
FROM data
WHERE has_sbom_ci = true
ORDER BY release_count DESC;

-- CI adoption rates
SELECT 
  SUM(CASE WHEN has_sbom_ci THEN 1 ELSE 0 END) as sbom_ci_count,
  SUM(CASE WHEN has_sign_ci THEN 1 ELSE 0 END) as sign_ci_count,
  SUM(CASE WHEN has_goreleaser_ci THEN 1 ELSE 0 END) as goreleaser_count,
  COUNT(*) as total_repos
FROM data;
```

### Artifact Analysis

```sql
-- Most common SBOM formats
SELECT sbom_format, COUNT(*) as count
FROM data
WHERE sbom_format IS NOT NULL
GROUP BY sbom_format
ORDER BY count DESC;

-- Signature file patterns
SELECT signature_format, COUNT(*) as count
FROM data
WHERE signature_format IS NOT NULL
GROUP BY signature_format
ORDER BY count DESC;
```

## Tips

1. **Start with the Local UI**: For regular analysis, the Local UI (Option 1) provides the best experience with notebooks, visual exploration, and full filesystem access
2. **Use CLI for Scripts**: The command-line interface (Option 2) is ideal for automation and scripting
3. **Web Shell for Quick Checks**: Use the browser-based shell (Option 3) when you need to quickly explore data without installing anything
4. **Performance**: DuckDB is optimized for analytical queries over columnar Parquet files - even large datasets are processed efficiently
5. **Memory**: Large files are processed via streaming, so you can analyze datasets larger than RAM
6. **Compression**: Files use ZSTD compression for optimal size/speed tradeoff
7. **Schema**: Use `DESCRIBE` to understand the data structure, or click tables in the Local UI
8. **Export**: Results can be exported to CSV, JSON, or other Parquet files from any interface

## Related Documentation

- [Data Model Documentation](./data-model.md) - Understanding raw vs analyzed data
- [Output Architecture](./output-architecture.md) - Directory structure and file organization
- [Schema Documentation](./schema.md) - Field definitions and data types

## External Resources

- [DuckDB Documentation](https://duckdb.org/docs/)
- [DuckDB Web Shell](https://shell.duckdb.org/)
- [Parquet Format Documentation](https://parquet.apache.org/docs/)
- [DuckDB Parquet Guide](https://duckdb.org/docs/data/parquet/overview)
</file>

<file path="input/graduated.jsonl">
{"owner":"kubeedge","name":"kubeedge"}
{"owner":"goharbor","name":"harbor"}
{"owner":"cert-manager","name":"cert-manager"}
{"owner":"falcosecurity","name":"falco"}
{"owner":"in-toto","name":"in-toto"}
{"owner":"open-policy-agent","name":"opa"}
{"owner":"theupdateframework","name":"python-tuf"}
{"owner":"spiffe","name":"spiffe"}
{"owner":"spiffe","name":"spire"}
{"owner":"cubeFS","name":"cubefs"}
{"owner":"rook","name":"rook"}
{"owner":"containerd","name":"containerd"}
{"owner":"cri-o","name":"cri-o"}
{"owner":"cilium","name":"cilium"}
{"owner":"kedacore","name":"keda"}
{"owner":"knative","name":"serving"}
{"owner":"kubernetes","name":"kubernetes"}
{"owner":"coredns","name":"coredns"}
{"owner":"etcd-io","name":"etcd"}
{"owner":"envoyproxy","name":"envoy"}
{"owner":"istio","name":"istio"}
{"owner":"linkerd","name":"linkerd2"}
{"owner":"tikv","name":"tikv"}
{"owner":"vitessio","name":"vitess"}
{"owner":"cloudevents","name":"spec"}
{"owner":"dapr","name":"dapr"}
{"owner":"helm","name":"helm"}
{"owner":"argoproj","name":"argo-cd"}
{"owner":"fluxcd","name":"flux2"}
{"owner":"fluent","name":"fluentd"}
{"owner":"jaegertracing","name":"jaeger"}
{"owner":"prometheus","name":"prometheus"}
</file>

<file path="input/incubation.jsonl">
{"owner":"cloud-custodian","name":"cloud-custodian"}
{"owner":"metal3-io","name":"baremetal-operator"}
{"owner":"openyurtio","name":"openyurt"}
{"owner":"dragonflyoss","name":"dragonfly"}
{"owner":"keycloak","name":"keycloak"}
{"owner":"kubescape","name":"kubescape"}
{"owner":"kyverno","name":"kyverno"}
{"owner":"notaryproject","name":"notation"}
{"owner":"longhorn","name":"longhorn"}
{"owner":"containernetworking","name":"cni"}
{"owner":"crossplane","name":"crossplane"}
{"owner":"karmada-io","name":"karmada"}
{"owner":"kubeflow","name":"kubeflow"}
{"owner":"volcano-sh","name":"volcano"}
{"owner":"wasmCloud","name":"wasmCloud"}
{"owner":"grpc","name":"grpc"}
{"owner":"projectcontour","name":"contour"}
{"owner":"emissary-ingress","name":"emissary"}
{"owner":"nats-io","name":"nats-server"}
{"owner":"strimzi","name":"strimzi-kafka-operator"}
{"owner":"artifacthub","name":"hub"}
{"owner":"backstage","name":"backstage"}
{"owner":"buildpacks","name":"pack"}
{"owner":"kubevela","name":"kubevela"}
{"owner":"kubevirt","name":"kubevirt"}
{"owner":"operator-framework","name":"operator-sdk"}
{"owner":"openkruise","name":"kruise"}
{"owner":"flatcar","name":"Flatcar"}
{"owner":"open-feature","name":"spec"}
{"owner":"chaos-mesh","name":"chaos-mesh"}
{"owner":"litmuschaos","name":"litmus"}
{"owner":"opencost","name":"opencost"}
{"owner":"cortexproject","name":"cortex"}
{"owner":"open-telemetry","name":"community"}
{"owner":"thanos-io","name":"thanos"}
{"owner":"kserve","name":"kserve"}
</file>

<file path="input/sandbox.jsonl">
{"owner":"project-akri","name":"akri"}
{"owner":"runatlantis","name":"atlantis"}
{"owner":"cadence-workflow","name":"cadence"}
{"owner":"cdk8s-team","name":"cdk8s"}
{"owner":"kagent-dev","name":"kagent"}
{"owner":"kairos-io","name":"kairos"}
{"owner":"kcl-lang","name":"kcl"}
{"owner":"kitops-ml","name":"kitops"}
{"owner":"kptdev","name":"kpt"}
{"owner":"kubean-io","name":"kubean"}
{"owner":"KusionStack","name":"kusion"}
{"owner":"meshery","name":"meshery"}
{"owner":"opentofu","name":"opentofu"}
{"owner":"runmedev","name":"runme"}
{"owner":"tinkerbell","name":"tinkerbell"}
{"owner":"distribution","name":"distribution"}
{"owner":"project-zot","name":"zot"}
{"owner":"bank-vaults","name":"bank-vaults"}
{"owner":"bpfman","name":"bpfman"}
{"owner":"cartography-cncf","name":"cartography"}
{"owner":"confidential-containers","name":"confidential-containers"}
{"owner":"containerssh","name":"containerssh"}
{"owner":"project-copacetic","name":"copacetic"}
{"owner":"dexidp","name":"dex"}
{"owner":"external-secrets","name":"external-secrets"}
{"owner":"hexa-org","name":"policy-orchestrator"}
{"owner":"keylime","name":"keylime"}
{"owner":"kubearmor","name":"kubearmor"}
{"owner":"kubewarden","name":"kubewarden-controller"}
{"owner":"opcr-io","name":"policy"}
{"owner":"openfga","name":"openfga"}
{"owner":"oscal-compass","name":"compliance-trestle"}
{"owner":"paralus","name":"paralus"}
{"owner":"parallaxsecond","name":"parsec"}
{"owner":"ratify-project","name":"ratify"}
{"owner":"slimtoolkit","name":"slim"}
{"owner":"getsops","name":"sops"}
{"owner":"tokenetes","name":"tokenetes"}
{"owner":"AthenZ","name":"athenz"}
{"owner":"carina-io","name":"carina"}
{"owner":"hwameistor","name":"hwameistor"}
{"owner":"k8up-io","name":"k8up"}
{"owner":"kanisterio","name":"kanister"}
{"owner":"openebs","name":"openebs"}
{"owner":"oras-project","name":"oras"}
{"owner":"piraeusdatastore","name":"piraeus-operator"}
{"owner":"v6d-io","name":"v6d"}
{"owner":"containers","name":"composefs"}
{"owner":"containers","name":"bootc"}
{"owner":"hyperlight-dev","name":"hyperlight"}
{"owner":"inclavare-containers","name":"inclavare-containers"}
{"owner":"kuasar-io","name":"kuasar"}
{"owner":"lima-vm","name":"lima"}
{"owner":"containers","name":"podman"}
{"owner":"urunc-dev","name":"urunc"}
{"owner":"virtual-kubelet","name":"virtual-kubelet"}
{"owner":"interTwin-eu","name":"interLink"}
{"owner":"WasmEdge","name":"WasmEdge"}
{"owner":"youki-dev","name":"youki"}
{"owner":"antrea-io","name":"antrea"}
{"owner":"kubeovn","name":"kube-ovn"}
{"owner":"kube-vip","name":"kube-vip"}
{"owner":"networkservicemesh","name":"api"}
{"owner":"ovn-kubernetes","name":"ovn-kubernetes"}
{"owner":"spidernet-io","name":"spiderpool"}
{"owner":"submariner-io","name":"submariner"}
{"owner":"armadaproject","name":"armada"}
{"owner":"projectcapsule","name":"capsule"}
{"owner":"clusternet","name":"clusternet"}
{"owner":"clusterpedia-io","name":"clusterpedia"}
{"owner":"eraser-dev","name":"eraser"}
{"owner":"fluid-cloudnative","name":"fluid"}
{"owner":"Project-HAMi","name":"HAMi"}
{"owner":"kcp-dev","name":"kcp"}
{"owner":"k0sproject","name":"k0s"}
{"owner":"koordinator-sh","name":"koordinator"}
{"owner":"kube-rs","name":"kube"}
{"owner":"kubefleet-dev","name":"kubefleet"}
{"owner":"kubeslice","name":"kubeslice"}
{"owner":"kubestellar","name":"kubestellar"}
{"owner":"kubereboot","name":"kured"}
{"owner":"open-cluster-management-io","name":"ocm"}
{"owner":"OpenFunction","name":"OpenFunction"}
{"owner":"serverless-devs","name":"serverless-devs"}
{"owner":"k8gb-io","name":"k8gb"}
{"owner":"connectrpc","name":"connect-go"}
{"owner":"bfenetworks","name":"bfe"}
{"owner":"loxilb-io","name":"loxilb"}
{"owner":"metallb","name":"metallb"}
{"owner":"easegress-io","name":"easegress"}
{"owner":"kgateway-dev","name":"kgateway"}
{"owner":"kuadrant","name":"kuadrant-operator"}
{"owner":"aeraki-mesh","name":"aeraki"}
{"owner":"kmesh-net","name":"kmesh"}
{"owner":"kumahq","name":"kuma"}
{"owner":"sermant-io","name":"Sermant"}
{"owner":"cloudnative-pg","name":"cloudnative-pg"}
{"owner":"openGemini","name":"openGemini"}
{"owner":"schemahero","name":"schemahero"}
{"owner":"drasi-project","name":"drasi-platform"}
{"owner":"tremor-rs","name":"tremor-runtime"}
{"owner":"carvel-dev","name":"ytt"}
{"owner":"devfile","name":"api"}
{"owner":"devspace-sh","name":"devspace"}
{"owner":"ko-build","name":"ko"}
{"owner":"konveyor","name":"operator"}
{"owner":"kudobuilder","name":"kudo"}
{"owner":"microcks","name":"microcks"}
{"owner":"modelpack","name":"model-spec"}
{"owner":"podman-desktop","name":"podman-desktop"}
{"owner":"getporter","name":"porter"}
{"owner":"radius-project","name":"radius"}
{"owner":"score-spec","name":"spec"}
{"owner":"serverlessworkflow","name":"specification"}
{"owner":"shipwright-io","name":"build"}
{"owner":"project-stacker","name":"stacker"}
{"owner":"telepresenceio","name":"telepresence"}
{"owner":"vscode-kubernetes-tools","name":"vscode-kubernetes-tools"}
{"owner":"xregistry","name":"server"}
{"owner":"open-gitops","name":"project"}
{"owner":"pipe-cd","name":"pipecd"}
{"owner":"werf","name":"werf"}
{"owner":"kube-burner","name":"kube-burner"}
{"owner":"k3s-io","name":"k3s"}
{"owner":"kubeclipper","name":"kubeclipper"}
{"owner":"cozystack","name":"cozystack"}
{"owner":"SlimPlanet","name":"SlimFaas"}
{"owner":"chaosblade-io","name":"chaosblade"}
{"owner":"krkn-chaos","name":"krkn"}
{"owner":"inspektor-gadget","name":"inspektor-gadget"}
{"owner":"k8sgpt-ai","name":"k8sgpt"}
{"owner":"sustainable-computing-io","name":"kepler"}
{"owner":"kuberhealthy","name":"kuberhealthy"}
{"owner":"kube-logging","name":"logging-operator"}
{"owner":"perses","name":"perses"}
{"owner":"pixie-io","name":"pixie"}
{"owner":"trickstercache","name":"trickster"}
{"owner":"spinframework","name":"spin-operator"}
{"owner":"container2wasm","name":"container2wasm"}
{"owner":"kaito-project","name":"kaito"}
</file>

<file path="input/test-allmocked.jsonl">
{"owner":"goharbor","name":"harbor"}
{"owner":"in-toto","name":"in-toto"}
{"owner":"oscal-compass","name":"compliance-trestle"}
{"owner":"cilium","name":"cilium"}
{"owner":"knative","name":"serving"}
{"owner":"fluxcd","name":"flux2"}
{"owner":"argoproj","name":"argo-cd"}
{"owner":"sigstore", "name":"cosign"}
</file>

<file path="input/test-flux.jsonl">
{"full_name":"fluxcd/flux2","name":"flux2","owner":"fluxcd"}
</file>

<file path="input/test-single.jsonl">
{"owner": "sigstore", "name": "cosign"}
</file>

<file path="input/test-three-repos.jsonl">
{"owner": "sigstore", "name": "cosign"}
{"owner": "anchore", "name": "syft"}
{"owner": "github", "name": "docs"}
</file>

<file path="schema/download-schema.sh">
#!/bin/sh
# Download the exact, unmodified GitHub GraphQL schema SDL for v15.26.0
# Usage: ./download-schema.sh

curl -sSL https://raw.githubusercontent.com/octokit/graphql-schema/v15.26.0/schema.graphql -o github-v15.26.0.graphql
</file>

<file path="scripts/ensure-env.sh">
#!/usr/bin/env bash
# scripts/ensure-env.sh
# Ensure .env exists; otherwise print helpful message and exit 1.

set -euo pipefail

if [ -f .env ]; then
  exit 0
fi

echo "Error: .env file not found."
echo "Copy the template and add your GitHub token:"
echo "  cp .env.template .env"
echo "Set GITHUB_PAT in .env with a Personal Access Token. Create one here:" 
echo "  https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token"
exit 1
</file>

<file path="scripts/run-cncf-all.sh">
#!/usr/bin/env bash
# scripts/run-cncf-all.sh
# Run sandbox, incubation, and graduated sequentially.
# Merging of outputs is intentionally left to DuckDB (or external tools) so rows
# retain their `maturity` column and can be combined there.

set -euo pipefail

ROOT=$(cd "$(dirname "$0")/.." && pwd)
cd "$ROOT"

echo "Running sandbox dataset..."
./scripts/run-target.sh graduated

echo "Running incubation dataset..."
./scripts/run-target.sh incubation

echo "Running sandbox dataset..."
./scripts/run-target.sh sandbox

echo "All runs complete. Check the timestamped directories under output/."
echo "Combine datasets in DuckDB by reading the CSV/JSON/Parquet files and filtering on the 'maturity' column."

*** End Patch
</file>

<file path="scripts/run-target.sh">
#!/usr/bin/env bash
# scripts/run-target.sh
# Usage: ./scripts/run-target.sh <dataset>
# dataset is one of: sandbox, incubation, graduated

set -euo pipefail

if [ "$#" -ne 1 ]; then
  echo "Usage: $0 <dataset>"
  exit 2
fi

DATASET=$1
INPUT_FILE="input/${DATASET}.jsonl"

./scripts/ensure-env.sh

if [ ! -f "$INPUT_FILE" ]; then
  echo "Input file not found: $INPUT_FILE"
  exit 1
fi

echo "Running pipeline for dataset: $DATASET"

# Generate codegen types (idempotent)
npm run codegen

# Run the main CLI against the dataset
npx ts-node src/main.ts --input "$INPUT_FILE" --output output --parallel

echo "Done: output for $DATASET should be in output/"
</file>

<file path="src/parquetWriter.ts">
// src/parquetWriter.ts
// DuckDB-based Parquet file generation with schema metadata preservation

import * as fs from 'fs/promises';
import duckdb from 'duckdb';
import chalk from 'chalk';

/**
 * Runtime metadata about the data collection run
 */
export type RunMetadata = {
  queryType: string;
  timestamp: string;
  totalRepos: number;
  successfulRepos: number;
  failedRepos: number;
};

/**
 * Schema field definition from our schema.json files
 */
type SchemaField = {
  type: string;
  description: string;
  example: unknown;
  category: string;
};

/**
 * Our custom schema documentation format
 */
type SchemaDocumentation = {
  title: string;
  version: string;
  description: string;
  generated_at: string;
  fields: Record<string, SchemaField>;
};

/**
 * Generate Parquet files from JSON data using DuckDB.
 * Embeds field descriptions from our schema.json as Parquet key-value metadata.
 * 
 * @param basePath - Base path for output files (without extension)
 * @param analyzedJsonPath - Path to the analyzed JSON file
 * @param schemaJsonPath - Path to our schema documentation JSON
 * @param runMetadata - Optional runtime metadata to embed
 */
export async function generateParquetFiles(
  basePath: string,
  analyzedJsonPath: string,
  schemaJsonPath: string,
  runMetadata?: RunMetadata
): Promise<void> {
  try {
    // Check if input files exist
    await fs.access(analyzedJsonPath);
    await fs.access(schemaJsonPath);
    
    // Load schema to extract field descriptions
    const schemaContent = await fs.readFile(schemaJsonPath, 'utf-8');
    const schema: SchemaDocumentation = JSON.parse(schemaContent);
    
    // Build KV_METADATA from schema fields
    const kvMetadata = buildKvMetadataFromSchema(schema, runMetadata);
    
    // Generate Parquet file
    const parquetPath = `${basePath}-analyzed.parquet`;
    await convertJsonToParquet(analyzedJsonPath, parquetPath, kvMetadata);
    
    console.log(chalk.green(`✅ Parquet file: ${parquetPath}`));
    
  } catch (error) {
    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {
      console.log(chalk.yellow(`⚠️  Skipping Parquet generation: Input files not found`));
    } else {
      console.error(chalk.red(`Failed to generate Parquet files:`), error);
    }
  }
}

/**
 * Build DuckDB KV_METADATA clause from our schema documentation
 */
function buildKvMetadataFromSchema(
  schema: SchemaDocumentation,
  runMetadata?: RunMetadata
): Record<string, string> {
  const kv: Record<string, string> = {
    // Schema-level metadata
    'schema_title': schema.title,
    'schema_version': schema.version,
    'schema_description': schema.description,
    'schema_generated_at': schema.generated_at,
  };
  
  // Add runtime metadata if provided
  if (runMetadata) {
    kv['run_query_type'] = runMetadata.queryType;
    kv['run_timestamp'] = runMetadata.timestamp;
    kv['run_total_repos'] = runMetadata.totalRepos.toString();
    kv['run_successful_repos'] = runMetadata.successfulRepos.toString();
    kv['run_failed_repos'] = runMetadata.failedRepos.toString();
  }
  
  // Add all field descriptions from schema
  Object.entries(schema.fields).forEach(([fieldName, fieldDef]) => {
    kv[`field_${fieldName}`] = fieldDef.description;
  });
  
  return kv;
}

/**
 * Convert JSON file to Parquet using DuckDB with custom key-value metadata
 */
async function convertJsonToParquet(
  jsonPath: string,
  parquetPath: string,
  kvMetadata: Record<string, string>
): Promise<void> {
  return new Promise((resolve, reject) => {
    const db = new duckdb.Database(':memory:');
    
    // Build KV_METADATA SQL clause
    const kvMetadataEntries = Object.entries(kvMetadata)
      .map(([key, value]) => {
        // Escape single quotes in values for SQL
        const escapedValue = value.replace(/'/g, "''");
        return `${key}: '${escapedValue}'`;
      })
      .join(',\n        ');
    
    const sql = `
      COPY (
        SELECT * FROM read_json_auto('${jsonPath}')
      ) TO '${parquetPath}' (
        FORMAT PARQUET,
        COMPRESSION ZSTD,
        ROW_GROUP_SIZE 100000,
        KV_METADATA {
          ${kvMetadataEntries}
        }
      );
    `;
    
    db.exec(sql, (err: Error | null) => {
      db.close();
      
      if (err) {
        reject(err);
      } else {
        resolve();
      }
    });
  });
}

/**
 * Query Parquet key-value metadata (for validation/debugging)
 */
export async function readParquetMetadata(parquetPath: string): Promise<Record<string, string>> {
  return new Promise((resolve, reject) => {
    const db = new duckdb.Database(':memory:');
    
    db.all(
      `SELECT * FROM parquet_kv_metadata('${parquetPath}')`,
      (err: Error | null, rows: unknown[]) => {
        db.close();
        
        if (err) {
          reject(err);
        } else {
          // Convert rows to key-value map
          const metadata: Record<string, string> = {};
          (rows as Array<{ key: string; value: string }>).forEach(row => {
            metadata[row.key] = row.value;
          });
          resolve(metadata);
        }
      }
    );
  });
}
</file>

<file path="src/rawResponseWriter.ts">
// src/rawResponseWriter.ts
// This module handles writing raw API responses to JSONL files with metadata.
// Each line contains: {"metadata": {...}, "response": {...}}

import * as fs from 'fs/promises';
import chalk from 'chalk';

/**
 * Metadata attached to each raw API response
 */
export type RawResponseMetadata = {
  queryType: string;  // e.g., 'GetRepoDataArtifacts', 'GetRepoDataExtendedInfo'
  timestamp?: string; // ISO 8601 timestamp
  owner: string;      // Repository owner
  repo: string;       // Repository name
  maturity?: string;  // Project maturity level (e.g., 'graduated')
  inputs?: Record<string, unknown>; // Query variables used (optional)
};

/**
 * Structure of each line in the JSONL file
 */
export type RawResponseEntry = {
  metadata: RawResponseMetadata;
  response: unknown; // The full GraphQL API response
};

/**
 * Appends a raw API response to a JSONL file with metadata.
 * Creates the file if it doesn't exist.
 * 
 * @param filePath - Absolute path to the JSONL file
 * @param data - Object containing queryType, owner, repo, maturity, and response
 */
export async function appendRawResponse(
  filePath: string,
  data: {
    queryType: string;
    owner: string;
    repo: string;
    maturity?: string;
    response: unknown;
    inputs?: Record<string, unknown>;
  }
): Promise<void> {
  const entry: RawResponseEntry = {
    metadata: {
      queryType: data.queryType,
      timestamp: new Date().toISOString(),
      owner: data.owner,
      repo: data.repo,
      maturity: data.maturity,
      inputs: data.inputs || { owner: data.owner, name: data.repo },
    },
    response: data.response,
  };

  try {
    // Append as a single line (JSONL format)
    const line = JSON.stringify(entry) + '\n';
    await fs.appendFile(filePath, line, 'utf-8');
  } catch (error) {
    console.error(chalk.red(`Failed to write raw response to ${filePath}:`), error);
  }
}

/**
 * TODO (Phase 2): Read raw responses from JSONL file for mock mode
 * 
 * @param filePath - Absolute path to the JSONL file
 * @param owner - Repository owner to filter by
 * @param repo - Repository name to filter by
 * @returns The most recent raw response for the given repo, or null if not found
 */
export async function readRawResponse(
  filePath: string,
  owner: string,
  repo: string
): Promise<unknown | null> {
  // TODO: Implement for Phase 1 mock mode integration
  // 1. Read file line by line
  // 2. Parse each line as JSON
  // 3. Filter by owner/repo
  // 4. Return most recent entry (highest timestamp)
  // 5. Return null if not found
  
  // Stub: acknowledge parameters to avoid lint errors
  void filePath;
  void owner;
  void repo;
  
  console.warn(chalk.yellow(`TODO: readRawResponse not yet implemented. Falling back to src/mockdata/`));
  return null;
}

/**
 * TODO (Phase 2): Export individual mock files from JSONL
 * 
 * @param sourcePath - Path to source JSONL file
 * @param destDir - Destination directory for individual files
 */
export async function exportMocksFromJsonl(
  sourcePath: string,
  destDir: string
): Promise<void> {
  // TODO: Implement for Phase 2 export-mocks subcommand
  // 1. Read JSONL file line by line
  // 2. For each entry, create file: GetRepoData_{owner}_{repo}.json
  // 3. File contains just the response field (not metadata)
  // 4. Overwrite existing files
  
  // Stub: acknowledge parameters to avoid lint errors
  void sourcePath;
  void destDir;
  
  console.warn(chalk.yellow(`TODO: exportMocksFromJsonl not yet implemented`));
}
</file>

<file path="src/sizeReport.ts">
// src/sizeReport.ts
// Generates file size reports for output artifacts

import * as fs from 'fs/promises';
import * as path from 'path';
import chalk from 'chalk';

/**
 * Information about a file and its size
 */
type FileSizeInfo = {
  name: string;
  path: string;
  sizeBytes: number;
  sizeFormatted: string;
  category: string;
};

/**
 * Summary statistics for a category of files
 */
type CategorySummary = {
  category: string;
  fileCount: number;
  totalBytes: number;
  totalFormatted: string;
  files: FileSizeInfo[];
};

/**
 * Metadata about the data collection run
 */
type RunMetadata = {
  queryType: string;
  timestamp: string;
  totalRepos: number;
  successfulRepos: number;
  failedRepos: number;
};

/**
 * Format bytes to human-readable string (KB, MB, GB)
 */
function formatBytes(bytes: number): string {
  if (bytes === 0) return '0 B';
  
  const k = 1024;
  const sizes = ['B', 'KB', 'MB', 'GB'];
  const i = Math.floor(Math.log(bytes) / Math.log(k));
  
  return `${(bytes / Math.pow(k, i)).toFixed(2)} ${sizes[i]}`;
}

/**
 * Categorize a file based on its name pattern
 */
function categorizeFile(filename: string): string {
  if (filename === 'raw-responses.jsonl') {
    return 'Raw Responses';
  }
  if (filename.endsWith('-analyzed.json')) {
    return 'Analyzed Data';
  }
  if (filename.endsWith('-analyzed.parquet')) {
    return 'Analyzed Data';
  }
  if (filename.endsWith('.csv')) {
    return 'Analyzed Data';
  }
  if (filename.endsWith('-schema.json')) {
    return 'Schema Documentation';
  }
  if (filename === 'SIZE-REPORT.md') {
    return 'Reports';
  }
  
  return 'Other';
}

/**
 * Collect file sizes from a directory
 */
async function collectFileSizes(outputDir: string): Promise<FileSizeInfo[]> {
  const fileSizes: FileSizeInfo[] = [];
  
  try {
    const entries = await fs.readdir(outputDir, { withFileTypes: true });
    
    for (const entry of entries) {
      if (entry.isFile()) {
        const filePath = path.join(outputDir, entry.name);
        const stats = await fs.stat(filePath);
        
        fileSizes.push({
          name: entry.name,
          path: filePath,
          sizeBytes: stats.size,
          sizeFormatted: formatBytes(stats.size),
          category: categorizeFile(entry.name),
        });
      }
    }
  } catch (error) {
    console.error(chalk.red('Error collecting file sizes:'), error);
  }
  
  return fileSizes;
}

/**
 * Group files by category and calculate subtotals
 */
function groupFilesByCategory(files: FileSizeInfo[]): CategorySummary[] {
  const categories = new Map<string, FileSizeInfo[]>();
  
  // Group files by category
  files.forEach(file => {
    if (!categories.has(file.category)) {
      categories.set(file.category, []);
    }
    categories.get(file.category)!.push(file);
  });
  
  // Calculate summaries for each category
  const summaries: CategorySummary[] = [];
  
  categories.forEach((categoryFiles, categoryName) => {
    const totalBytes = categoryFiles.reduce((sum, file) => sum + file.sizeBytes, 0);
    
    summaries.push({
      category: categoryName,
      fileCount: categoryFiles.length,
      totalBytes,
      totalFormatted: formatBytes(totalBytes),
      files: categoryFiles.sort((a, b) => b.sizeBytes - a.sizeBytes), // Sort by size desc
    });
  });
  
  // Sort categories by total size (descending)
  return summaries.sort((a, b) => b.totalBytes - a.totalBytes);
}

/**
 * Generate a Markdown size report
 */
function generateMarkdownReport(
  summaries: CategorySummary[],
  totalBytes: number,
  metadata: RunMetadata
): string {
  const lines: string[] = [];
  
  // Header
  lines.push('# Output Size Report');
  lines.push('');
  lines.push(`**Generated:** ${new Date().toISOString()}`);
  lines.push(`**Dataset:** ${metadata.queryType}`);
  lines.push(`**Repositories:** ${metadata.totalRepos} (${metadata.successfulRepos} successful, ${metadata.failedRepos} failed)`);
  lines.push('');
  
  // Files by category
  lines.push('## Files by Category');
  lines.push('');
  
  summaries.forEach(summary => {
    lines.push(`### ${summary.category}`);
    lines.push('');
    
    summary.files.forEach(file => {
      lines.push(`- \`${file.name}\`: ${file.sizeFormatted}`);
    });
    
    lines.push('');
  });
  
  // Summary table
  lines.push('## Summary');
  lines.push('');
  lines.push('| Category | Files | Total Size |');
  lines.push('|----------|-------|------------|');
  
  summaries.forEach(summary => {
    lines.push(`| ${summary.category} | ${summary.fileCount} | ${summary.totalFormatted} |`);
  });
  
  const totalFileCount = summaries.reduce((sum, s) => sum + s.fileCount, 0);
  lines.push(`| **Total** | **${totalFileCount}** | **${formatBytes(totalBytes)}** |`);
  
  return lines.join('\n');
}

/**
 * Generate a size report for output files and save it to the output directory.
 * Also logs a summary to the console.
 * 
 * @param outputDir - Directory containing the generated output files
 * @param metadata - Metadata about the run
 */
export async function generateSizeReport(
  outputDir: string,
  metadata: RunMetadata
): Promise<void> {
  try {
    // Collect file sizes
    const files = await collectFileSizes(outputDir);
    
    if (files.length === 0) {
      console.log(chalk.yellow('⚠️  No files found for size report'));
      return;
    }
    
    // Group by category
    const summaries = groupFilesByCategory(files);
    
    // Calculate total
    const totalBytes = files.reduce((sum, file) => sum + file.sizeBytes, 0);
    
    // Generate Markdown report
    const markdown = generateMarkdownReport(summaries, totalBytes, metadata);
    
    // Save to file
    const reportPath = path.join(outputDir, 'SIZE-REPORT.md');
    await fs.writeFile(reportPath, markdown);
    
    // Console output
    console.log(chalk.bold.blue('\n📊 Output Size Summary'));
    console.log(chalk.gray('─'.repeat(60)));
    
    summaries.forEach(summary => {
      console.log(chalk.bold(`\n${summary.category}:`));
      summary.files.forEach(file => {
        const padding = 50 - file.name.length;
        console.log(`  ${chalk.cyan(file.name)}${' '.repeat(Math.max(0, padding))} ${chalk.green(file.sizeFormatted)}`);
      });
    });
    
    console.log(chalk.gray('\n' + '─'.repeat(60)));
    const totalFileCount = summaries.reduce((sum, s) => sum + s.fileCount, 0);
    console.log(chalk.bold(`Total: ${totalFileCount} files, ${chalk.green(formatBytes(totalBytes))}`));
    console.log(chalk.gray('─'.repeat(60)));
    
    console.log(chalk.green(`\n✅ Size report saved: ${reportPath}`));
    
  } catch (error) {
    console.error(chalk.red('Failed to generate size report:'), error);
  }
}
</file>

<file path=".env.template">
GITHUB_PAT=ghp_YourPersonalAccessTokenHere
</file>

<file path=".markdownlintrc">
{
  "MD013": false
}
</file>

<file path="test-extended.sh">
#!/usr/bin/env bash
# Test script for extended query functionality
# Tests both GetRepoDataArtifacts and GetRepoDataExtendedInfo queries
# against a single repository using mock data

set -e  # Exit on error

echo "=== Extended Query Test Suite (Mock Mode) ==="
echo ""
echo "⚠️  Note: Mock mode does not save raw responses to JSONL."
echo "    This test verifies query type handling and workflow detection."
echo ""

# Clean up any existing output
echo "🧹 Cleaning output directory..."
rm -f output/test-single-analyzed.json
rm -f output/test-single.csv
rm -f output/test-single-schema.json
echo ""

# Test 1: Run with artifacts query (default)
echo "📦 Test 1: Running with GetRepoDataArtifacts (default)..."
npm run start -- --input input/test-single.jsonl --mock --verbose
echo "✅ Artifacts query completed"
echo ""

# Test 2: Run with extended query
echo "🔍 Test 2: Running with GetRepoDataExtendedInfo (--extended)..."
npm run start -- --input input/test-single.jsonl --mock --extended --verbose
echo "✅ Extended query completed"
echo ""

# Validation
echo "🔎 Validating results..."
echo ""

# Check analyzed JSON exists
if [ ! -f output/test-single-analyzed.json ]; then
  echo "❌ test-single-analyzed.json not found!"
  exit 1
fi
echo "✅ Analyzed JSON exists"

# Check for workflow-related fields in extended query
echo ""
echo "Checking for workflow detection in extended query:"
if grep -q '"workflows"' output/test-single-analyzed.json; then
  echo "✅ workflows field present"
  
  # Show what workflows and tools were detected
  echo ""
  echo "Detected workflows and security tools:"
  node -e "
    const data = require('./output/test-single-analyzed.json');
    data.forEach(repo => {
      console.log(\`  \${repo.repository.owner}/\${repo.repository.name}:\`);
      if (repo.workflows && repo.workflows.length > 0) {
        repo.workflows.forEach(workflow => {
          console.log(\`    \${workflow.name}:\`);
          if (workflow.detectedSbomTools && workflow.detectedSbomTools.length > 0) {
            workflow.detectedSbomTools.forEach(tool => console.log(\`      - \${tool}\`));
          } else {
            console.log('      (no tools detected)');
          }
        });
      } else {
        console.log('    (no workflows)');
      }
      console.log(\`    Summary sbomCiTools: \${JSON.stringify(repo.summary.sbomCiTools)}\`);
    });
  "
else
  echo "⚠️  workflows field not found"
fi

# Check CSV and schema exist
if [ ! -f output/test-single.csv ]; then
  echo "❌ CSV not found!"
  exit 1
fi
echo "✅ CSV exists"

if [ ! -f output/test-single-schema.json ]; then
  echo "❌ Schema JSON not found!"
  exit 1
fi
echo "✅ Schema JSON exists"

echo ""
echo "=== All Tests Passed! ==="
echo ""
echo "📊 Generated files:"
echo "  - output/test-single-analyzed.json"
echo "  - output/test-single.csv"
echo "  - output/test-single-schema.json"
echo ""
echo "💡 To test with real API and verify JSONL generation:"
echo "   ./test-extended.sh --real-api"
</file>

<file path="docs/decisions.md">
# Architecture Decisions

## Table of Contents

- [2025-10-06: Initial Feedback](#2025-10-06-initial-feedback)
  - [Original State](#original-state)
  - [Changes Made](#changes-made)
- [GraphQL Architecture](#graphql-architecture)
- [Data Strategy](#data-strategy)
- [Type Safety and Code Generation](#type-safety-and-code-generation)
- [Development Standards](#development-standards)

## 2025-10-06: Initial Feedback

### Original State

The tool started as a single GraphQL query (`GetRepoData`) that fetched:

- Last 1 release with up to 50 artifacts
- All workflow files from `.github/workflows` (full YAML content)
- Basic repo metadata

Output was JSON and CSV only. The query mixed artifact detection with workflow analysis in one call.

### Changes Made

Based on feedback and active iteration, I'm making the following changes to the tool:

- The tool continues to collect data for a set of repos (coming soon, full org scan) as input, and provides Sandbox, Incubating, and Graduated input config files.
- For each repo, run queries (2 presently)
  - search for the existence of SBOM's and other security artifacts in the release assets.
  - search CI workflows for existence of tools / actions that generate security artifacts
- artifacts generated are:
  - the raw response data (JSON)
  - A report on findings (including .csv, and .jsonl)
  - parquet schema file containing the descriptions and type info from the GraphQL Schema for all returned types. Used to generate documentation, and to validate future data scrapes.
  - codegen created typed Typescript SDK.
  - (experimenting) a JSON "instrospection style" schema for returned types.
- removing wasm and parquet (data) output, I was experimenting, that was a good call-out

Also, as we discussed, working on a notebook to walk thru results.

I've also taken an action item to follow up with CNCF Infra to ensure we can run CI jobs for the tool itself, as well as a daily or weekly scrape.

Targeting community demo for TAG-SC on 10/22.

## GraphQL Architecture

### Query Splitting

**Decision**: Split into multiple specialized GraphQL queries instead of one mega-query.

**Rationale**:

- API rate limits matter. Don't fetch workflow YAML if you only need release artifacts.
- Each query returns a distinct type. No union types, no runtime type guards.
- Easy to extend - just add another query file and API function.

**Implementation**:

- `GetRepoDataArtifacts`: Release artifacts only (default)
- `GetRepoDataExtendedInfo`: Adds workflows, security policies, etc.

See `docs/QUERY-ARCHITECTURE.md` for details.

### No Union Types

**Decision**: Each API function returns its own specific query type.

**Rationale**: Union types require runtime checking and make code harder to reason about. TypeScript should handle everything at compile time.

**Example**:

```typescript
// Good
function fetchRepositoryArtifacts(): GetRepoDataArtifactsQuery | null
function fetchRepositoryExtendedInfo(): GetRepoDataExtendedInfoQuery | null

// Bad
type RepoData = ArtifactsQuery | ExtendedQuery  // Requires runtime type guards
```

### Presence Detection Only

**Decision**: Tool reports what security artifacts exist, not what's in them.

**Rationale**: Scope creep is real. Parsing SBOMs, validating signatures, etc. are separate concerns. This tool answers "does this repo ship SBOMs?" not "is the SBOM valid?".

**Detection targets**:

- Release artifacts: SBOMs, signatures, attestations
- CI workflows: Tool invocations (syft, cosign, etc.)

### Release Fetching

**Decision**: Fetch last 5 releases, up to 50 artifacts per release.

**Rationale**:

- Covers recent history without overloading the API.
- 50 artifacts handles most repos. Exceptions can be addressed later.
- Ordered by creation date (newest first).

### Workflow Analysis

**Decision**: Not implemented yet, but query is ready.

**Rationale**:

- Fetching workflow YAML is expensive (rate limits).
- Most use cases only need artifact presence.
- Query exists in `GetRepoDataExtendedInfo` for when we need it.

**Future**: Add `--extended` flag to main.ts to enable workflow analysis.

## Data Strategy

### Output Formats

**Decision**: Generate JSON, CSV, and Parquet files with embedded schema metadata.

**Rationale**:

- JSON is the source of truth and easiest to work with.
- CSV for spreadsheet analysis without code.
- Parquet files with embedded schema metadata for efficient querying and long-term archival.
- Schema metadata embedded as key-value pairs enables self-documenting datasets.

**Parquet Generation**: Implemented using DuckDB for JSON-to-Parquet conversion with schema metadata embedding. See [Viewing Parquet Data](./viewing-parquet-data.md) for exploration tools and techniques.

### Normalized CSV Structure

**Decision**: Flatten hierarchical JSON into wide CSV with one row per artifact.

**Rationale**:

- Easier to analyze in spreadsheets and SQL.
- Repository and release info duplicated across artifact rows.
- JSON arrays encoded as strings in CSV.

**Tradeoff**: Some data duplication, but worth it for analysis tools that expect flat tables.

### Schema Documentation

**Decision**: Generate schema docs from actual output files, not just TypeScript types.

**Rationale**:

- What actually gets written matters more than what the types say.
- Validates that output matches expectations.
- Self-documenting based on real data.

**Tools**: `scripts/generate-schema-docs.ts` reads JSON output and generates `docs/schema.md`.

### Mock Data Strategy

**Decision**: Mock files use the old `GetRepoData` naming but contain full extended data.

**Rationale**:

- TypeScript extracts only the fields defined in the query type.
- One set of mocks supports multiple queries.
- Extra fields in mocks are safely ignored.

## Type Safety and Code Generation

### GraphQL Code Generation

**Decision**: Use `@graphql-codegen/cli` with client preset.

**Rationale**:

- Full end-to-end type safety from query to response.
- Schema changes caught at compile time.
- Generated code goes in `src/generated/` and is excluded from linting.

**Process**: Edit `.graphql` files → run `npm run codegen` → get typed SDK.

### TypeScript Configuration

**Decision**: Use Node16 module resolution and ES2022 target.

**Rationale**:

- Modern package.json exports work correctly.
- Matches current Node.js LTS behavior.
- Avoids CommonJS/ESM issues.

## Development Standards

### Linting

**Decision**: ESLint on all hand-written code. Generated code excluded.

**Rationale**:

- Zero tolerance for warnings in code we write.
- Generated code gets `/* eslint-disable */` header.
- `src/generated/` in `.eslintignore`.

**Standard**: All commits must be ESLint clean.

### No Vendor Directory

**Decision**: Removed `vendor/` directory with graphql-to-parquet spec.

**Rationale**: Not using it. Keep the repo focused on what's actually implemented.
</file>

<file path="scripts/fetch-cncf-landscape.ts">
// This script fetches the CNCF landscape YAML and generates input files for Sandbox, Incubation, and Graduated projects.
// It is intended to be run with: npx ts-node scripts/fetch-cncf-landscape.ts
// Requires: node-fetch and yaml (install with npm install node-fetch@2 yaml)

import fetch from 'node-fetch';
import * as fs from 'fs';
import * as path from 'path';
import * as yaml from 'yaml';

// URL of the CNCF landscape YAML file
const CNCF_YML_URL = 'https://raw.githubusercontent.com/cncf/landscape/refs/heads/master/landscape.yml';
const OUTPUT_DIR = path.join(__dirname, '../input');
const YML_FILE = path.join(__dirname, 'landscape.yml');

// Helper type for a repo entry, now including maturity
interface RepoTarget {
  owner: string;
  name: string;
  maturity: string;
}

// Helper function to recursively find all items with a repo_url and project maturity
function findAllRepos(obj: any, results: RepoTarget[] = []): RepoTarget[] {
  if (obj && typeof obj === 'object') {
    // If this object has a repo_url and a project status, extract the data
    if (obj.repo_url && obj.project && typeof obj.repo_url === 'string' && typeof obj.project === 'string') {
      const maturity = obj.project.toLowerCase().replace('incubating', 'incubation');
      // We are interested in all maturity levels
      if (['graduated', 'incubation', 'sandbox'].includes(maturity)) {
        const match = obj.repo_url.match(/github.com\/([^/]+)\/([^/]+)$/);
        if (match) {
          results.push({ owner: match[1], name: match[2], maturity: maturity });
        }
      }
    }
    // Recurse through the object
    for (const key of Object.keys(obj)) {
      findAllRepos(obj[key], results);
    }
  } else if (Array.isArray(obj)) {
    for (const item of obj) {
      findAllRepos(item, results);
    }
  }
  return results;
}


async function main() {
  // Ensure output directory exists
  if (!fs.existsSync(OUTPUT_DIR)) {
    fs.mkdirSync(OUTPUT_DIR, { recursive: true });
  }

  // Download the YAML file
  console.log('Downloading CNCF landscape.yml...');
  const res = await fetch(CNCF_YML_URL);
  if (!res.ok) {
    throw new Error(`Failed to fetch YAML: ${res.statusText}`);
  }
  const ymlText = await res.text();
  fs.writeFileSync(YML_FILE, ymlText);

  // Parse YAML
  const doc = yaml.parse(ymlText);

  // Find all repos and their maturities in a single pass
  console.log('Extracting all CNCF projects...');
  const allRepos = findAllRepos(doc);

  // Write to a single consolidated file
  const consolidatedOutPath = path.join(OUTPUT_DIR, 'cncf-all.jsonl');
  const consolidatedLines = allRepos.map(r => JSON.stringify(r)).join('\n');
  fs.writeFileSync(consolidatedOutPath, consolidatedLines + (consolidatedLines ? '\n' : ''));
  console.log(`  Wrote ${allRepos.length} total repos to ${consolidatedOutPath}`);

  // Also write the individual files for compatibility with old scripts/commands
  for (const maturity of ['sandbox', 'incubation', 'graduated']) {
    console.log(`Extracting ${maturity} projects for compatibility...`);
    const outPath = path.join(OUTPUT_DIR, `${maturity}.jsonl`);
    const repos = allRepos.filter(r => r.maturity === maturity).map(({owner, name}) => ({owner, name}));
    const lines = repos.map(r => JSON.stringify(r)).join('\n');
    fs.writeFileSync(outPath, lines + (lines ? '\n' : ''));
    console.log(`  Wrote ${repos.length} repos to ${outPath}`);
  }

  console.log('Done.');
}

main().catch(err => {
  console.error('Error:', err);
  process.exit(1);
});
</file>

<file path="scripts/generate-schema-docs.sh">
#!/bin/bash

# generate-schema-docs.sh
# Generates comprehensive schema documentation from JSON data files and schema definitions

set -e

echo "🔄 Generating schema documentation from output files..."

# Run the TypeScript schema generator
npx ts-node scripts/generate-schema-docs.ts

echo "✨ Schema documentation generation complete!"

This document describes the data schema for the GitHub Repository Supply Chain Security Analyzer. The tool outputs normalized data in JSON and CSV formats, along with Parquet schema files for downstream conversion.

## Generated Schema

*This documentation is automatically generated. Do not edit manually.*
*Last updated: $(date)*

EOF

# Extract schema from TypeScript types
echo "📊 Extracting schema from TypeScript definitions..."

# Get the NormalizedRow type definition
node -e "
const fs = require('fs');
const reportContent = fs.readFileSync('src/report.ts', 'utf8');

// Extract NormalizedRow interface
const normalizedRowMatch = reportContent.match(/interface NormalizedRow \{([^}]+)\}/s);
if (!normalizedRowMatch) {
  console.error('Could not find NormalizedRow interface');
  process.exit(1);
}

const fields = normalizedRowMatch[1]
  .split('\n')
  .map(line => line.trim())
  .filter(line => line && !line.startsWith('//') && !line.startsWith('/*'))
  .map(line => {
    const match = line.match(/(\w+)\??\s*:\s*([^;]+);?/);
    if (match) {
      const [, name, type] = match;
      const isOptional = line.includes('?');
      return {
        name,
        type: type.replace(/\s*\/\/.*$/, '').trim(),
        optional: isOptional,
        description: ''
      };
    }
    return null;
  })
  .filter(Boolean);

console.log('## Fields\n');
console.log('| Field | Type | Optional | Description |');
console.log('|-------|------|----------|-------------|');

fields.forEach(field => {
  const optional = field.optional ? '✓' : '';
  console.log(\`| \${field.name} | \\\`\${field.type}\\\` | \${optional} | TODO: Add description |\`);
});

console.log('\n## Field Categories\n');

// Categorize fields
const categories = {
  'Repository Identity': ['id', 'nodeId', 'name', 'owner', 'fullName', 'url'],
  'Repository Metadata': ['description', 'primaryLanguage', 'languages', 'topics', 'isPrivate', 'isFork', 'isArchived'],
  'Repository Statistics': ['starCount', 'forkCount', 'watcherCount', 'issueCount', 'pullRequestCount'],
  'Licensing & Security': ['license', 'hasVulnerabilityAlerts', 'hasSecurityPolicy', 'hasDependabot'],
  'Release Information': ['latestReleaseTag', 'latestReleaseDate', 'releaseCount'],
  'Artifact Detection': ['artifactTypes', 'artifactCount', 'hasSBOM', 'hasSLSA', 'hasInToto', 'hasVEX'],
  'CI/CD Analysis': ['ciTools', 'ciToolCount', 'hasGitHubActions', 'hasOtherCI'],
  'Quality Metrics': ['hasReadme', 'hasContributing', 'hasCodeOfConduct', 'hasLicense'],
  'Branch Protection': ['hasProtectedBranches', 'branchProtectionRules'],
  'Timestamps': ['createdAt', 'updatedAt', 'pushedAt', 'analysisDate']
};

Object.entries(categories).forEach(([category, fieldNames]) => {
  console.log(\`### \${category}\n\`);
  const categoryFields = fields.filter(f => fieldNames.includes(f.name));
  if (categoryFields.length > 0) {
    categoryFields.forEach(field => {
      console.log(\`- **\${field.name}** (\\\`\${field.type}\\\`): TODO: Add description\`);
    });
    console.log('');
  }
});
" >> docs/schema.md

# Generate JSON schema
echo "📄 Creating JSON schema..."
node -e "
const fs = require('fs');
const reportContent = fs.readFileSync('src/report.ts', 'utf8');

// Extract field information for JSON schema
const normalizedRowMatch = reportContent.match(/interface NormalizedRow \{([^}]+)\}/s);
if (!normalizedRowMatch) {
  console.error('Could not find NormalizedRow interface');
  process.exit(1);
}

const schema = {
  '\$schema': 'http://json-schema.org/draft-07/schema#',
  title: 'GitHub Repository Supply Chain Security Analysis',
  description: 'Schema for normalized repository analysis data',
  type: 'object',
  properties: {},
  required: []
};

const fields = normalizedRowMatch[1]
  .split('\n')
  .map(line => line.trim())
  .filter(line => line && !line.startsWith('//') && !line.startsWith('/*'))
  .map(line => {
    const match = line.match(/(\w+)\??\s*:\s*([^;]+);?/);
    if (match) {
      const [, name, type] = match;
      const isOptional = line.includes('?');
      
      let jsonType = 'string';
      if (type.includes('number')) jsonType = 'number';
      else if (type.includes('boolean')) jsonType = 'boolean';
      else if (type.includes('Date')) jsonType = 'string';
      else if (type.includes('[]')) jsonType = 'array';
      
      return { name, type: jsonType, optional: isOptional };
    }
    return null;
  })
  .filter(Boolean);

fields.forEach(field => {
  schema.properties[field.name] = { type: field.type };
  if (!field.optional) {
    schema.required.push(field.name);
  }
});

fs.writeFileSync('docs/schema.json', JSON.stringify(schema, null, 2));
console.log('✅ JSON schema generated: docs/schema.json');
" 2>/dev/null || echo "⚠️  Could not generate JSON schema"

# Add usage examples to schema documentation
cat >> docs/schema.md << 'EOF'

## Usage Examples

### Loading Data in Python

```python
import pandas as pd
import json

# Load CSV data
df = pd.read_csv('output/analysis.csv')

# Load JSON data  
with open('output/analysis.json', 'r') as f:
    data = json.load(f)

# Convert to DataFrame
df = pd.json_normalize(data)
```

### Loading Data in R

```r
library(jsonlite)
library(readr)

# Load CSV data
df <- read_csv("output/analysis.csv")

# Load JSON data
data <- fromJSON("output/analysis.json")
df <- as.data.frame(data)
```

### Loading Parquet Data

```python
import pandas as pd

# Load Parquet data (when available)
df = pd.read_parquet('output/analysis.parquet')
```

## Data Quality Notes

- All timestamps are in ISO 8601 format
- Array fields in CSV are JSON-encoded strings
- Missing values are represented as null/NULL/NA depending on format
- Boolean fields use true/false values

## Schema Versioning

This schema follows semantic versioning:
- Major version: Breaking changes to field names or types
- Minor version: New fields added
- Patch version: Documentation or validation updates

Current version: 0.0.0

EOF

echo "✅ Schema documentation generated successfully!"
echo "📁 Files created:"
echo "   - docs/schema.md"
echo "   - docs/schema.json"

# Validate the generated files
if [ -f "docs/schema.md" ] && [ -f "docs/schema.json" ]; then
    echo "🎉 All schema documentation files generated successfully!"
else
    echo "❌ Some files failed to generate"
    exit 1
fi
</file>

<file path="scripts/generate-schema-docs.ts">
#!/usr/bin/env npx ts-node

/**
 * Schema Documentation Generator
 * 
 * Reads JSON output files to extract schema and generates comprehensive documentation.
 * This script generates schema.md with field descriptions, types, and examples.
 */

import { promises as fs } from 'fs';
import path from 'path';

interface SchemaField {
  name: string;
  type: string;
  nullable: boolean;
  description: string;
  category: string;
  example?: unknown;
}

interface SchemaDocumentation {
  title: string;
  description: string;
  version: string;
  generatedAt: string;
  sourceFile: string;
  fields: SchemaField[];
  categories: Record<string, SchemaField[]>;
  totalFields: number;
}

/**
 * Find the most recent JSON data file or schema file
 */
async function findDataFile(): Promise<string> {
  const outputDir = 'output';
  
  try {
    const files = await fs.readdir(outputDir);
    
    // Look for schema JSON files first
    let dataFiles = files.filter(file => file.endsWith('-schema.json'));
    
    // If no schema files, look for regular JSON files
    if (dataFiles.length === 0) {
      dataFiles = files.filter(file => file.endsWith('.json') && !file.includes('parquet'));
      
      if (dataFiles.length === 0) {
        throw new Error(`No JSON data files found in output directory. Run a test first: npm run test:single`);
      }
    }
    
    // Get the most recent file
    const fileStats = await Promise.all(
      dataFiles.map(async (file: string) => {
        const filePath = path.join(outputDir, file);
        return {
          file: filePath,
          mtime: (await fs.stat(filePath)).mtime
        };
      })
    );
    
    fileStats.sort((a: { mtime: Date }, b: { mtime: Date }) => b.mtime.getTime() - a.mtime.getTime());
    return fileStats[0].file;
  } catch (error) {
    throw new Error(`Error finding JSON data files: ${error}`);
  }
}

/**
 * Extract schema from JSON data file
 */
async function extractJsonSchema(filePath: string): Promise<SchemaField[]> {
  try {
    // Try to read JSON data directly
    let jsonPath = filePath;
    
    // If it's a schema file, look for the corresponding data file
    if (filePath.endsWith('-schema.json')) {
      // Try to find corresponding .json file (not .parquet.json)
      const baseName = path.basename(filePath, '-schema.json');
      const dataPath = path.join(path.dirname(filePath), `${baseName}.json`);
      
      if (await fs.access(dataPath).then(() => true).catch(() => false)) {
        jsonPath = dataPath;
      } else {
        throw new Error(`Cannot find data file for schema ${filePath}`);
      }
    }
    
    console.log(`📖 Reading JSON data from: ${jsonPath}`);
    
    const content = await fs.readFile(jsonPath, 'utf-8');
    const data = JSON.parse(content);
    
    // Assume data is an array of objects with a normalized structure
    if (!Array.isArray(data) || data.length === 0) {
      throw new Error('Expected JSON data to be a non-empty array');
    }
    
    const sampleRow = data[0];
    const fields: SchemaField[] = [];
    
    for (const [key, value] of Object.entries(sampleRow)) {
      fields.push({
        name: key,
        type: inferTypeFromValue(value),
        nullable: value === null || value === undefined,
        description: getFieldDescription(key),
        category: categorizeField(key),
        example: value
      });
    }
    
    return fields;
  } catch (error) {
    throw new Error(`Error extracting schema from JSON: ${error}`);
  }
}

/**
 * Categorize field for documentation organization
 */
function categorizeField(fieldName: string): string {
  if (fieldName.startsWith('repository_') || fieldName.startsWith('repo_')) {
    return 'repository';
  }
  if (fieldName.startsWith('workflow_')) {
    return 'workflows';
  }
  if (fieldName.startsWith('release_')) {
    return 'releases';
  }
  if (fieldName.startsWith('artifact_')) {
    return 'artifacts';
  }
  return 'metadata';
}

/**
 * Infer TypeScript type from value
 */
function inferTypeFromValue(value: unknown): string {
  if (value === null || value === undefined) return 'string | null';
  if (typeof value === 'boolean') return 'boolean';
  if (typeof value === 'number') return Number.isInteger(value) ? 'number' : 'number';
  if (typeof value === 'string') {
    // Check if it's a JSON array
    if (value.startsWith('[') && value.endsWith(']')) {
      return 'string'; // JSON-encoded array
    }
    // Check if it's a date
    if (value.match(/^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/)) {
      return 'string'; // ISO date string
    }
    return 'string';
  }
  if (Array.isArray(value)) return 'string[]';
  return 'object';
}

/**
 * Get field description based on name
 */
function getFieldDescription(fieldName: string): string {
  const descriptions: Record<string, string> = {
    // Repository Identity
    'repository_id': 'Unique repository identifier (GitHub URL)',
    'repository_owner': 'GitHub organization or user name',
    'repository_name': 'Repository name without owner',
    'repository_name_with_owner': 'Full repository path (owner/name)',
    'repository_url': 'GitHub repository URL',
    'repository_description': 'Repository description text',
    
    // Security Artifacts
    'repo_has_sbom_artifact': 'Repository contains SBOM (Software Bill of Materials) files',
    'repo_has_signature_artifact': 'Repository contains cryptographic signature files',
    'repo_has_attestation_artifact': 'Repository contains attestation files',
    'artifact_is_sbom': 'This artifact is an SBOM file',
    'artifact_is_signature': 'This artifact is a cryptographic signature',
    'artifact_is_attestation': 'This artifact is an attestation document',
    'artifact_is_vex': 'This artifact is a VEX (Vulnerability Exchange) document',
    'artifact_is_slsa_provenance': 'This artifact contains SLSA provenance information',
    'artifact_is_in_toto_link': 'This artifact is an in-toto link metadata file',
    'artifact_is_container_attestation': 'This artifact is a container attestation',
    
    // CI/CD Security Tools
    'repo_security_tools_json': 'JSON array of detected security tools across the repository',
    'repo_security_tools_count': 'Total count of unique security tool types',
    'workflow_security_tools_json': 'JSON array of security tools found in each workflow',
    'workflow_count': 'Total number of CI/CD workflow files',
    'workflow_names_json': 'JSON array of workflow file names',
    
    // Release Information
    'release_index': 'Zero-based index of this release (0 = latest)',
    'release_tag_name': 'Git tag name for this release',
    'release_name': 'Display name of the release',
    'release_created_at': 'ISO timestamp of release creation',
    'release_artifact_count': 'Total number of artifacts in this release',
    'release_has_container_images': 'Release contains container images',
    'release_has_slsa_provenance': 'Release contains SLSA provenance documents',
    'release_has_in_toto_attestation': 'Release contains in-toto attestations',
    'release_sbom_formats_json': 'JSON array of SBOM formats found in release',
    
    // Artifact Details
    'artifact_index': 'Zero-based index of this artifact within the release',
    'artifact_name': 'Filename of the artifact',
    'artifact_download_url': 'Direct download URL for the artifact',
    'artifact_size_hint': 'Estimated size of the artifact',
    'artifact_file_extension': 'File extension extracted from artifact name',
    'artifact_type': 'Classified type of artifact (signature, sbom, etc.)',
    'artifact_platform_hint': 'Detected platform/architecture hints',
    'artifact_sbom_format': 'Specific SBOM format if this is an SBOM file'
  };
  
  return descriptions[fieldName] || 'Supply chain security metadata field';
}



/**
 * Generate comprehensive schema documentation
 */
async function generateSchemaDocumentation(fields: SchemaField[], sourceFile: string): Promise<SchemaDocumentation> {
  // Group fields by category
  const categories: Record<string, SchemaField[]> = {};
  for (const field of fields) {
    if (!categories[field.category]) {
      categories[field.category] = [];
    }
    categories[field.category].push(field);
  }
  
  return {
    title: 'GitHub Supply Chain Security Analysis Schema',
    description: 'Normalized data schema for supply chain security analysis of GitHub repositories',
    version: '1.0.0',
    generatedAt: new Date().toISOString(),
    sourceFile: path.basename(sourceFile),
    fields,
    categories,
    totalFields: fields.length
  };
}

/**
 * Write Markdown documentation
 */
async function writeMarkdownDocs(schema: SchemaDocumentation): Promise<void> {
  const docsDir = 'docs';
  await fs.mkdir(docsDir, { recursive: true });
  
  const markdown = `# ${schema.title}

## Overview

${schema.description}

**Generated from:** \`${schema.sourceFile}\`  
**Generated at:** ${new Date(schema.generatedAt).toLocaleString()}  
**Total fields:** ${schema.totalFields}

## Schema Fields

| Field | Type | Nullable | Category | Description |
|-------|------|----------|----------|-------------|
${schema.fields.map(field => 
  `| \`${field.name}\` | \`${field.type}\` | ${field.nullable ? '✓' : ''} | ${field.category} | ${field.description} |`
).join('\n')}

## Field Categories

${Object.entries(schema.categories).map(([category, categoryFields]) => `
### ${category}

${categoryFields.map(field => 
  `- **\`${field.name}\`** (\`${field.type}\`): ${field.description}`
).join('\n')}
`).join('')}

## Usage Examples

### Loading in Python (Pandas)

\`\`\`python
import pandas as pd

# Load CSV data
df = pd.read_csv('output/analysis.csv')

# Load Parquet data (more efficient)
df = pd.read_parquet('output/analysis.parquet')

# Display schema info
print(df.info())
print(f"Total rows: {len(df)}")
print(f"Total repositories: {df['repository_name'].nunique()}")
\`\`\`

### Loading in R

\`\`\`r
library(arrow)
library(dplyr)

# Load Parquet data
df <- read_parquet("output/analysis.parquet")

# Display schema
glimpse(df)

# Summary statistics
df %>% 
  group_by(repository_name) %>% 
  summarise(
    total_artifacts = n(),
    has_sbom = any(artifact_is_sbom),
    has_signatures = any(artifact_is_signature)
  )
\`\`\`

### Loading in JavaScript/Node.js

\`\`\`javascript
// Load JSON data
const data = require('./output/analysis.json');

// Load using Apache Arrow (for Parquet)
const { tableFromIPC } = require('apache-arrow');
const fs = require('fs');

// Process data
const repositories = [...new Set(data.map(row => row.repository_name))];
console.log(\`Found \${repositories.length} repositories\`);
\`\`\`

## Data Quality Notes

- All timestamps are in ISO 8601 format
- JSON array fields are stored as JSON-encoded strings in CSV format
- Boolean fields use \`true\`/\`false\` values
- Missing values are represented as \`null\`
- Platform hints are comma-separated values (e.g., "linux,amd64")

## Schema Evolution

This schema follows semantic versioning principles:
- **Major version**: Breaking changes to field names or types
- **Minor version**: New fields added
- **Patch version**: Documentation updates or bug fixes

Current version: ${schema.version}
`;

  await fs.writeFile(path.join(docsDir, 'schema.md'), markdown);
  console.log('✅ Markdown documentation saved to: docs/schema.md');
}

/**
 * Write JSON schema
 */
async function writeJSONSchema(schema: SchemaDocumentation): Promise<void> {
  const docsDir = 'docs';
  const jsonPath = path.join(docsDir, 'schema.json');
  
  await fs.writeFile(jsonPath, JSON.stringify(schema, null, 2));
  console.log('✅ JSON schema saved to: docs/schema.json');
}

/**
 * Main execution
 */
async function main(): Promise<void> {
  try {
    console.log('🔄 Generating schema documentation from JSON data files...');
    
    // Find the most recent data file
    const dataFile = await findDataFile();
    console.log(`✅ Found data file: ${dataFile}`);
    
    // Extract schema
    const fields = await extractJsonSchema(dataFile);
    
    // Generate documentation
    const schema = await generateSchemaDocumentation(fields, dataFile);
    
    // Write documentation files
    await writeMarkdownDocs(schema);
    await writeJSONSchema(schema);
    
    console.log('🎉 Schema documentation generated successfully!');
    console.log(`📁 Files created:`);
    console.log(`   - docs/schema.md`);
    console.log(`   - docs/schema.json`);
    
  } catch (error) {
    console.error('❌ Error generating schema documentation:', error);
    process.exit(1);
  }
}

// Run if called directly
if (require.main === module) {
  main();
}

export { main as generateSchemaDocs };
</file>

<file path="scripts/view-parquet.sh">
#!/usr/bin/env bash
# scripts/view-parquet.sh
# Interactive DuckDB session for exploring Parquet files
#
# Usage:
#   ./scripts/view-parquet.sh <parquet-file>
#   ./scripts/view-parquet.sh output/run-20250108-120000/test-single-analyzed.parquet
#
# This script opens an interactive DuckDB session with the specified Parquet file
# loaded as a table named 'data'. You can then run SQL queries interactively.

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Check for required arguments
if [ $# -eq 0 ]; then
    echo -e "${RED}Error: No Parquet file specified${NC}"
    echo ""
    echo "Usage: $0 <parquet-file>"
    echo ""
    echo "Examples:"
    echo "  $0 output/run-20250108-120000/test-single-analyzed.parquet"
    echo "  $0 output/latest/sandbox-analyzed.parquet"
    exit 1
fi

PARQUET_FILE="$1"

# Check if file exists
if [ ! -f "$PARQUET_FILE" ]; then
    echo -e "${RED}Error: File not found: $PARQUET_FILE${NC}"
    exit 1
fi

# Check if duckdb is installed
if ! command -v duckdb &> /dev/null; then
    echo -e "${RED}Error: DuckDB is not installed${NC}"
    echo ""
    echo "Install DuckDB using one of these methods:"
    echo "  - macOS: brew install duckdb"
    echo "  - Linux/macOS: curl https://install.duckdb.org | sh"
    echo ""
    echo "See https://duckdb.org/docs/installation/ for more options"
    exit 1
fi

# Get file size
FILE_SIZE=$(du -h "$PARQUET_FILE" | cut -f1)

# Print welcome message
echo -e "${GREEN}╔══════════════════════════════════════════════════════════════════╗${NC}"
echo -e "${GREEN}║${NC}  ${BLUE}DuckDB Interactive Parquet Explorer${NC}                          ${GREEN}║${NC}"
echo -e "${GREEN}╚══════════════════════════════════════════════════════════════════╝${NC}"
echo ""
echo -e "${YELLOW}File:${NC} $PARQUET_FILE"
echo -e "${YELLOW}Size:${NC} $FILE_SIZE"
echo ""
echo -e "${BLUE}💡 TIP: For a visual interface, use the DuckDB Local UI:${NC}"
echo -e "   ${GREEN}duckdb -cmd \"CREATE VIEW data AS FROM '$PARQUET_FILE';\" -ui${NC}"
echo ""
echo -e "${BLUE}Your Parquet file is loaded as a table named 'data'.${NC}"
echo ""
echo -e "${YELLOW}Quick Start Commands:${NC}"
echo -e "  ${GREEN}DESCRIBE data;${NC}                           -- Show schema"
echo -e "  ${GREEN}SELECT COUNT(*) FROM data;${NC}               -- Count rows"
echo -e "  ${GREEN}SELECT * FROM data LIMIT 10;${NC}             -- View first 10 rows"
echo ""
echo -e "${YELLOW}Metadata Commands:${NC}"
echo -e "  ${GREEN}SELECT * FROM parquet_kv_metadata('$PARQUET_FILE');${NC}"
echo -e "  ${GREEN}SELECT * FROM parquet_schema('$PARQUET_FILE');${NC}"
echo ""
echo -e "${YELLOW}Analysis Examples:${NC}"
echo -e "  ${GREEN}SELECT has_sbom, has_signatures, COUNT(*) FROM data GROUP BY has_sbom, has_signatures;${NC}"
echo -e "  ${GREEN}SELECT repo_owner, repo_name FROM data WHERE has_sbom = true LIMIT 10;${NC}"
echo ""
echo -e "${YELLOW}Export Results:${NC}"
echo -e "  ${GREEN}COPY (SELECT * FROM data WHERE has_sbom = true) TO 'results.csv' (HEADER);${NC}"
echo ""
echo -e "Type ${GREEN}.help${NC} for DuckDB commands, ${GREEN}.quit${NC} to exit"
echo ""
echo -e "${GREEN}════════════════════════════════════════════════════════════════════${NC}"
echo ""

# Create a temporary SQL file with the CREATE VIEW statement
TEMP_SQL=$(mktemp)
trap "rm -f $TEMP_SQL" EXIT

# Write initialization SQL
cat > "$TEMP_SQL" <<EOF
-- Create a view named 'data' that references the Parquet file
CREATE OR REPLACE VIEW data AS SELECT * FROM '$PARQUET_FILE';

-- Show basic info
.mode box
SELECT 
    COUNT(*) as total_rows,
    (SELECT COUNT(DISTINCT repo_owner || '/' || repo_name) FROM data) as unique_repos
FROM data;
EOF

# Launch DuckDB with the initialization script
duckdb -init "$TEMP_SQL"
</file>

<file path="src/graphql/GetRepoDataArtifacts.graphql">
query GetRepoDataArtifacts($owner: String!, $name: String!) {
  repository(owner: $owner, name: $name) {
    id
    name
    nameWithOwner
    releases(last: 5, orderBy: { field: CREATED_AT, direction: DESC }) {
      nodes {
        id
        name
        tagName
        url
        createdAt
        releaseAssets(first: 50) {
          nodes {
            id
            name
            downloadUrl
          }
        }
      }
    }
  }
}
</file>

<file path="src/graphql/GetRepoDataExtendedInfo.graphql">
query GetRepoDataExtendedInfo($owner: String!, $name: String!) {
  repository(owner: $owner, name: $name) {
    id
    name
    nameWithOwner
    url
    description
    licenseInfo {
      key
      name
      spdxId
    }
    hasVulnerabilityAlertsEnabled
    defaultBranchRef {
      name
      branchProtectionRule {
        allowsDeletions
        allowsForcePushes
        dismissesStaleReviews
        isAdminEnforced
        requiresStatusChecks
        requiresStrictStatusChecks
        requiresCodeOwnerReviews
        requiredApprovingReviewCount
        pattern
      }
    }
    branchProtectionRules(first: 10) {
      nodes {
        allowsDeletions
        allowsForcePushes
        dismissesStaleReviews
        isAdminEnforced
        requiresStatusChecks
        requiresStrictStatusChecks
        requiresCodeOwnerReviews
        requiredApprovingReviewCount
        pattern
      }
    }
    securityPolicy: object(expression: "HEAD:SECURITY.md") {
      ... on Blob {
        id
        text
      }
    }
    dependabot: object(expression: "HEAD:.github/dependabot.yml") {
      ... on Blob {
        id
        text
      }
    }
    releases(last: 5, orderBy: { field: CREATED_AT, direction: DESC }) {
      nodes {
        id
        name
        tagName
        url
        createdAt
        releaseAssets(first: 50) {
          nodes {
            id
            name
            downloadUrl
          }
        }
      }
    }
    workflows: object(expression: "HEAD:.github/workflows") {
      ... on Tree {
        entries {
          name
          object {
            ... on Blob {
              id
              text
            }
          }
        }
      }
    }
  }
}
</file>

<file path="AGENTS.md">
## Primary Agent: Implementation Architect Agent


**Role:** The Implementation Architect Agent is responsible for the end-to-end realization of the **GitHub Supply Chain Security Analyzer** project. This agent acts as a lead developer and architect, translating detailed specifications into a functional codebase and providing comprehensive documentation for deployment and usage.

**Directives:**

- Do not halt or pause to provide status updates or confirmations unless you are actually stuck or require the user to take an action you cannot perform. Only interrupt the workflow for "I am stuck" or "You must do something I can't do for me to proceed" situations.
- Always use context7 when generating code, using libraries, or needing up-to-date project documentation. This ensures the most accurate and current implementation.
- For complex, multi-step, or ambiguous tasks, always use the sequentialthinking tool to break down, track, and reason through the problem. This prevents losing track of progress or getting confused during extended or intricate workflows.

**Core Responsibilities:**


**Linting & Code Quality:**

- All code (excluding generated files) must always be ESLint clean: zero warnings and zero errors. This is a strict requirement for every commit and PR. Linting must be run with the current ESLint config and all issues must be fixed immediately.

**Key Performance Indicators (KPIs):**

- All specified files are created with exact content from the provided documentation.
- The generated setup and execution instructions are complete, accurate, and easy to follow.
- The project structure matches the actual flat layout (top-level `src/`, `output/`, `.cache/`, etc.).
- The `.gitignore` file correctly excludes sensitive and build artifacts.
- The `AGENTS.md` file accurately reflects the agent's role.

**Interaction with other (Conceptual) Agents:**

- **Requirements Analyst Agent (Implicit):** The Implementation Architect Agent implicitly relies on the clear, detailed specifications provided by an upstream Requirements Analyst (i.e., the user's prompt and documents).
- **Documentation Specialist Agent (Implicit):** The Implementation Architect Agent assumes the role of a Documentation Specialist by generating the `AGENTS.md` and user instructions.
- **Quality Assurance Agent (Self-Integration):** The Implementation Architect Agent integrates QA checks into its workflow to ensure fidelity to the provided plans.

**Always use context7** - when generating any code or documents, using libraries, or referencing project documentation, always use the context7 tool to ensure the most accurate and relevant output. This applies to all code generation, library usage, and documentation retrieval tasks.
</file>

<file path="eslint.config.js">
// Modern ESLint flat config for TypeScript (ESLint v9+)
// Using CommonJS syntax to align with tsconfig.json's "module": "commonjs"
const tseslint = require('@typescript-eslint/eslint-plugin');
const tsParser = require('@typescript-eslint/parser');

module.exports = [
  {
    // Add all ignore patterns here, taken from .eslintignore
    ignores: [
      "node_modules/",
      "dist/",
      ".cache/",
      "output/",
      "src/generated/",
      "*.js", // This config itself is a .js file, so we ignore it here
    ]
  },
  {
    files: ["**/*.ts"],
    languageOptions: {
      parser: tsParser,
      parserOptions: {
        project: "./tsconfig.json",
        sourceType: "module",
        ecmaVersion: 2022,
      },
    },
    plugins: {
      "@typescript-eslint": tseslint,
    },
    rules: {
      ...tseslint.configs.recommended.rules,
      "@typescript-eslint/no-unused-vars": "error", // Changed from default to error
      "@typescript-eslint/explicit-function-return-type": "off",
      "@typescript-eslint/no-explicit-any": "warn"
    },
  }
];
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "node16",
    "moduleResolution": "node16",
    "outDir": "./dist",
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "strict": true,
    "skipLibCheck": true,
    "resolveJsonModule": true
  },
  "include": [
    "src/**/*",
    "codegen.ts",
    "eslint.config.js",
    "scripts/**/*"
  ]
}
</file>

<file path="validate-with-mocks.sh">
#!/usr/bin/env bash
# validate-with-mocks.sh - Trust-building validation for GitHub Supply Chain Security Analyzer
# Usage: ./validate-with-mocks.sh

set -euo pipefail

print_section() {
  echo
  echo "============================================================"
  echo "== $1"
  echo "============================================================"
}

print_section "1. Environment & Prerequisites"
node -v && npm -v && which npx && which ts-node && echo "✅ Node.js, npm, npx, ts-node found."

if [ -f .env ]; then
  echo "✅ .env file found."
else
  echo "⚠️  .env file not found. (Not required for mock validation)"
fi

print_section "2. Dependency Installation"
echo "Running: npm install --color=always"
npm install --color=always

print_section "3. TypeScript Code Generation"
echo "Running: npm run codegen"
npm run codegen

print_section "4. Linting (ESLint)"
if npx eslint --version >/dev/null 2>&1; then
  echo "Running: npx eslint src/"
  npx eslint src/
  echo "✅ ESLint passed."
else
  echo "⚠️  ESLint not installed. Skipping lint."
fi

print_section "5. Type Checking (tsc)"
echo "Running: npx tsc --noEmit"
if npx tsc --noEmit; then
  echo "✅ TypeScript type check passed."
else
  echo "❌ TypeScript type check failed."; exit 1;
fi

# print_section "6. Tests (if defined)"
# if npm run | grep -q "test"; then
#   echo "Running: npm test"
#   npm test
#   echo "✅ Tests passed."
# else
#   echo "⚠️  No test script defined in package.json. Skipping tests."
# fi

print_section "7. CLI Mock Run"
export MOCK_GITHUB=1
INPUT_FILE=input/test-single.jsonl
if [ -f "$INPUT_FILE" ]; then
  echo "Running: npx ts-node src/main.ts --mock --input $INPUT_FILE --output output"
  npx ts-node src/main.ts --mock --input "$INPUT_FILE" --output output
  echo "✅ CLI mock run completed."
else
  echo "❌ $INPUT_FILE not found. Cannot run CLI mock validation."; exit 1;
fi

print_section "8. Report File Checks"
if [ -f output/test-single-analyzed.json ]; then
  echo "✅ output/test-single-analyzed.json generated."
else
  echo "❌ output/test-single-analyzed.json missing!"; exit 1;
fi
if [ -f output/test-single.csv ]; then
  echo "✅ output/test-single.csv generated."
else
  echo "❌ output/test-single.csv missing!"; exit 1;
fi
if [ -f output/test-single-schema.json ]; then
  echo "✅ output/test-single-schema.json generated."
else
  echo "❌ output/test-single-schema.json missing!"; exit 1;
fi

print_section "ALL VALIDATION STEPS PASSED"
echo "🎉 All checks passed. Your environment and codebase are healthy and ready!"
</file>

<file path="src/api.ts">
// src/api.ts
// This module encapsulates all interactions with the GitHub GraphQL API.

import { GraphQLClient, ClientError } from 'graphql-request';
import chalk from 'chalk';
import {
  GetRepoDataArtifactsDocument,
  GetRepoDataArtifactsQuery,
  GetRepoDataExtendedInfoDocument,
  GetRepoDataExtendedInfoQuery,
} from './generated/graphql';

/**
 * Creates and configures a GraphQLClient for the GitHub API.
 * @param token - The GitHub Personal Access Token.
 * @returns An initialized GraphQLClient instance.
 */
export function createApiClient(token: string): GraphQLClient {
  return new GraphQLClient('https://api.github.com/graphql', {
    headers: {
      Authorization: `Bearer ${token}`,
    },
  });
}

/**
 * Fetches repository artifact data (releases and assets) from the GitHub API.
 * This is a lightweight query that fetches only basic repository info and release artifacts.
 *
 * @param client - The GraphQLClient instance to use for the request.
 * @param variables - The owner and name of the repository to fetch.
 * @param verbose - A flag to enable or disable detailed logging.
 * @returns The GraphQL query result, or null if the fetch fails or the repository is not found.
 */
export async function fetchRepositoryArtifacts(
  client: GraphQLClient,
  variables: { owner: string; name: string },
  verbose: boolean
): Promise<GetRepoDataArtifactsQuery | null> {
  const repoIdentifier = `${variables.owner}/${variables.name}`;

  if (verbose) {
    console.log(chalk.gray(`[API] Fetching artifacts for ${repoIdentifier}...`));
  }

  try {
    const data = await client.request<GetRepoDataArtifactsQuery>(GetRepoDataArtifactsDocument, variables);

    if (verbose) {
      console.log(chalk.green(`[API] Success for ${repoIdentifier}.`));
    }

    // The GitHub API returns { repository: null } for non-existent or private repos.
    if (data.repository === null) {
      console.log(chalk.yellow(`[API] Repository not found or access denied for ${repoIdentifier}. Skipping.`));
      return null;
    }

    return data;
  } catch (error: unknown) {
    console.error(chalk.red.bold(`[API] Request failed for ${repoIdentifier}.`));

    // graphql-request throws an error with a .response property on GraphQL or HTTP errors
    if (error instanceof ClientError) {
      if (error.response.status !== undefined) {
        console.error(chalk.red('  HTTP Status:'), error.response.status);
      }
      if (error.response.errors) {
        console.error(chalk.red('  GraphQL Errors:'), JSON.stringify(error.response.errors, null, 2));
      }
      // Try to surface rate limit info if present
      const { headers } = error.response;
      if (headers && typeof (headers as Headers).get === 'function') {
        const h = headers as Headers;
        const remaining = h.get('x-ratelimit-remaining');
        const reset = h.get('x-ratelimit-reset');
        if (remaining !== null) {
          console.error(chalk.yellow('  Rate Limit Remaining:'), remaining);
        }
        if (reset !== null) {
          const resetTime = new Date(Number(reset) * 1000);
          console.error(chalk.yellow('  Rate Limit Resets At:'), resetTime.toLocaleTimeString());
        }
      }
    } else if (error instanceof Error) {
      console.error(chalk.red('  Error Details:'), error.message);
    } else {
      console.error(chalk.red('  An unknown error occurred:'), error);
    }
    return null;
  }
}

/**
 * Fetches extended repository data including workflows, security policies, and branch protection.
 * This is a more comprehensive query that includes everything from the artifacts query plus additional data.
 *
 * @param client - The GraphQLClient instance to use for the request.
 * @param variables - The owner and name of the repository to fetch.
 * @param verbose - A flag to enable or disable detailed logging.
 * @returns The GraphQL query result, or null if the fetch fails or the repository is not found.
 */
export async function fetchRepositoryExtendedInfo(
  client: GraphQLClient,
  variables: { owner: string; name: string },
  verbose: boolean
): Promise<GetRepoDataExtendedInfoQuery | null> {
  const repoIdentifier = `${variables.owner}/${variables.name}`;

  if (verbose) {
    console.log(chalk.gray(`[API] Fetching extended info for ${repoIdentifier}...`));
  }

  try {
    const data = await client.request<GetRepoDataExtendedInfoQuery>(GetRepoDataExtendedInfoDocument, variables);

    if (verbose) {
      console.log(chalk.green(`[API] Success for ${repoIdentifier}.`));
    }

    // The GitHub API returns { repository: null } for non-existent or private repos.
    if (data.repository === null) {
      console.log(chalk.yellow(`[API] Repository not found or access denied for ${repoIdentifier}. Skipping.`));
      return null;
    }

    return data;
  } catch (error: unknown) {
    console.error(chalk.red.bold(`[API] Request failed for ${repoIdentifier}.`));

    // graphql-request throws an error with a .response property on GraphQL or HTTP errors
    if (error instanceof ClientError) {
      if (error.response.status !== undefined) {
        console.error(chalk.red('  HTTP Status:'), error.response.status);
      }
      if (error.response.errors) {
        console.error(chalk.red('  GraphQL Errors:'), JSON.stringify(error.response.errors, null, 2));
      }
      // Try to surface rate limit info if present
      const { headers } = error.response;
      if (headers && typeof (headers as Headers).get === 'function') {
        const h = headers as Headers;
        const remaining = h.get('x-ratelimit-remaining');
        const reset = h.get('x-ratelimit-reset');
        if (remaining !== null) {
          console.error(chalk.yellow('  Rate Limit Remaining:'), remaining);
        }
        if (reset !== null) {
          const resetTime = new Date(Number(reset) * 1000);
          console.error(chalk.yellow('  Rate Limit Resets At:'), resetTime.toLocaleTimeString());
        }
      }
    } else if (error instanceof Error) {
      console.error(chalk.red('  Error Details:'), error.message);
    } else {
      console.error(chalk.red('  An unknown error occurred:'), error);
    }
    return null;
  }
}
</file>

<file path="src/config.ts">
// RepositoryTarget defines the shape of a repository to analyze
export interface RepositoryTarget {
  owner: string; // GitHub organization or user
  name: string;  // Repository name
}

// Note: Repository list is now provided via input JSONL files for all test and production runs.
// This file is not used for test automation or CLI runs with --input.
</file>

<file path=".gitignore">
# Node
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
package-lock.json

# TypeScript
dist/
build/
*.tsbuildinfo

# GraphQL Codegen
src/generated/

# Output and cache
output/
# But allow SIZE-REPORT.md to be committed for tracking storage growth
!output/*/SIZE-REPORT.md


# Environment and secrets
.env
.env.*
!*.env.template

# IDE/editor
.vscode/
.DS_Store
*.swp
*.swo

# Misc
coverage/
*.log
.private-docs/
</file>

<file path="codegen.ts">
import type { CodegenConfig } from '@graphql-codegen/cli';

const config: CodegenConfig = {
  overwrite: true,
  // The schema is the single source of truth.
  schema: 'schema/github-v15.26.0.graphql',
  // Point to where your GraphQL operations are defined.
  documents: ['src/graphql/**/*.graphql'],

  generates: {
    // A single output directory for all generated artifacts.
    'src/generated/': {
      preset: 'client',
      // Add the eslint-disable banner.
      plugins: [
        {
          add: { content: '/* eslint-disable */' },
        },
      ],
      presetConfig: {
        // This is good practice and aligns with your guide.
        gqlTagName: 'gql',
      },
      // All plugin configurations go here, applied to the entire preset.
      config: {
        prune: true,
        avoidOptionals: true,
        strictScalars: true,
        enumsAsTypes: true,
        scalars: {
          DateTime: 'string',
          Date: 'string',
          URI: 'string',
          GitObjectID: 'string',
          GitTimestamp: 'string',
          HTML: 'string',
          X509Certificate: 'string',
          Base64String: 'string',
          BigInt: 'string',
          GitRefname: 'string',
          GitSSHRemote: 'string',
          PreciseDateTime: 'string',
        },
      },
    },
  },
  ignoreNoDocuments: true,
};

export default config;
</file>

<file path="src/mockData.ts">
// This file provides mock data for development and testing in MOCK_GITHUB=1 mode.
// The structure matches the shape of the real GitHub GraphQL API response for GetRepoData.


import * as path from 'path';
import * as fs from 'fs';

/**
 * Loads mock data from src/mockdata/ based on query, owner, repo, and optional variant.
 * Returns the full API response shape as returned by GitHub (e.g., { data: { ... } }).
 *
 * Usage:
 *   getMockApiResponse('GetRepoData', 'sigstore', 'cosign')
 *   getMockApiResponse('GetRepoData', 'sigstore', 'cosign', 'release-v0.1.0')
 */

export function getMockApiResponse(query: string, owner: string, repo: string, variant?: string): unknown {
  const base = path.join(__dirname, 'mockdata');
  // Use underscores as separators to avoid ambiguity with dashes in org/repo names
  const fileName = [query, owner, repo, variant].filter(Boolean).join('_') + '.json';
  const filePath = path.join(base, fileName);
  if (!fs.existsSync(filePath)) {
    throw new Error(`Mock data file not found: ${filePath}`);
  }
  return JSON.parse(fs.readFileSync(filePath, 'utf-8')) as unknown;
}
</file>

<file path="src/analysis.ts">
// src/analysis.ts

// Regex patterns to identify potential SBOM/security artifacts in release asset names
const ARTIFACT_KEYWORDS = {
  // Legacy patterns for backward compatibility
  SBOM: /\b(sbom|spdx|cyclonedx)\b/i,
  SIGNATURE: /\.(sig|asc|pem|pub)$/i,
  ATTESTATION: /attestation/i,
  
  // Enhanced supply chain security patterns
  SPDX_SBOM: /\b(spdx|\.spdx)\b/i,
  CYCLONEDX_SBOM: /\b(cyclonedx|cdx|\.cdx)\b/i,
  VEX_DOCUMENT: /\b(vex|\.vex)\b/i,
  SLSA_PROVENANCE: /\b(provenance|slsa|\.intoto\.jsonl)\b/i,
  IN_TOTO_LINK: /\b(link|\.link)\b/i,
  IN_TOTO_LAYOUT: /\b(layout|\.layout)\b/i,
  CONTAINER_ATTESTATION: /\b(cosign|rekor|fulcio)\b/i,
  LICENSE_FILE: /\b(license|copying|notice)\b/i,
};

// CI/CD tool detection patterns for workflow analysis
const CI_TOOL_KEYWORDS = {
  SBOM_GENERATORS: /\b(syft|trivy|cdxgen|spdx-sbom-generator)\b/i,
  SIGNERS: /\b(cosign|sigstore|slsa-github-generator)\b/i,
  GORELEASER: /\b(goreleaser\/goreleaser-action)\b/i,
  VULNERABILITY_SCANNERS: /\b(snyk|anchore|twistlock|aqua|clair)\b/i,
  DEPENDENCY_SCANNERS: /\b(dependabot|renovate|whitesource|fossa)\b/i,
  CODE_SCANNERS: /\b(codeql|semgrep|bandit|eslint-security)\b/i,
  CONTAINER_SCANNERS: /\b(docker-scout|grype|trivy)\b/i,
};

/**
 * Analyze arbitrary repository data for supply chain security signals.
 * This function accepts any object structure and extracts security artifacts
 * using runtime checks. Works with any GraphQL query response.
 * 
 * @param repo - Any object that might contain repository data
 * @returns Analysis object with releases, workflows, and summary flags, or null if invalid
 */
export function analyzeRepositoryData(repo: unknown, maturity?: string) {
  if (!repo || typeof repo !== 'object') return null;
  
  const repoObj = repo as Record<string, unknown>;

  // --- Data shape definitions for clarity ---
  // Artifact: describes a single release asset and its detected security properties
  type Artifact = {
    name: string;
    isSbom: boolean;
    isSignature: boolean;
    isAttestation: boolean;
    sbomFormat: string; // 'spdx' | 'cyclonedx' | 'unknown' | ''
    isVex: boolean;
    isSlsaProvenance: boolean;
    isInTotoLink: boolean;
    isContainerAttestation: boolean;
    downloadUrl?: string;
  };
  // ReleaseInfo: describes a single release and its artifacts
  type ReleaseInfo = {
    tagName: string;
    name: string | null | undefined;
    createdAt: string;
    artifacts: Artifact[];
  };
  // WorkflowInfo: describes a single GitHub Actions workflow and detected tools
  type WorkflowInfo = {
    name: string;
    detectedSbomTools: string[];
  };
  // Analysis: the main output structure for a repository
  type Analysis = {
    repository: {
      id: string;
      owner: string;
      name: string;
      url: string;
      description?: string | null;
      maturity?: string;
    };
    releases: ReleaseInfo[];
    workflows: WorkflowInfo[];
    summary: {
      hasSbomArtifact: boolean;
      hasSignatureArtifact: boolean;
      hasAttestationArtifact: boolean;
      // FIX: Ensure this is always an array for consistency
      sbomCiTools: string[];
    };
  };

  // Use a local Set to collect CI tools detected, then convert to array at the end
  const collectedSbomCiTools = new Set<string>();

  // Initialize the analysis object for this repository
  const analysis: Analysis = {
    repository: {
      id: String(repoObj.id || 'unknown'),
      owner: String(repoObj.nameWithOwner || '').split('/')[0] || 'unknown',
      name: String(repoObj.name || 'unknown'),
      url: `https://github.com/${String(repoObj.nameWithOwner || 'unknown/unknown')}`,
      description: repoObj.description ? String(repoObj.description) : undefined,
      maturity: maturity,
    },
    releases: [],
    workflows: [],
    summary: {
      hasSbomArtifact: false,
      hasSignatureArtifact: false,
      hasAttestationArtifact: false,
      sbomCiTools: [], // FIX: Initialize as an empty array
    },
  };

  // --- Analyze Releases ---
  // Extract releases.nodes array if it exists
  const releases = repoObj.releases as Record<string, unknown> | undefined;
  const nodes = releases?.nodes;
  if (Array.isArray(nodes)) {
    nodes.forEach((release: unknown) => {
      if (!release || typeof release !== 'object') return;
      const releaseObj = release as Record<string, unknown>;
      
      const releaseInfo: ReleaseInfo = {
        tagName: String(releaseObj.tagName || ''),
        name: releaseObj.name ? String(releaseObj.name) : null,
        createdAt: String(releaseObj.createdAt || ''),
        artifacts: [],
      };

      // Extract releaseAssets.nodes array if it exists
      const releaseAssets = releaseObj.releaseAssets as Record<string, unknown> | undefined;
      const assetNodes = releaseAssets?.nodes;
      if (Array.isArray(assetNodes)) {
        assetNodes.forEach((asset: unknown) => {
          if (!asset || typeof asset !== 'object') return;
          const assetObj = asset as Record<string, unknown>;
          
          const assetName = String(assetObj.name || '');
          
          // Determine SBOM format
          let sbomFormat = '';
          if (ARTIFACT_KEYWORDS.SPDX_SBOM.test(assetName)) {
            sbomFormat = 'spdx';
          } else if (ARTIFACT_KEYWORDS.CYCLONEDX_SBOM.test(assetName)) {
            sbomFormat = 'cyclonedx';
          } else if (ARTIFACT_KEYWORDS.SBOM.test(assetName)) {
            sbomFormat = 'unknown';
          }
          
          const artifact: Artifact = {
            name: assetName,
            isSbom: ARTIFACT_KEYWORDS.SBOM.test(assetName),
            isSignature: ARTIFACT_KEYWORDS.SIGNATURE.test(assetName),
            isAttestation: ARTIFACT_KEYWORDS.ATTESTATION.test(assetName),
            sbomFormat,
            isVex: ARTIFACT_KEYWORDS.VEX_DOCUMENT.test(assetName),
            isSlsaProvenance: ARTIFACT_KEYWORDS.SLSA_PROVENANCE.test(assetName),
            isInTotoLink: ARTIFACT_KEYWORDS.IN_TOTO_LINK.test(assetName),
            isContainerAttestation: ARTIFACT_KEYWORDS.CONTAINER_ATTESTATION.test(assetName),
            downloadUrl: assetObj.downloadUrl ? String(assetObj.downloadUrl) : undefined,
          };
          releaseInfo.artifacts.push(artifact);

          // Track which types of artifacts are present for summary and reporting
          if (artifact.isSbom) collectedSbomCiTools.add('sbom');
          if (artifact.isSignature) collectedSbomCiTools.add('signature');
          if (artifact.isAttestation) collectedSbomCiTools.add('attestation');
          if (artifact.isVex) collectedSbomCiTools.add('vex');
          if (artifact.isSlsaProvenance) collectedSbomCiTools.add('slsa-provenance');
          if (artifact.isInTotoLink) collectedSbomCiTools.add('in-toto-link');
          if (artifact.isContainerAttestation) collectedSbomCiTools.add('container-attestation');
          
          if (artifact.isSbom) analysis.summary.hasSbomArtifact = true;
          if (artifact.isSignature) analysis.summary.hasSignatureArtifact = true;
          if (artifact.isAttestation) analysis.summary.hasAttestationArtifact = true;
        });
      }
      analysis.releases.push(releaseInfo);
    });
  }


  // --- Analyze CI Workflows ---
  // Extract workflows if present (only in extended query)
  if ('workflows' in repoObj && repoObj.workflows && typeof repoObj.workflows === 'object') {
    const workflows = repoObj.workflows as Record<string, unknown>;
    if ('entries' in workflows && Array.isArray(workflows.entries)) {
      const workflowEntries = workflows.entries;
      
      workflowEntries.forEach((entry: unknown) => {
        if (!entry || typeof entry !== 'object') return;
        const typedEntry = entry as Record<string, unknown>;
        
        if (!('object' in typedEntry) || !typedEntry.object || typeof typedEntry.object !== 'object') return;
        const entryObject = typedEntry.object as Record<string, unknown>;
        
        if (!('text' in entryObject)) return;
        
        const workflowYaml = String(entryObject.text || '');
        const detectedTools: string[] = [];
        
        // Check for each CI tool category
        if (CI_TOOL_KEYWORDS.SBOM_GENERATORS.test(workflowYaml)) {
          detectedTools.push('sbom-generator');
          collectedSbomCiTools.add('sbom-generator');
        }
        if (CI_TOOL_KEYWORDS.SIGNERS.test(workflowYaml)) {
          detectedTools.push('signer');
          collectedSbomCiTools.add('signer');
        }
        if (CI_TOOL_KEYWORDS.GORELEASER.test(workflowYaml)) {
          detectedTools.push('goreleaser');
          collectedSbomCiTools.add('goreleaser');
        }
        if (CI_TOOL_KEYWORDS.VULNERABILITY_SCANNERS.test(workflowYaml)) {
          detectedTools.push('vulnerability-scanner');
        }
        if (CI_TOOL_KEYWORDS.DEPENDENCY_SCANNERS.test(workflowYaml)) {
          detectedTools.push('dependency-scanner');
        }
        if (CI_TOOL_KEYWORDS.CODE_SCANNERS.test(workflowYaml)) {
          detectedTools.push('code-scanner');
        }
        if (CI_TOOL_KEYWORDS.CONTAINER_SCANNERS.test(workflowYaml)) {
          detectedTools.push('container-scanner');
        }
        
        analysis.workflows.push({
          name: String(typedEntry.name || 'unknown'),
          detectedSbomTools: detectedTools,
        });
      });
    }
  }

  // Finalize summary: convert collectedSbomCiTools Set to array for serialization
  analysis.summary.sbomCiTools = Array.from(collectedSbomCiTools);

  // Return the full analysis object for this repository
  return analysis;
}
</file>

<file path="src/report.ts">
// src/report.ts
// This module handles report generation for the analyzer.
// It outputs comprehensive JSON, normalized CSV, and Parquet schema files.
// Actual Parquet file generation is handled by external tooling.

import * as fs from 'fs/promises';
import * as path from 'path';
import { json2csv } from 'json-2-csv';
import chalk from 'chalk';
import { analyzeRepositoryData } from './analysis';
import { generateParquetFiles } from './parquetWriter';

// AnalysisResult can be null, so we need to handle that.
type AnalysisResult = ReturnType<typeof analyzeRepositoryData>;

// Type definitions for normalized data structure
type NormalizedRow = {
  maturity: string;
  repository_id: string;
  repository_owner: string;
  repository_name: string;
  repository_name_with_owner: string;
  repository_url: string;
  repository_description: string;
  repo_has_sbom_artifact: boolean;
  repo_has_signature_artifact: boolean;
  repo_has_attestation_artifact: boolean;
  repo_security_tools_json: string;
  repo_security_tools_count: number;
  workflow_count: number;
  workflow_names_json: string;
  workflow_security_tools_json: string;
  release_index: number | null;
  release_tag_name: string;
  release_name: string;
  release_created_at: string;
  release_artifact_count: number;
  release_has_container_images: boolean;
  release_has_slsa_provenance: boolean;
  release_has_in_toto_attestation: boolean;
  release_sbom_formats_json: string;
  artifact_index: number | null;
  artifact_name: string;
  artifact_download_url: string;
  artifact_size_hint: string;
  artifact_file_extension: string;
  artifact_is_sbom: boolean;
  artifact_is_signature: boolean;
  artifact_is_attestation: boolean;
  artifact_sbom_format: string;
  artifact_is_vex: boolean;
  artifact_is_slsa_provenance: boolean;
  artifact_is_in_toto_link: boolean;
  artifact_is_container_attestation: boolean;
  artifact_type: string;
  artifact_platform_hint: string;
};

type SchemaField = {
  type: string;
  description: string;
  example: unknown;
  category: string;
};

type SchemaDocumentation = {
  title: string;
  version: string;
  description: string;
  generated_at: string;
  fields: Record<string, SchemaField>;
};

/**
 * Normalizes the analysis results into a flat structure suitable for data analysis.
 * This function recursively flattens the JSON structure into individual rows.
 */
function normalizeAnalysisResults(analysisResults: Exclude<AnalysisResult, null>[], maturity = 'unknown'): NormalizedRow[] {
  const flattenedData: NormalizedRow[] = [];
  
  analysisResults.forEach(res => {
    const baseRepoData = {
      // The maturity is now a first-class citizen of the analysis result.
      maturity: res.repository.maturity || maturity,
      // Repository metadata
      repository_id: res.repository.id,
      repository_owner: res.repository.owner,
      repository_name: res.repository.name,
      repository_name_with_owner: `${res.repository.owner}/${res.repository.name}`,
      repository_url: res.repository.url,
      repository_description: res.repository.description || '',
      
      // Repository-level security summary flags
      repo_has_sbom_artifact: res.summary.hasSbomArtifact,
      repo_has_signature_artifact: res.summary.hasSignatureArtifact,
      repo_has_attestation_artifact: res.summary.hasAttestationArtifact,
      
      // CI/CD security tools detected (as JSON string for complex data)
      repo_security_tools_json: JSON.stringify(res.summary.sbomCiTools || []),
      repo_security_tools_count: (res.summary.sbomCiTools || []).length,
      
      // Workflow information
      workflow_count: res.workflows.length,
      workflow_names_json: JSON.stringify(res.workflows.map(w => w.name)),
      
      // Aggregated workflow security tools
      workflow_security_tools_json: JSON.stringify(
        res.workflows.flatMap(w => w.detectedSbomTools)
      ),
    };

    // If repository has releases, create rows for each release-artifact combination
    if (res.releases && res.releases.length > 0) {
      res.releases.forEach((release, releaseIndex) => {
        const baseReleaseData = {
          ...baseRepoData,
          
          // Release metadata
          release_index: releaseIndex,
          release_tag_name: release.tagName,
          release_name: release.name || '',
          release_created_at: release.createdAt,
          release_artifact_count: release.artifacts.length,
          
          // Detect container images in release
          release_has_container_images: release.artifacts.some(a => 
            /\.(tar|tar\.gz|oci|docker)$/i.test(a.name) || 
            /container|image|docker/i.test(a.name)
          ),
          
          // SLSA compliance indicators
          release_has_slsa_provenance: release.artifacts.some(a => a.isSlsaProvenance),
          release_has_in_toto_attestation: release.artifacts.some(a => a.isInTotoLink),
          
          // Aggregate artifact types for this release
          release_sbom_formats_json: JSON.stringify(
            [...new Set(release.artifacts
              .filter(a => a.sbomFormat)
              .map(a => a.sbomFormat)
            )]
          ),
        };

        // If release has artifacts, create one row per artifact
        if (release.artifacts && release.artifacts.length > 0) {
          release.artifacts.forEach((artifact, artifactIndex) => {
            flattenedData.push({
              ...baseReleaseData,
              
              // Artifact metadata
              artifact_index: artifactIndex,
              artifact_name: artifact.name,
              artifact_download_url: artifact.downloadUrl || '',
              artifact_size_hint: extractSizeHint(artifact.name),
              artifact_file_extension: path.extname(artifact.name).toLowerCase(),
              
              // Artifact security classification
              artifact_is_sbom: artifact.isSbom,
              artifact_is_signature: artifact.isSignature,
              artifact_is_attestation: artifact.isAttestation,
              artifact_sbom_format: artifact.sbomFormat,
              artifact_is_vex: artifact.isVex,
              artifact_is_slsa_provenance: artifact.isSlsaProvenance,
              artifact_is_in_toto_link: artifact.isInTotoLink,
              artifact_is_container_attestation: artifact.isContainerAttestation,
              
              // Artifact type classification
              artifact_type: classifyArtifactType(artifact),
              artifact_platform_hint: extractPlatformHint(artifact.name),
            });
          });
        } else {
          // Release has no artifacts, create one row for the release
          flattenedData.push({
            ...baseReleaseData,
            artifact_index: null,
            artifact_name: '',
            artifact_download_url: '',
            artifact_size_hint: '',
            artifact_file_extension: '',
            artifact_is_sbom: false,
            artifact_is_signature: false,
            artifact_is_attestation: false,
            artifact_sbom_format: '',
            artifact_is_vex: false,
            artifact_is_slsa_provenance: false,
            artifact_is_in_toto_link: false,
            artifact_is_container_attestation: false,
            artifact_type: 'none',
            artifact_platform_hint: '',
          });
        }
      });
    } else {
      // Repository has no releases, create one row for the repository
      flattenedData.push({
        ...baseRepoData,
        release_index: null,
        release_tag_name: '',
        release_name: '',
        release_created_at: '',
        release_artifact_count: 0,
        release_has_container_images: false,
        release_has_slsa_provenance: false,
        release_has_in_toto_attestation: false,
        release_sbom_formats_json: '[]',
        artifact_index: null,
        artifact_name: '',
        artifact_download_url: '',
        artifact_size_hint: '',
        artifact_file_extension: '',
        artifact_is_sbom: false,
        artifact_is_signature: false,
        artifact_is_attestation: false,
        artifact_sbom_format: '',
        artifact_is_vex: false,
        artifact_is_slsa_provenance: false,
        artifact_is_in_toto_link: false,
        artifact_is_container_attestation: false,
        artifact_type: 'none',
        artifact_platform_hint: '',
      });
    }
  });

  return flattenedData;
}

/**
 * Extract size hints from artifact names (e.g., file sizes, architecture)
 */
function extractSizeHint(filename: string): string {
  const sizeMatch = filename.match(/([0-9]+(?:\.[0-9]+)?[KMGT]?B)/i);
  return sizeMatch ? sizeMatch[1] : '';
}

/**
 * Extract platform/architecture hints from artifact names
 */
function extractPlatformHint(filename: string): string {
  const platforms = [
    'linux', 'windows', 'darwin', 'macos',
    'amd64', 'x86_64', 'arm64', 'armv7', 'i386',
    'alpine', 'ubuntu', 'debian'
  ];
  
  const found = platforms.filter(platform => 
    new RegExp(`\\b${platform}\\b`, 'i').test(filename)
  );
  
  return found.join(',');
}

/**
 * Classify artifact type based on name patterns
 */
function classifyArtifactType(artifact: { 
  isSbom: boolean; 
  isSignature: boolean; 
  isAttestation: boolean;
  isSlsaProvenance: boolean;
  isVex: boolean;
  isInTotoLink: boolean;
  isContainerAttestation: boolean;
  name: string;
}): string {
  if (artifact.isSbom) return 'sbom';
  if (artifact.isSignature) return 'signature';
  if (artifact.isAttestation) return 'attestation';
  if (artifact.isSlsaProvenance) return 'slsa-provenance';
  if (artifact.isVex) return 'vex';
  if (artifact.isInTotoLink) return 'in-toto-link';
  if (artifact.isContainerAttestation) return 'container-attestation';
  
  const name = artifact.name.toLowerCase();
  if (/\.(tar|tar\.gz|zip|7z)$/.test(name)) return 'archive';
  if (/\.(exe|msi|dmg|pkg|deb|rpm)$/.test(name)) return 'installer';
  if (/\.(dll|so|dylib)$/.test(name)) return 'library';
  if (/\.(json|yaml|yml|xml|toml)$/.test(name)) return 'config';
  if (/\.(md|txt|pdf)$/.test(name)) return 'documentation';
  if (/checksum|hash|sha|md5/.test(name)) return 'checksum';
  
  return 'binary';
}



/**
 * Generate comprehensive reports from the analysis results.
 * @param analysisResults - Array of per-repository analysis objects
 * @param outputDir - Output directory for reports (run-specific directory)
 * @param baseName - Base name for output files (dataset name without extension)
 * @param runMetadata - Metadata about this run for Parquet KV metadata
 */
export async function generateReports(
  analysisResults: AnalysisResult[], 
  outputDir: string,
  baseName: string,
  runMetadata?: {
    queryType: string;
    timestamp: string;
    totalRepos: number;
    successfulRepos: number;
    failedRepos: number;
  }
) {

  try {
    // Ensure output directory exists
    await fs.mkdir(outputDir, { recursive: true });

    // Filter out any null results before processing
    const validAnalysisResults = analysisResults.filter(
      (res): res is Exclude<AnalysisResult, null> => res !== null
    );

    if (validAnalysisResults.length === 0) {
      console.log(chalk.yellow('No valid analysis results to generate reports.'));
      return;
    }

    // --- JSON Report (Analyzed Domain Model) ---
    const jsonPath = path.join(outputDir, `${baseName}-analyzed.json`);
    await fs.writeFile(jsonPath, JSON.stringify(validAnalysisResults, null, 2));
    console.log(chalk.green(`✅ Analyzed JSON report saved to: ${jsonPath}`));

  // --- Normalized Data ---
  // Pass the dataset baseName (inputBase) as the maturity indicator for each row
  const normalizedData = normalizeAnalysisResults(validAnalysisResults, baseName);

    // --- CSV Report (Normalized) ---
    const csvPath = path.join(outputDir, `${baseName}.csv`);
    const csv = await json2csv(normalizedData);
    await fs.writeFile(csvPath, csv);
    console.log(chalk.green(`✅ Normalized CSV report saved to: ${csvPath}`));

    // --- Parquet Schema (For External Conversion) ---
    const schemaPath = path.join(outputDir, `${baseName}-schema.json`);
    const schema = generateSchemaDocumentation(normalizedData[0] || {});
    await fs.writeFile(schemaPath, JSON.stringify(schema, null, 2));
    console.log(chalk.green(`✅ Schema documentation saved to: ${schemaPath}`));

    // --- Parquet Files (DuckDB-based with metadata) ---
    if (runMetadata) {
      const basePathForParquet = path.join(outputDir, baseName);
      await generateParquetFiles(
        basePathForParquet,
        jsonPath,
        schemaPath,
        runMetadata
      );
    }

  } catch (error) {
    console.error(chalk.red('Failed to generate reports:'), error);
  }
}

/**
 * Generate schema documentation from the normalized data structure
 */
function generateSchemaDocumentation(sampleRow: Record<string, unknown>): SchemaDocumentation {
  const schema: SchemaDocumentation = {
    title: "GitHub Supply Chain Security Analysis Schema",
    version: "1.0.0",
    description: "Normalized dataset schema for GitHub repository supply chain security analysis",
    generated_at: new Date().toISOString(),
    fields: {}
  };

  Object.keys(sampleRow).forEach(key => {
    const value = sampleRow[key];
    const type = typeof value;
    
    schema.fields[key] = {
      type: value === null ? 'nullable' : type,
      description: generateFieldDescription(key),
      example: value,
      category: categorizeField(key)
    };
  });

  return schema;
}

/**
 * Generate human-readable descriptions for schema fields
 */
function generateFieldDescription(fieldName: string): string {
  const descriptions: Record<string, string> = {
    repository_id: "Unique identifier for the repository (GraphQL node ID)",
  maturity: "Maturity classification of the repository dataset (e.g., sandbox, incubation, graduated)",
    repository_owner: "GitHub username/organization that owns the repository", 
    repository_name: "Name of the repository",
    repository_name_with_owner: "Full repository name in owner/repo format",
    repository_url: "GitHub URL of the repository",
    repository_description: "Repository description from GitHub",
    repo_has_sbom_artifact: "Boolean indicating if repository has SBOM artifacts in releases",
    repo_has_signature_artifact: "Boolean indicating if repository has signature artifacts in releases",
    repo_has_attestation_artifact: "Boolean indicating if repository has attestation artifacts in releases",
    repo_security_tools_json: "JSON array of security tools detected in CI/CD workflows",
    repo_security_tools_count: "Count of unique security tools detected",
    workflow_count: "Number of GitHub Actions workflows in the repository",
    workflow_names_json: "JSON array of workflow filenames",
    workflow_security_tools_json: "JSON array of security tools found in all workflows",
    release_index: "Index of the release (0 = latest, 1 = second latest, etc.)",
    release_tag_name: "Git tag name for the release",
    release_name: "Display name of the release",
    release_created_at: "ISO timestamp when the release was created",
    release_artifact_count: "Number of artifacts attached to this release",
    release_has_container_images: "Boolean indicating if release contains container images",
    release_has_slsa_provenance: "Boolean indicating if release has SLSA provenance attestations",
    release_has_in_toto_attestation: "Boolean indicating if release has in-toto attestations",
    release_sbom_formats_json: "JSON array of SBOM formats detected in this release",
    artifact_index: "Index of the artifact within the release",
    artifact_name: "Filename of the release artifact",
    artifact_download_url: "Direct download URL for the artifact",
    artifact_size_hint: "Extracted size information from filename",
    artifact_file_extension: "File extension of the artifact",
    artifact_is_sbom: "Boolean indicating if artifact is a Software Bill of Materials",
    artifact_is_signature: "Boolean indicating if artifact is a cryptographic signature",
    artifact_is_attestation: "Boolean indicating if artifact is an attestation",
    artifact_sbom_format: "Specific SBOM format (spdx, cyclonedx, unknown, or empty)",
    artifact_is_vex: "Boolean indicating if artifact is a VEX (Vulnerability Exploitability eXchange) document",
    artifact_is_slsa_provenance: "Boolean indicating if artifact is SLSA provenance",
    artifact_is_in_toto_link: "Boolean indicating if artifact is an in-toto link file",
    artifact_is_container_attestation: "Boolean indicating if artifact is a container attestation",
    artifact_type: "Classified type of artifact (sbom, signature, binary, etc.)",
    artifact_platform_hint: "Extracted platform/architecture information from filename"
  };

  return descriptions[fieldName] || `Field: ${fieldName}`;
}

/**
 * Categorize fields for documentation organization
 */
function categorizeField(fieldName: string): string {
  if (fieldName.startsWith('repository_') || fieldName.startsWith('repo_')) {
    return 'repository';
  }
  if (fieldName.startsWith('workflow_')) {
    return 'workflows';
  }
  if (fieldName.startsWith('release_')) {
    return 'releases';
  }
  if (fieldName.startsWith('artifact_')) {
    return 'artifacts';
  }
  return 'metadata';
}
</file>

<file path="package.json">
{
  "name": "github-supply-chain-analyzer",
  "version": "1.0.0",
  "description": "Analyzes GitHub repos for supply chain security artifacts.",
  "main": "dist/main.js",
  "scripts": {
    "start": "ts-node src/main.ts",
    "build": "tsc",
    "codegen": "graphql-codegen --config codegen.ts",
    "lint": "npx eslint src/",
    "typecheck": "npx tsc --noEmit",
    "validate": "./validate-with-mocks.sh",
    "clean": "rm -rf dist output .cache src/generated",
    "fetch:cncf": "npx ts-node scripts/fetch-cncf-landscape.ts",
    "docs:schema": "./scripts/generate-schema-docs.sh",
    "docs:verify": "echo 'TODO: Add README verification script'",
    "test:single": "npx ts-node src/main.ts --mock --input input/test-single.jsonl --output output --verbose",
    "test:three": "npx ts-node src/main.ts --mock --input input/test-three-repos.jsonl --output output --verbose",
    "test:all": "npx ts-node src/main.ts --mock --input input/test-allmocked.jsonl --output output",
    "validate:verbose": "npx ts-node src/main.ts --mock --input input/test-single.jsonl --output output --verbose",
    "validate:three-repos": "npx ts-node src/main.ts --mock --input input/test-three-repos.jsonl --output output --verbose",
    "validate:all-mocked": "npx ts-node src/main.ts --mock --input input/test-allmocked.jsonl --output output",
    "run:sandbox": "./scripts/run-target.sh sandbox",
    "run:incubation": "./scripts/run-target.sh incubation",
    "run:graduated": "./scripts/run-target.sh graduated",
    "run:cncf-all": "./scripts/run-cncf-all.sh"
  },
  "keywords": [
    "github",
    "graphql",
    "sbom",
    "supply-chain-security"
  ],
  "author": "",
  "license": "MIT",
  "dependencies": {
    "chalk": "4.1.2",
    "cli-table3": "^0.6.5",
    "commander": "^14.0.1",
    "dotenv": "^16.3.1",
    "duckdb": "^1.1.3",
    "graphql": "^16.8.1",
    "graphql-request": "^6.1.0",
    "js-yaml": "^4.1.0",
    "json-2-csv": "^4.1.0",
    "node-cache": "^5.1.2",
    "node-fetch": "^2.7.0",
    "yaml": "^2.8.1"
  },
  "devDependencies": {
    "@graphql-codegen/cli": "5.0.0",
    "@graphql-codegen/client-preset": "4.1.0",
    "@graphql-codegen/typescript": "4.0.1",
    "@graphql-codegen/typescript-graphql-request": "^6.0.0",
    "@graphql-codegen/typescript-operations": "4.0.1",
    "@types/js-yaml": "4.0.9",
    "@types/node": "^20.8.9",
    "@types/node-fetch": "^2.6.13",
    "@typescript-eslint/eslint-plugin": "^8.43.0",
    "@typescript-eslint/parser": "^8.43.0",
    "eslint": "^9.35.0",
    "ts-node": "^10.9.1",
    "typescript": "^5.2.2"
  }
}
</file>

<file path="README.md">
# GitHub Supply Chain Data Collector

Welcome — this repository is a practical, opinionated tool for measuring what projects ship in their releases and CI workflows. It focuses on **presence detection**: does a repository ship SBOMs, signatures, or attestations? Does its CI invoke key security tooling?

If your team is building observability or supply-chain tooling, this repository gives you a reproducible pipeline that collects, normalizes, documents, and prepares that data for analysis. It’s designed for maintainers, researchers, and platform teams who want reliable signals about supply chain security practices across many repositories.

### Why This Matters

-   **Discover** where SBOMs and signatures are being produced and where they’re missing across your ecosystem.
-   **Understand** CI adoption of key security tools (`syft`, `cosign`, `goreleaser`, etc.).
-   **Produce** compact, queryable artifacts (Parquet + CSV) for downstream analysis and powerful dashboards.

### 🚀 Highlights

*   **Robust Data Collection**: A type-safe GraphQL client using codegen ensures API calls are maintainable and catch schema changes at compile time.
*   **Flexible & Fast**: Run in a lightweight "artifacts-only" mode or a deep "--extended" mode to inspect CI workflows. Process large datasets quickly with parallel execution.
*   **Powerful Outputs**: Generates multiple analysis-ready formats from a single run:
    *   `raw-responses.jsonl`: Preserves the full, raw API response for ultimate reproducibility.
    *   `*-analyzed.json`: A clean, enriched domain model with computed security flags.
    *   `*.csv`: A flattened, normalized table perfect for spreadsheets or quick database imports.
    *   `*-analyzed.parquet`: A highly compressed, columnar dataset with embedded schema descriptions, ideal for fast analytics with tools like DuckDB.
*   **Built-in Cost Tracking**: Each run produces a `SIZE-REPORT.md` so you can track storage growth and commit only the most efficient artifacts to your repository.
*   **Awesome Developer Experience**: Comes with a mock mode for instant local development, validation scripts, and a clear, extensible architecture.

## Get Started in 3 Steps

1.  **Clone the repo and install dependencies:**

    ```bash
    git clone https://github.com/your-org/your-repo-name.git
    cd your-repo-name
    npm install
    ```

2.  **Run a quick mock analysis (no GitHub API calls needed!):**

    This command runs against a pre-configured test file (`input/test-single.jsonl`) and generates a full set of reports in the `output/` directory.

    ```bash
    npm start -- --mock --input input/test-single.jsonl
    ```

3.  **Run against real repositories:**

    Create a `.env` file from the template and add your GitHub Personal Access Token. Then, run the analyzer against the CNCF graduated projects list.

    ```bash
    # 1. Create your .env file
    cp .env.template .env

    # 2. Edit .env and set your GITHUB_PAT

    # 3. Run the analysis!
    npm start -- --input input/graduated.jsonl --parallel
    ```

    All output will be saved to a clean, timestamped directory like `output/graduated-2025-10-10T03-33-00/`.

## Viewing and Analyzing Your Data

The real power of this tool is unlocked when you start exploring the data. We highly recommend using **DuckDB** for a fantastic analysis experience.

The project includes a helper script and guide for getting started. For a great visual experience, you can launch the DuckDB Local UI directly against your results:

```bash
# Launch the Local UI with your Parquet file loaded as a 'data' view
duckdb -cmd "CREATE VIEW data AS FROM 'output/graduated-2025-10-10T03-33-00/graduated-analyzed.parquet';" -ui
```

This opens a powerful, local-only web interface in your browser where you can write SQL, explore the schema, and visualize results.

Here are a few queries to get you started:

```sql
-- Which CNCF graduated projects ship SBOMs in their releases?
SELECT repository_name_with_owner, release_tag_name, artifact_name
FROM data
WHERE artifact_is_sbom = true;

-- Get a summary of security artifact adoption
SELECT
  repo_has_sbom_artifact,
  repo_has_signature_artifact,
  COUNT(DISTINCT repository_name_with_owner) as project_count
FROM data
GROUP BY ALL
ORDER BY project_count DESC;
```

## What the Tool Produces

Each run creates a timestamped directory with a consistent set of artifacts, giving you a complete, historical record.

-   `raw-responses.jsonl` — The raw, unmodified GraphQL responses. The ultimate source of truth for reproducibility.
-   `*-analyzed.json` — An enriched domain model with computed detection flags (e.g., `isSbom`, `isSignature`).
-   `*.csv` — Flattened rows ready for SQL, spreadsheets, or any tool that loves tables.
-   `*-schema.json` — A machine-readable schema used to embed rich metadata into the Parquet file.
-   `*-analyzed.parquet` — A compressed, columnar dataset with all field descriptions and run metadata embedded. This is the **recommended format for analytics**.
-   `SIZE-REPORT.md` — A human-readable summary of file sizes from the run. We recommend committing this to Git to track artifact growth over time!

## Diving Deeper: CLI Usage

The CLI is designed for flexibility. Here are some common patterns:

```bash
# Run against all CNCF projects sequentially
./scripts/run-cncf-all.sh

# Analyze a specific CNCF maturity level
./scripts/run-target.sh incubation

# Run the extended query to analyze GitHub Actions workflows for security tools
npm start -- --input input/sandbox.jsonl --extended --parallel

# Run against a custom list of repositories
npm start -- --input path/to/my-repos.jsonl
```

## For Developers & Contributors

This project is built to be extensible. We welcome you to join the conversation, open issues, and contribute!

-   `npm run lint` — Lint hand-written code (generated code is ignored).
-   `npm run typecheck` — Run `tsc` to validate types before you commit.
-   `npm run codegen` — Regenerate the GraphQL client and types after changing queries in `src/graphql/`.
-   `npm run validate` — A helpful script that runs a full pipeline (lint, typecheck, mock run) to ensure your environment is healthy.

### How to Contribute

1.  **Add a new detection**: Extend the regex patterns in `src/analysis.ts` to find new artifacts or CI tools.
2.  **Add new data**: Add a new GraphQL query in `src/graphql/`, run `npm run codegen`, and integrate the new data into the analysis and reporting pipeline.
3.  **Improve tooling**: Enhance the reporting, add new output formats, or create new analysis notebooks.

Open a PR or join the issue tracker to get started. We'd love to hear about your use case!

### Resources

-   [DuckDB Documentation](https://duckdb.org/docs/)
-   [GraphQL Code Generator](https://www.graphql-code-generator.com/)
-   [Apache Parquet Format](https://parquet.apache.org/docs/)

## License

This project is released under the MIT License. See the `LICENSE` file for details.
</file>

<file path="src/main.ts">
// src/main.ts

import 'dotenv/config';
import chalk from 'chalk';
import Table from 'cli-table3';
import { Command } from 'commander';
import * as fs from 'fs';
import * as path from 'path';

import { analyzeRepositoryData } from './analysis';
import { generateReports } from './report';
import { generateSizeReport } from './sizeReport';
import { getMockApiResponse } from './mockData';
import { createApiClient, fetchRepositoryArtifacts, fetchRepositoryExtendedInfo } from './api';
import { GetRepoDataArtifactsQuery, GetRepoDataExtendedInfoQuery } from './generated/graphql';
import { appendRawResponse } from './rawResponseWriter';

// Define common types used throughout the script
type AnalysisResult = ReturnType<typeof analyzeRepositoryData>;
type RepositoryTarget = { owner: string; name: string; maturity?: string };

const program = new Command();
program
  .name('github-supply-chain-analyzer')
  .description('Analyze GitHub repositories for supply chain security artifacts')
  .version('1.0.0')
  .option('-i, --input <file>', 'Input JSONL file with repository list', 'input/sandbox.jsonl')
  .option('--mock', 'Run in mock mode (no GitHub API calls)', false)
  .option('--extended', 'Use extended query (includes workflows, security policies)', false)
  .option('-o, --output <dir>', 'Output directory for reports', 'output')
  .option('-v, --verbose', 'Show detailed API logging and column explanations', false)
  .option(
    '-p, --parallel [batchSize]',
    'Run fetches in parallel. Optionally set a batch size (e.g., --parallel 10). Defaults to 10.',
    false
  )
  .helpOption('-h, --help', 'Display help for command');
program.parse(process.argv);
const opts = program.opts();

// Removed cache logic

/**
 * Fetches, analyzes, and caches data for a single repository.
 * This helper function encapsulates the logic for one repo, making it reusable
 * for both sequential and parallel execution modes.
 */
async function fetchAndAnalyzeRepo(
  repo: RepositoryTarget,
  client: ReturnType<typeof createApiClient> | null,
  useMock: boolean,
  useExtended: boolean,
  verbose: boolean,
  rawResponsesPath?: string
): Promise<AnalysisResult | null> {
  const repoKey = `${repo.owner}/${repo.name}`;
  console.log(`Processing repository: ${chalk.cyan(repoKey)}`);

  let repoData: GetRepoDataArtifactsQuery | GetRepoDataExtendedInfoQuery | null = null;
  const queryType = useExtended ? 'GetRepoDataExtendedInfo' : 'GetRepoDataArtifacts';

  if (useMock) {
    try {
      // Mock data uses the old GetRepoData naming convention
      // The mock files contain extended info, so they work for both query types
      const mockResponse = getMockApiResponse('GetRepoData', repo.owner, repo.name) as {
        data: GetRepoDataExtendedInfoQuery;
      };
      repoData = mockResponse.data;
    } catch {
      console.log(chalk.yellow('⚠️  No mock data found for this repository. Skipping.'));
      return null;
    }
  } else {
    // Fetch from GitHub API using appropriate query
    if (useExtended) {
      repoData = await fetchRepositoryExtendedInfo(client!, { owner: repo.owner, name: repo.name }, verbose);
    } else {
      repoData = await fetchRepositoryArtifacts(client!, { owner: repo.owner, name: repo.name }, verbose);
    }
    
    // Save raw API response to JSONL with metadata
    if (repoData && rawResponsesPath) {
      await appendRawResponse(rawResponsesPath, {
        queryType,
        owner: repo.owner,
        repo: repo.name,
        maturity: repo.maturity,
        response: repoData,
      });
    }
  }

  if (repoData && repoData.repository) {
    const analysis = analyzeRepositoryData(repoData.repository, repo.maturity);
    return analysis;
  }
  return null;
}

async function main() {
  const { verbose, mock, extended, input, output, parallel, help } = opts;
  console.log(chalk.blue.bold('🚀 Starting GitHub Supply Chain Security Analysis...'));

  if (help) {
    program.help();
    return;
  }

  const useMock = mock || process.env.MOCK_GITHUB === '1';
  const useExtended = extended;
  
  if (useMock) {
    console.log(chalk.magenta.bold('🧪 MOCK MODE ENABLED: Using mock GitHub data.'));
  }
  
  if (useExtended) {
    console.log(chalk.cyan.bold('🔍 EXTENDED MODE: Fetching workflows and security policies.'));
  }

  const githubPat = process.env.GITHUB_PAT;
  if (!useMock && !githubPat) {
    console.error(chalk.red.bold('Error: GITHUB_PAT environment variable not set.'));
    process.exit(1);
  }
  const client = useMock ? null : createApiClient(githubPat!);

  // Support multiple input files by allowing comma-separated values in --input
  const inputOption = String(input || '');
  const inputFiles = inputOption.split(',').map(s => s.trim()).filter(Boolean);
  if (inputFiles.length === 0) {
    console.error(chalk.red.bold('No input files provided.')); program.help(); process.exit(1);
  }

  const repositories: RepositoryTarget[] = [];
  for (const infile of inputFiles) {
    if (!fs.existsSync(infile)) {
      console.error(chalk.red.bold(`Input file not found: ${infile}`));
      program.help();
      process.exit(1);
    }

    const repoLines = fs.readFileSync(infile, 'utf-8').split('\n').filter(Boolean);
    const inferredMaturity = path.basename(infile, path.extname(infile));
    for (const line of repoLines) {
      try {
        const parsed = JSON.parse(line);
        if (!parsed.maturity) parsed.maturity = inferredMaturity;
        repositories.push(parsed as RepositoryTarget);
      } catch {
        console.error(chalk.red(`Invalid JSON in input file ${infile}: ${line}`));
        process.exit(1);
      }
    }
  }

  const allAnalysisResults: (AnalysisResult | null)[] = [];

  // Create timestamped run directory
  const inputBase = inputFiles.length === 1 ? path.basename(inputFiles[0], path.extname(inputFiles[0])) : 'combined';
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19); // Format: 2025-10-06T22-30-15
  const runDir = path.join(output, `${inputBase}-${timestamp}`);
  
  // Create run directory
  if (!fs.existsSync(runDir)) {
    fs.mkdirSync(runDir, { recursive: true });
  }
  
  console.log(chalk.gray(`📁 Output directory: ${runDir}`));
  
  // Determine file paths within run directory
  const rawResponsesPath = useMock ? undefined : path.join(runDir, 'raw-responses.jsonl');

  // =============================================================================================
  // EXECUTION MODE: SEQUENTIAL VS. PARALLEL
  // =============================================================================================
  // This tool supports two modes for processing the list of repositories, controlled by the
  // `--parallel` or `-p` flag.
  //
  // 1. SEQUENTIAL (Default):
  //    - Processes one repository at a time, waiting for each request to complete before
  //      starting the next.
  //    - PROS: Excellent for debugging. The console output is ordered and easy to follow.
  //      It's also gentler on the network and the remote API.
  //    - CONS: Slower. The total execution time is the sum of all individual request times.
  //
  // 2. PARALLEL (`--parallel`):
  //    - Processes a "batch" of repositories concurrently, dramatically speeding up the run.
  //    - PROS: Much faster. Total execution time is roughly the time of the slowest batch.
  //    - CONS: Console output will be interleaved and harder to follow for a single repo.
  //    - ROBUSTNESS: To handle potential issues with a large burst of requests:
  //      - BATCHING: We don't fire all 200+ requests at once. We process them in manageable
  //        chunks (defaulting to 5) to avoid overwhelming the API and triggering
  //        secondary (abuse) rate limits.
  //      - DELAY: We wait 1 second between batches to prevent burst detection.
  //      - RESILIENT ERROR HANDLING: We use `Promise.allSettled()`, which waits for all
  //        promises in a batch to finish, regardless of whether they succeed or fail. This
  //        ensures that one failed repository request doesn't crash the entire run.
  // =============================================================================================

  if (parallel) {
    // --- PARALLEL EXECUTION ---
    const batchSize = typeof parallel === 'string' ? parseInt(parallel, 10) || 5 : 5;
    console.log(chalk.bold.yellow(`⚡ Running in PARALLEL mode with a batch size of ${batchSize}.`));

    for (let i = 0; i < repositories.length; i += batchSize) {
      const batch = repositories.slice(i, i + batchSize);
      console.log(
        chalk.gray(`\n-- Processing batch ${i / batchSize + 1} (${batch.length} repos) --`)
      );

      const promises = batch.map(repo => fetchAndAnalyzeRepo(repo, client, useMock, useExtended, verbose, rawResponsesPath));
      const results = await Promise.allSettled(promises);

      results.forEach(result => {
        if (result.status === 'fulfilled') {
          allAnalysisResults.push(result.value);
        } else {
          // A specific repo fetch failed; log it and continue.
          console.error(chalk.red('A repository fetch failed:'), result.reason);
          allAnalysisResults.push(null); // Add null to signify failure.
        }
      });

      // Add a 1-second delay between batches to avoid secondary rate limits
      if (i + batchSize < repositories.length) {
        if (verbose) {
          console.log(chalk.gray('  Waiting 1 second before next batch...'));
        }
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }
  } else {
    // --- SEQUENTIAL EXECUTION ---
    console.log(chalk.bold.blue('🐌 Running in SEQUENTIAL mode.'));
    for (const repo of repositories) {
      const result = await fetchAndAnalyzeRepo(repo, client, useMock, useExtended, verbose, rawResponsesPath);
      allAnalysisResults.push(result);
    }
  }

  // --- REPORT GENERATION ---
  const validAnalysisResults = allAnalysisResults.filter(
    (res): res is Exclude<AnalysisResult, null> => res !== null
  );

  if (validAnalysisResults.length > 0) {
    if (verbose) {
      console.log(chalk.bold.bgWhite.black('\n  Column Legend & Detection Logic  '));
      const legendRows = [
        [chalk.bold('Column'), chalk.bold('Description'), chalk.bold('Detection Logic')],
        ['SBOM', 'SBOM artifact in release', 'Filename: sbom|spdx|cyclonedx'],
        ['Sig', 'Signature artifact in release', 'Extension: .sig, .asc, .pem, .pub'],
        ['Att', 'Attestation artifact in release', 'Filename: attestation'],
        ['SBOM CI', 'SBOM generator tool in CI', 'Regex: syft|trivy|cdxgen|spdx-sbom-generator in workflow YAML'],
        ['Sign CI', 'Signature/attestation tool in CI', 'Regex: cosign|sigstore|slsa-github-generator in workflow YAML'],
        ['GoRel CI', 'Goreleaser used in CI', 'Regex: goreleaser/goreleaser-action in workflow YAML'],
      ];
      const legendTable = new Table({
        head: legendRows[0],
        colWidths: [16, 32, 48],
        wordWrap: true,
        style: { head: [], border: [] },
      });
      for (const row of legendRows.slice(1)) legendTable.push(row);
      console.log(legendTable.toString());
    }
    
    const runMetadata = {
      queryType: useExtended ? 'GetRepoDataExtendedInfo' : 'GetRepoDataArtifacts',
      timestamp,
      totalRepos: repositories.length,
      successfulRepos: validAnalysisResults.length,
      failedRepos: repositories.length - validAnalysisResults.length,
    };
    
    await generateReports(validAnalysisResults, runDir, inputBase, runMetadata);
    
    // Generate file size report
    await generateSizeReport(runDir, runMetadata);

    const ciToolTypes = [
        { key: 'sbom', label: 'SBOM' },
        { key: 'signature', label: 'Sig' },
        { key: 'attestation', label: 'Att' },
        { key: 'sbom-generator', label: 'SBOM CI' },
        { key: 'signer', label: 'Sign CI' },
        { key: 'goreleaser', label: 'GoRel CI' },
    ];

    // --- Summary Table: One row per repo ---
    console.log(chalk.bold.bgBlueBright.white('\n  Repository Summary  '));
    const summaryTable = new Table({
      head: [chalk.bold('Repository'), chalk.bold('Releases'), ...ciToolTypes.map(t => chalk.bold(t.label)), chalk.bold('Total Artifacts')],
      colWidths: [30, 10, ...Array(ciToolTypes.length).fill(10), 16],
      style: { head: [], border: [] },
    });

    const ciToolCounts: Record<string, number> = Object.fromEntries(ciToolTypes.map(t => [t.key, 0]));
    let totalReleases = 0;
    let totalArtifacts = 0;

    for (const result of validAnalysisResults) {
      const releaseCount = result.releases?.length || 0;
      totalReleases += releaseCount;
      
      // Count total artifacts across all releases for this repo
      let repoArtifactCount = 0;
      result.releases?.forEach(release => {
        repoArtifactCount += release.artifacts.length;
      });
      totalArtifacts += repoArtifactCount;
      
      const ciTools = new Set(result.summary.sbomCiTools);
      
      const summaryRow = [
        chalk.cyan(result.repository.owner + '/' + result.repository.name),
        chalk.white(releaseCount.toString()),
        ...ciToolTypes.map(t => {
          const present = ciTools.has(t.key);
          if (present) ciToolCounts[t.key]++;
          return present ? chalk.greenBright('✔') : '';
        }),
        repoArtifactCount > 0 ? chalk.gray(repoArtifactCount.toString()) : '',
      ];
      summaryTable.push(summaryRow);
    }
    console.log(summaryTable.toString());
    
    const summaryTotals = ciToolTypes.map(t => `${chalk.bold(t.label)}: ${ciToolCounts[t.key]}`).join(' | ');
    console.log(chalk.blue.bold(`\nSummary: ${validAnalysisResults.length} repos, ${totalReleases} releases, ${totalArtifacts} artifacts | ${summaryTotals}`));

    // --- Detailed Table: One row per release ---
    console.log(chalk.bold.bgBlueBright.white('\n  Detailed Release View  '));
    const detailTable = new Table({
      head: [chalk.bold('Repo'), chalk.bold('Release'), ...ciToolTypes.map(t => chalk.bold(t.label)), chalk.bold('Artifacts')],
      colWidths: [25, 20, ...Array(ciToolTypes.length).fill(10), 12],
      style: { head: [], border: [] },
    });

    for (const result of validAnalysisResults) {
      const repoName = result.repository.name;
      
      // Show each release as a separate row
      if (result.releases && result.releases.length > 0) {
        result.releases.forEach((release, idx) => {
          // Build set of tools present in this specific release's artifacts
          const releaseTools = new Set<string>();
          let artifactCount = 0;
          
          release.artifacts.forEach(artifact => {
            artifactCount++;
            if (artifact.isSbom) releaseTools.add('sbom');
            if (artifact.isSignature) releaseTools.add('signature');
            if (artifact.isAttestation) releaseTools.add('attestation');
          });
          
          // Also check summary for CI tools (these are repo-level, not release-level)
          const ciTools = new Set(result.summary.sbomCiTools);
          
          const row = [
            idx === 0 ? chalk.cyan(repoName) : '', // Only show repo name on first release
            chalk.white(release.tagName),
            ...ciToolTypes.map(t => {
              const present = releaseTools.has(t.key) || ciTools.has(t.key);
              return present ? chalk.greenBright('✔') : '';
            }),
            artifactCount > 0 ? chalk.gray(artifactCount.toString()) : '',
          ];
          detailTable.push(row);
        });
      } else {
        // Repo with no releases
        const row = [
          chalk.cyan(repoName),
          chalk.gray('(no releases)'),
          ...Array(ciToolTypes.length).fill(''),
          '',
        ];
        detailTable.push(row);
      }
    }
    console.log(detailTable.toString());
  } else {
    console.log(chalk.yellow('No data was analyzed. Reports will not be generated.'));
  }

  console.log(chalk.blue.bold('\n✨ Analysis complete.'));
}

main().catch((error) => {
  console.error(chalk.red.bold('An unexpected error occurred:'), error);
});
</file>

</files>
