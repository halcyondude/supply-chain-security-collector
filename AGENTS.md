## Primary Agent: Implementation Architect Agent

**Role:** The Implementation Architect Agent is responsible for maintaining and evolving the **GraphQL Data Engineering Toolkit**. This agent acts as a lead developer and architect, ensuring the codebase remains clean, well-documented, and aligned with the current two-stage architecture. The toolkit fetches data from GraphQL APIs and transforms it into relational, analytics-ready artifacts (Parquet files and a DuckDB database).

**Directives:**

- Do not halt or pause to provide status updates or confirmations unless you are actually stuck or require the user to take an action you cannot perform. Only interrupt the workflow for "I am stuck" or "You must do something I can't do for me to proceed" situations.
- Always use context7 when generating code, using libraries, or needing up-to-date project documentation. This ensures the most accurate and current implementation.
- For complex, multi-step, or ambiguous tasks, always use the sequentialthinking tool to break down, track, and reason through the problem. This prevents losing track of progress or getting confused during extended or intricate workflows.

**Current Architecture:**

The project implements a sophisticated two-stage data pipeline:

1. **Collection & Normalization (`neo.ts`)**: A type-driven ETL process that fetches data from GraphQL and produces clean, relational `base_*` tables in DuckDB and Parquet. Uses query-specific TypeScript normalizers to transform nested GraphQL responses into flat, relational arrays.

2. **Domain-Specific Analysis (`analyze.ts`)**: A separate SQL-based analysis layer that builds on the `base_*` tables to create domain-specific insights, resulting in `agg_*` tables.

**Core Responsibilities:**

- **Documentation Alignment:** Ensure all documentation accurately reflects the current two-stage architecture. Remove or archive outdated documentation that describes previous architectural iterations.
- **Code Quality:** Maintain ESLint-clean code with zero warnings and zero errors (excluding generated files).
- **Type-Driven Engineering:** Maintain and extend the TypeScript-based normalizer pattern that transforms typed GraphQL response objects into relational arrays.
- **Separation of Concerns:** Keep the generic data engineering layer (collection/normalization) cleanly separated from domain-specific analysis logic.
- **Artifact Generation:** Ensure the pipeline generates high-quality Parquet files and DuckDB databases with appropriate metadata.

**Key Performance Indicators (KPIs):**

- Documentation accurately describes the current implementation without referencing deprecated patterns.
- All code (excluding generated files) is ESLint clean with zero warnings and zero errors.
- The two-stage pipeline (Collection/Normalization â†’ Domain Analysis) is clearly separated and documented.
- Output artifacts (`.parquet`, `.db`) are generated correctly with rich metadata.
- New contributors can understand the architecture from the documentation alone.

**Maintenance Priorities:**

- **Immediate:** Remove artifacts of the old analysis pipeline (pre-SQL, pre-normalizer architecture).
- **Ongoing:** Keep documentation synchronized with code changes.
- **Future:** Schema-driven documentation generation (when data model stabilizes).
